import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="WOS Analysis Dashboard", layout="centered")

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_data
def load_data(uploaded_file):
    encodings_to_try = ['utf-8', 'latin1', 'cp949', 'utf-8-sig']
    df = None
    # WoS Plain text fileì€ 'PT'ë¡œ ì‹œì‘í•˜ëŠ” ë ˆì½”ë“œë“¤ë¡œ êµ¬ì„±ë¨
    # ì´ë¥¼ ë¨¼ì € ì²´í¬í•˜ì—¬ ë¹ ë¥´ê²Œ íŒŒì¼ì„ ì½ìŒ
    try:
        uploaded_file.seek(0)
        content = uploaded_file.read().decode('utf-8-sig')
        if content.strip().startswith('FN') or content.strip().startswith('PT'):
            # íŒŒì¼ ë‚´ìš©ì„ ë ˆì½”ë“œ ë‹¨ìœ„ë¡œ ë¶„í• 
            records_str = content.strip().split('\nER\n')
            all_records = []
            for record_str in records_str:
                if not record_str.strip(): continue
                record_data = {}
                current_tag = ''
                for line in record_str.split('\n'):
                    if len(line) > 2 and line[2] == ' ':
                        current_tag = line[:2]
                        # ë‹¤ì¤‘ ê°’ í•„ë“œ (ì˜ˆ: AU, CR) ì²˜ë¦¬ë¥¼ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                        if current_tag not in record_data:
                            record_data[current_tag] = []
                        record_data[current_tag].append(line[3:])
                    elif line.startswith('   '): # ë“¤ì—¬ì“°ê¸° ëœ ë¼ì¸
                        if current_tag in record_data:
                            record_data[current_tag].append(line[3:])
                # ë¦¬ìŠ¤íŠ¸ë¥¼ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ í•©ì³ì„œ DataFrameì— ì €ì¥
                for tag, values in record_data.items():
                    record_data[tag] = '; '.join(values)
                all_records.append(record_data)
            df = pd.DataFrame(all_records)
            # íŒŒì¼ ëì— ERì´ ì—†ëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬
            if 'ER' in df.columns:
                 df = df.drop(columns=['ER'])
            return df
    except Exception as e:
        st.warning(f"WoS í˜•ì‹ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì¼ë°˜ CSV/TSV ì½ê¸°ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")

    # ì¼ë°˜ CSV/TSV íŒŒì¼ ì½ê¸° ì‹œë„
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df_try = pd.read_csv(uploaded_file, sep='\t', encoding=encoding)
            if df_try.shape[1] > 1: return df_try
        except Exception: continue
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df_try = pd.read_csv(uploaded_file, encoding=encoding)
            if df_try.shape[1] > 1: return df_try
        except Exception: continue
    return None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def classify_article(row):
    inclusion_keywords = ['user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'behavior', 'experience', 'engagement', 'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 'society', 'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization', 'education', 'learning', 'influencer']
    exclusion_keywords = ['protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer', 'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput', 'sensor data', 'geoscience', 'environmental data', 'wlan', 'ofdm', 'error correction', 'tcp', 'udp', 'network traffic']
    title = str(row.get('TI', '')).lower()
    source_title = str(row.get('SO', '')).lower()
    author_keywords = str(row.get('DE', '')).lower()
    keywords_plus = str(row.get('ID', '')).lower()
    abstract = str(row.get('AB', '')).lower()
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    if any(keyword in full_text for keyword in exclusion_keywords): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in full_text for keyword in inclusion_keywords): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    if not isinstance(keywords_str, str): return ""
    all_keywords = keywords_str.lower().split(';')
    cleaned_keywords = set()
    for keyword in all_keywords:
        keyword = keyword.strip().replace('-', ' ')
        keyword = re.sub(r'[^a-z\s]', '', keyword)
        final_words = [lemmatizer.lemmatize(w) for w in keyword.split() if w not in stop_words and len(w) > 2]
        if final_words: cleaned_keywords.add(" ".join(final_words))
    return '; '.join(sorted(list(cleaned_keywords)))

# --- **ìˆ˜ì •**: SCIMAT í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìƒˆ í•¨ìˆ˜ ---
@st.cache_data
def convert_df_to_scimat_format(df_to_convert):
    # SciMATì´ ì¸ì‹í•˜ëŠ” í•„ë“œ ìˆœì„œ (ì¼ë°˜ì ì¸ ìˆœì„œ)
    wos_field_order = [
        'FN', 'VR', 'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB',
        'C1', 'RP', 'EM', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU',
        'PI', 'PA', 'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP',
        'DI', 'PG', 'WC', 'SC', 'GA', 'UT', 'PM', 'DA'
    ]
    
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'CR'] # ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ëˆ ì•¼ í•  í•„ë“œ

    for _, row in df_to_convert.iterrows():
        # ì •ì˜ëœ ìˆœì„œëŒ€ë¡œ í•„ë“œë¥¼ ì •ë ¬í•˜ë˜, ë°ì´í„°ì— ì—†ëŠ” í•„ë“œëŠ” ê±´ë„ˆëœ€
        sorted_tags = [tag for tag in wos_field_order if tag in row.index and pd.notna(row[tag])]
        
        for tag in sorted_tags:
            value = row[tag]
            if not value: continue

            # ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            if not isinstance(value, str):
                value = str(value)
            
            # ì—¬ëŸ¬ ì¤„ í•„ë“œ ì²˜ë¦¬
            if tag in multi_line_fields:
                items = [item.strip() for item in value.split(';')]
                if items:
                    file_content.append(f"{tag} {items[0]}")
                    for item in items[1:]:
                        file_content.append(f"   {item}")
            # í•œ ì¤„ í•„ë“œ ì²˜ë¦¬
            else:
                file_content.append(f"{tag} {value}")

        file_content.append("ER\n") # ë ˆì½”ë“œ ë

    # ë§ˆì§€ë§‰ ë ˆì½”ë“œì˜ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    if file_content[-1] == "ER\n":
        file_content[-1] = "ER"
        
    return "\n".join(file_content).encode('utf-8-sig')

# --- Streamlit UI ë° ì‹¤í–‰ ë¡œì§ ---
st.title("WOS ë°ì´í„° ë¶„ì„ ë° ì •ì œ ë„êµ¬")
st.caption("WOS Data Classifier & Preprocessor")

uploaded_file = st.file_uploader("WoS Raw Data íŒŒì¼(CSV/TXT)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['csv', 'txt'])

if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is None:
        st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ 'Tab-delimited' ë˜ëŠ” 'Plain Text' í˜•ì‹ì˜ íŒŒì¼ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    with st.spinner("ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... / Analyzing data..."):
        
        df['Classification'] = df.apply(classify_article, axis=1)
        
        stop_words = set(stopwords.words('english'))
        custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article'}
        stop_words.update(custom_stop_words)
        lemmatizer = WordNetLemmatizer()
        include_mask = df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'

        if 'DE' in df.columns:
            df['DE_cleaned'] = df['DE'].copy()
            df.loc[include_mask, 'DE_cleaned'] = df.loc[include_mask, 'DE'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))
        if 'ID' in df.columns:
            df['ID_cleaned'] = df['ID'].copy()
            df.loc[include_mask, 'ID_cleaned'] = df.loc[include_mask, 'ID'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))

        st.success("âœ… ë¶„ì„ ë° ë³€í™˜ ì™„ë£Œ! / Process Complete!")

        # --- ê²°ê³¼ ìš”ì•½ ë° ì‹œê°í™” ---
        st.subheader("ë¶„ì„ ê²°ê³¼ ìš”ì•½ / Analysis Summary")
        st.write("##### ë…¼ë¬¸ ë¶„ë¥˜ ê²°ê³¼")
        classification_counts = df['Classification'].value_counts().reset_index()
        classification_counts.columns = ['Classification', 'Count']
        st.dataframe(classification_counts)
        chart = alt.Chart(classification_counts).mark_arc(innerRadius=50).encode(theta=alt.Theta(field="Count", type="quantitative"), color=alt.Color(field="Classification", type="nominal", title="ë¶„ë¥˜"), tooltip=['Classification', 'Count']).properties(title='ë…¼ë¬¸ ë¶„ë¥˜ ë¶„í¬')
        st.altair_chart(chart, use_container_width=True)
        st.markdown("---")
        st.write("##### 'ê´€ë ¨ì—°êµ¬(Include)' ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")
        all_keywords = []
        if 'DE_cleaned' in df.columns: all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'DE_cleaned'].dropna() for kw in text.split(';') if kw.strip()])
        if 'ID_cleaned' in df.columns: all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'ID_cleaned'].dropna() for kw in text.split(';') if kw.strip()])
        if all_keywords:
            keyword_counts = Counter(all_keywords)
            top_n = 20; top_keywords = keyword_counts.most_common(top_n); df_keywords = pd.DataFrame(top_keywords, columns=['Keyword', 'Frequency'])
            keyword_chart = alt.Chart(df_keywords).mark_bar().encode(x=alt.X('Frequency:Q', title='ë¹ˆë„'), y=alt.Y('Keyword:N', title='í‚¤ì›Œë“œ', sort='-x'), tooltip=['Keyword', 'Frequency']).properties(title=f'ìƒìœ„ {top_n} í‚¤ì›Œë“œ ë¹ˆë„')
            st.altair_chart(keyword_chart, use_container_width=True)
        else: st.warning("'ê´€ë ¨ì—°êµ¬'ë¡œ ë¶„ë¥˜ëœ ë…¼ë¬¸ì—ì„œ ìœ íš¨í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # --- ìµœì¢… ì¶œë ¥ íŒŒì¼ ìƒì„± ---
        st.markdown("---")
        st.subheader("ë°ì´í„° ë‹¤ìš´ë¡œë“œ / Download Data")
        df_final = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
        if 'DE_cleaned' in df_final.columns: df_final['DE'] = df_final['DE_cleaned']
        if 'ID_cleaned' in df_final.columns: df_final['ID'] = df_final['ID_cleaned']
        cols_to_drop = ['Classification', 'DE_cleaned', 'ID_cleaned']; df_final_output = df_final.drop(columns=[col for col in cols_to_drop if col in df_final.columns])
        st.metric("ìµœì¢… ë¶„ì„ ëŒ€ìƒ ë…¼ë¬¸ ìˆ˜ (Include + Review)", len(df_final_output))
        st.dataframe(df_final_output.head(10))

        # **ìˆ˜ì •**: ìƒˆë¡œ ë§Œë“  SCIMAT ë³€í™˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ìƒì„±
        text_data = convert_df_to_scimat_format(df_final_output)
        
        st.download_button(label="ğŸ“¥ ìµœì¢… íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.txt for SciMAT)", data=text_data, file_name="wos_processed_for_scimat.txt", mime="text/plain")
