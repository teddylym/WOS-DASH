import streamlit as st
import pandas as pd
import altair as alt
import io
import re
from collections import Counter

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Multi-File Merger | SCIMAT Edition",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- í† ìŠ¤ ìŠ¤íƒ€ì¼ CSS ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@400;500;600;700&display=swap');
    
    .main-container {
        background: #f2f4f6;
        min-height: 100vh;
        font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, sans-serif;
    }
    
    .metric-card {
        background: white;
        border-radius: 8px;
        padding: 20px;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        margin-bottom: 12px;
        transition: all 0.2s ease;
    }
    
    .metric-card:hover {
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        transform: translateY(-1px);
        border-color: #3182f6;
    }
    
    .metric-value {
        font-size: 2.2rem;
        font-weight: 700;
        color: #191f28;
        margin: 0;
        line-height: 1.2;
        letter-spacing: -0.02em;
    }
    
    .metric-label {
        font-size: 13px;
        color: #8b95a1;
        margin: 6px 0 0 0;
        font-weight: 500;
        letter-spacing: -0.01em;
    }
    
    .metric-icon {
        background: #3182f6;
        color: white;
        width: 32px;
        height: 32px;
        border-radius: 6px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 16px;
        margin-bottom: 12px;
    }
    
    .chart-container {
        background: white;
        border-radius: 8px;
        padding: 24px;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        margin: 16px 0;
    }
    
    .chart-title {
        font-size: 16px;
        font-weight: 600;
        color: #191f28;
        margin-bottom: 16px;
        letter-spacing: -0.01em;
    }
    
    .section-header {
        background: white;
        color: #191f28;
        padding: 16px 20px;
        border-radius: 8px;
        margin: 20px 0 12px 0;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        position: relative;
        overflow: hidden;
    }
    
    .section-header::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        width: 4px;
        height: 100%;
        background: #3182f6;
    }
    
    .section-title {
        font-size: 16px;
        font-weight: 600;
        margin: 0;
        letter-spacing: -0.01em;
        color: #191f28;
    }
    
    .section-subtitle {
        font-size: 13px;
        color: #8b95a1;
        margin: 4px 0 0 0;
        font-weight: 400;
        letter-spacing: 0;
    }
</style>
""", unsafe_allow_html=True)

# --- ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ë¡œë”© ë° ë³‘í•© í•¨ìˆ˜ ---
def load_and_merge_wos_files(uploaded_files):
    """ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ì„ ë¡œë”©í•˜ê³  ë³‘í•© - ì¤‘ë³µ ì œê±° ì™„ì „ ìˆ˜ì •"""
    all_dataframes = []
    file_status = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            file_bytes = uploaded_file.getvalue()
            encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'iso-8859-1']
            
            df = None
            encoding_used = None
            
            for encoding in encodings_to_try:
                try:
                    file_content = file_bytes.decode(encoding)
                    
                    # WOS ì›ë³¸ í˜•ì‹ ê²€ì¦ (FNìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•¨)
                    if not file_content.strip().startswith('FN '):
                        continue
                        
                    # WOS í˜•ì‹ íŒŒì‹±
                    df = parse_wos_format(file_content)
                    if df is not None and len(df) > 0:
                        encoding_used = encoding
                        break
                        
                except Exception:
                    continue
            
            if df is not None:
                all_dataframes.append(df)
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'SUCCESS',
                    'papers': len(df),
                    'encoding': encoding_used,
                    'message': f'âœ… {len(df)}í¸ ë…¼ë¬¸ ë¡œë”© ì„±ê³µ'
                })
            else:
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'ERROR',
                    'papers': 0,
                    'encoding': 'N/A',
                    'message': 'âŒ WOS Plain Text í˜•ì‹ì´ ì•„ë‹˜'
                })
                
        except Exception as e:
            file_status.append({
                'filename': uploaded_file.name,
                'status': 'ERROR',
                'papers': 0,
                'encoding': 'N/A',
                'message': f'âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}'
            })
    
    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ë³‘í•©
    if all_dataframes:
        merged_df = pd.concat(all_dataframes, ignore_index=True)
        original_count = len(merged_df)
        
        # ì¤‘ë³µ ì œê±° ë¡œì§
        duplicates_removed = 0
        
        if 'UT' in merged_df.columns:
            # UT í•„ë“œì˜ ì‹¤ì œ ê°’ë“¤ í™•ì¸
            ut_series = merged_df['UT'].copy()
            
            # ìœ íš¨í•œ UT ê°’ë§Œ í•„í„°ë§ (ë” ì—„ê²©í•œ ì¡°ê±´)
            def is_meaningful_ut(value):
                if pd.isna(value):
                    return False
                str_value = str(value).strip()
                # ë¹ˆ ë¬¸ìì—´, 'nan', 'None', ë§¤ìš° ì§§ì€ ê°’ë“¤ ì œì™¸
                if len(str_value) == 0 or str_value.lower() in ['nan', 'none', 'null', '']:
                    return False
                # WOS UTëŠ” ì¼ë°˜ì ìœ¼ë¡œ 'WOS:' ë˜ëŠ” ë¬¸ì+ìˆ«ì ì¡°í•©
                # ìµœì†Œ 10ì ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ê°’ë§Œ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
                if len(str_value) < 10:
                    return False
                # 'WOS:' ë¡œ ì‹œì‘í•˜ê±°ë‚˜ ì¶©ë¶„íˆ ê¸´ ì˜ìˆ«ì ì¡°í•©ì¸ ê²½ìš°ë§Œ ìœ íš¨
                if str_value.startswith('WOS:') or (len(str_value) >= 15 and any(c.isalnum() for c in str_value)):
                    return True
                return False
            
            # ìœ íš¨í•œ UTë¥¼ ê°€ì§„ í–‰ë“¤ë§Œ ì„ ë³„
            meaningful_ut_mask = ut_series.apply(is_meaningful_ut)
            rows_with_meaningful_ut = merged_df[meaningful_ut_mask]
            rows_without_meaningful_ut = merged_df[~meaningful_ut_mask]
            
            # ì‹¤ì œë¡œ ì˜ë¯¸ìˆëŠ” UTê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¤‘ë³µ ê²€ì‚¬
            if len(rows_with_meaningful_ut) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒ ìˆì–´ì•¼ ì¤‘ë³µ ê²€ì‚¬ ì˜ë¯¸
                # ì¤‘ë³µ ì œê±° ì „í›„ ë¹„êµ
                before_dedup = len(rows_with_meaningful_ut)
                deduplicated_meaningful = rows_with_meaningful_ut.drop_duplicates(subset=['UT'], keep='first')
                after_dedup = len(deduplicated_meaningful)
                
                actual_duplicates = before_dedup - after_dedup
                
                if actual_duplicates > 0:
                    duplicates_removed = actual_duplicates
                    
                    # ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ì™€ UT ì—†ëŠ” í–‰ë“¤ ì¬ê²°í•©
                    merged_df = pd.concat([deduplicated_meaningful, rows_without_meaningful_ut], ignore_index=True)
        
        # ëŒ€ì•ˆ: UTê°€ ì—†ê±°ë‚˜ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì œëª©+ì €ì ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        if duplicates_removed == 0 and 'TI' in merged_df.columns:
            # ì œëª©ê³¼ ì²« ë²ˆì§¸ ì €ì ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸ (ë§¤ìš° ë³´ìˆ˜ì )
            title_author_before = len(merged_df)
            
            # ì œëª©ì´ ìˆê³  ì €ìê°€ ìˆëŠ” í–‰ë“¤ë§Œ ëŒ€ìƒ
            has_title = merged_df['TI'].notna() & (merged_df['TI'].str.strip() != '')
            has_author = merged_df.get('AU', pd.Series()).notna() if 'AU' in merged_df.columns else pd.Series([False] * len(merged_df))
            
            complete_rows = merged_df[has_title & has_author] if 'AU' in merged_df.columns else merged_df[has_title]
            incomplete_rows = merged_df[~(has_title & has_author)] if 'AU' in merged_df.columns else merged_df[~has_title]
            
            if len(complete_rows) > 1:
                dedup_columns = ['TI', 'AU'] if 'AU' in merged_df.columns else ['TI']
                deduplicated_complete = complete_rows.drop_duplicates(subset=dedup_columns, keep='first')
                title_author_removed = len(complete_rows) - len(deduplicated_complete)
                
                if title_author_removed > 0:
                    duplicates_removed = title_author_removed
                    merged_df = pd.concat([deduplicated_complete, incomplete_rows], ignore_index=True)
        
        return merged_df, file_status, duplicates_removed
    else:
        return None, file_status, 0

def parse_wos_format(content):
    """WOS Plain Text í˜•ì‹ì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    lines = content.split('\n')
    records = []
    current_record = {}
    current_field = None
    
    for line in lines:
        line = line.rstrip()
        
        if not line:
            continue
            
        # ë ˆì½”ë“œ ì¢…ë£Œ
        if line == 'ER':
            if current_record:
                records.append(current_record.copy())
                current_record = {}
                current_field = None
            continue
            
        # í—¤ë” ë¼ì¸ ê±´ë„ˆë›°ê¸°
        if line.startswith(('FN ', 'VR ')):
            continue
            
        # ìƒˆ í•„ë“œ ì‹œì‘
        if not line.startswith('   ') and ' ' in line:
            parts = line.split(' ', 1)
            if len(parts) == 2:
                field_tag, field_value = parts
                current_field = field_tag
                current_record[field_tag] = field_value.strip()
        
        # ê¸°ì¡´ í•„ë“œ ì—°ì†
        elif line.startswith('   ') and current_field and current_field in current_record:
            continuation_value = line[3:].strip()
            if continuation_value:
                current_record[current_field] += '; ' + continuation_value
    
    # ë§ˆì§€ë§‰ ë ˆì½”ë“œ ì²˜ë¦¬
    if current_record:
        records.append(current_record)
    
    if not records:
        return None
        
    return pd.DataFrame(records)

# --- ìˆ˜ì •ëœ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° íŠ¹í™” ë¶„ë¥˜ í•¨ìˆ˜ (Natureê¸‰ ê¸°ì¤€) ---
def classify_article(row):
    """Natureê¸‰ ì €ë„ ê¸°ì¤€ì— ë§ì¶˜ ê· í˜•ì¡íŒ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ ë¶„ë¥˜"""
    
    # --- í‚¤ì›Œë“œ ì…‹ ì •ì˜ (ë” í¬ê´„ì ì´ê³  ê· í˜•ì¡íŒ ì ‘ê·¼) ---
    
    # IC1: í•µì‹¬ í”Œë«í¼ ë° ê°œë… (í™•ì¥ëœ ë²”ìœ„)
    core_streaming_keywords = [
        'live stream', 'livestream', 'live-stream', 'live video', 'live broadcast',
        'streaming platform', 'streaming service', 'streaming media',
        'live commerce', 'live shopping', 'social commerce', 'shoppertainment',
        'streamer', 'broadcaster', 'content creator', 'influencer',
        'viewer', 'audience', 'streaming community',
        'twitch', 'youtube', 'facebook live', 'instagram live', 'tiktok', 
        'douyin', 'kuaishou', 'taobao live', 'amazon live',
        'game streaming', 'esports', 'gaming broadcast',
        'educational streaming', 'webinar', 'virtual event',
        'live content', 'real-time content', 'synchronous media'
    ]
    
    # IC2: ì—°êµ¬ ì°¨ì› í‚¤ì›Œë“œ (6ê°œ ì°¨ì›ìœ¼ë¡œ ë¶„ë¥˜)
    research_dimensions = {
        'Technical': [
            'latency', 'bandwidth', 'quality', 'qos', 'qoe', 'buffering',
            'cdn', 'p2p', 'webrtc', 'streaming protocol', 'video quality',
            'adaptive bitrate', 'transcoding', 'edge computing', '5g'
        ],
        'Platform': [
            'platform', 'ecosystem', 'business model', 'monetization',
            'revenue', 'creator economy', 'platform governance', 'algorithm',
            'recommendation', 'content moderation', 'platform policy'
        ],
        'User': [
            'user behavior', 'user experience', 'engagement', 'interaction',
            'participation', 'motivation', 'satisfaction', 'loyalty',
            'parasocial', 'social presence', 'community', 'fandom'
        ],
        'Commercial': [
            'commerce', 'e-commerce', 'marketing', 'advertising', 'brand',
            'purchase', 'sales', 'conversion', 'roi', 'influencer marketing',
            'product placement', 'sponsorship', 'donation', 'virtual gift'
        ],
        'Social': [
            'social', 'cultural', 'identity', 'social capital', 'social impact',
            'digital culture', 'online community', 'social interaction',
            'social media', 'social network', 'viral', 'trend'
        ],
        'Educational': [
            'education', 'learning', 'teaching', 'mooc', 'online education',
            'distance learning', 'virtual classroom', 'student engagement',
            'pedagogical', 'instructional', 'training', 'skill development'
        ]
    }
    
    # EC: ë°°ì œ ê¸°ì¤€ (ìµœì†Œí™” ë° ëª…í™•í™”)
    
    # EC1: ìˆœìˆ˜ ê¸°ìˆ  ì¸í”„ë¼ (ì‚¬íšŒì  ë§¥ë½ ì™„ì „ ë¶€ì¬)
    pure_infrastructure = [
        'network topology', 'routing algorithm', 'packet loss',
        'tcp congestion', 'udp optimization', 'multicast routing',
        'satellite communication', 'optical fiber', 'base station',
        'antenna design', 'signal processing', 'modulation scheme'
    ]
    
    # EC2: ì˜ë£Œ/ìƒë¬¼í•™ì  ì‹ í˜¸ ì²˜ë¦¬ (ì™„ì „íˆ ë‹¤ë¥¸ ë„ë©”ì¸)
    medical_signals = [
        'ecg streaming', 'eeg monitoring', 'medical imaging',
        'biosignal', 'telemedicine device', 'remote surgery',
        'patient monitoring', 'health sensor'
    ]
    
    # EC3: ë¹„í•™ìˆ ì  ì½˜í…ì¸ 
    non_academic = [
        'conference abstract only', 'editorial', 'news item',
        'book review', 'obituary', 'erratum', 'retraction'
    ]
    
    # --- í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ì¶œ ë° ê²°í•© ---
    def extract_text(value):
        return str(value).lower().strip() if pd.notna(value) else ""
    
    title = extract_text(row.get('TI', ''))
    abstract = extract_text(row.get('AB', ''))
    author_keywords = extract_text(row.get('DE', ''))
    keywords_plus = extract_text(row.get('ID', ''))
    document_type = extract_text(row.get('DT', ''))
    
    # ì œëª©ê³¼ í‚¤ì›Œë“œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê²€í†  (ì´ˆë¡ì€ ë³´ì¡°ì )
    primary_text = ' '.join([title, author_keywords, keywords_plus])
    full_text = ' '.join([title, abstract, author_keywords, keywords_plus])
    
    # --- ë¶„ë¥˜ ë¡œì§ (ê· í˜•ì¡íŒ ì ‘ê·¼) ---
    
    # Stage 1: ë¬¸ì„œ ìœ í˜• í™•ì¸
    if document_type and any(exc in document_type for exc in ['editorial', 'correction', 'retraction']):
        return 'EC3 - Non-academic content'
    
    # Stage 2: í•µì‹¬ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ê´€ë ¨ì„± í™•ì¸ (ì œëª©/í‚¤ì›Œë“œ ìš°ì„ )
    core_relevance_score = sum(1 for kw in core_streaming_keywords if kw in primary_text)
    
    # ì œëª©ì´ë‚˜ í‚¤ì›Œë“œì— ëª…í™•í•œ ì–¸ê¸‰ì´ ìˆìœ¼ë©´ í¬í•¨
    if core_relevance_score >= 1:
        # ì—°êµ¬ ì°¨ì› í™•ì¸
        dimension_scores = {}
        for dim, keywords in research_dimensions.items():
            score = sum(1 for kw in keywords if kw in full_text)
            if score > 0:
                dimension_scores[dim] = score
        
        if dimension_scores:
            # ê°€ì¥ ê°•í•œ ì—°êµ¬ ì°¨ì› ì‹ë³„
            primary_dimension = max(dimension_scores, key=dimension_scores.get)
            
            # ìˆœìˆ˜ ì¸í”„ë¼ë‚˜ ì˜ë£Œ ì‹ í˜¸ê°€ ì£¼ìš” ì£¼ì œê°€ ì•„ë‹Œì§€ í™•ì¸
            infra_score = sum(1 for kw in pure_infrastructure if kw in full_text)
            medical_score = sum(1 for kw in medical_signals if kw in full_text)
            
            # ì¸í”„ë¼/ì˜ë£Œê°€ ë¶€ì°¨ì ì¸ ê²½ìš°ì—ë§Œ ë°°ì œ
            if infra_score > core_relevance_score * 2:
                return 'EC1 - Pure infrastructure focus'
            elif medical_score > core_relevance_score:
                return 'EC2 - Medical domain'
            else:
                return f'Include - {primary_dimension}'
        else:
            # ì—°êµ¬ ì°¨ì›ì´ ëª…í™•í•˜ì§€ ì•Šì§€ë§Œ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°ì€ ì–¸ê¸‰ë¨
            return 'Review - Dimension unclear'
    
    # Stage 3: ì´ˆë¡ì—ì„œë§Œ ì–¸ê¸‰ë˜ëŠ” ê²½ìš° (ë” ì‹ ì¤‘í•œ í‰ê°€)
    abstract_relevance = sum(1 for kw in core_streaming_keywords if kw in abstract)
    
    if abstract_relevance >= 2:  # ì´ˆë¡ì— ì—¬ëŸ¬ ë²ˆ ì–¸ê¸‰
        # ì—°êµ¬ ì°¨ì› ì¬í™•ì¸
        for dim, keywords in research_dimensions.items():
            if any(kw in full_text for kw in keywords):
                return f'Include - {dim}'
        return 'Review - Peripheral mention'
    elif abstract_relevance == 1:  # ì´ˆë¡ì— í•œ ë²ˆë§Œ ì–¸ê¸‰
        return 'Review - Minimal relevance'
    
    # Stage 4: ì™„ì „íˆ ë¬´ê´€í•œ ì—°êµ¬
    return 'Exclude - No relevance'

# --- ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ í•¨ìˆ˜ ---
def diagnose_merged_quality(df, file_count, duplicates_removed):
    """ë³‘í•©ëœ WOS ë°ì´í„°ì˜ í’ˆì§ˆ ì§„ë‹¨"""
    issues = []
    recommendations = []
    
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    required_fields = ['TI', 'AU', 'SO', 'PY']
    keyword_fields = ['DE', 'ID']
    
    for field in required_fields:
        if field not in df.columns:
            issues.append(f"âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        else:
            valid_count = df[field].notna().sum()
            total_count = len(df)
            missing_rate = (total_count - valid_count) / total_count * 100
            
            if missing_rate > 10:
                issues.append(f"âš ï¸ {field} í•„ë“œì˜ {missing_rate:.1f}%ê°€ ëˆ„ë½ë¨")
    
    # í‚¤ì›Œë“œ í•„ë“œ í’ˆì§ˆ í™•ì¸
    has_keywords = False
    for field in keyword_fields:
        if field in df.columns:
            has_keywords = True
            valid_keywords = df[field].notna() & (df[field] != '') & (df[field] != 'nan')
            valid_count = valid_keywords.sum()
            total_count = len(df)
            
            if valid_count < total_count * 0.7:
                issues.append(f"âš ï¸ {field} í•„ë“œì˜ {((total_count-valid_count)/total_count*100):.1f}%ê°€ ë¹„ì–´ìˆìŒ")
    
    if not has_keywords:
        issues.append("âŒ í‚¤ì›Œë“œ í•„ë“œ ì—†ìŒ: DE ë˜ëŠ” ID í•„ë“œ í•„ìš”")
    
    # ë³‘í•© ê´€ë ¨ ì •ë³´
    recommendations.append(f"âœ… {file_count}ê°œ íŒŒì¼ ì„±ê³µì ìœ¼ë¡œ ë³‘í•©ë¨")
    
    if duplicates_removed > 0:
        recommendations.append(f"ğŸ”„ ì¤‘ë³µ ë…¼ë¬¸ {duplicates_removed}í¸ ìë™ ì œê±°ë¨")
    else:
        recommendations.append("âœ… ì¤‘ë³µ ë…¼ë¬¸ ì—†ìŒ - ëª¨ë“  ë…¼ë¬¸ì´ ê³ ìœ  ë°ì´í„°")
    
    recommendations.append("âœ… WOS Plain Text í˜•ì‹ - SCIMAT ìµœì  í˜¸í™˜ì„± í™•ë³´")
    
    return issues, recommendations

# --- WOS Plain Text í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ---
def convert_to_scimat_wos_format(df_to_convert):
    """SCIMAT ì™„ì „ í˜¸í™˜ WOS Plain Text í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']

    for _, row in df_to_convert.iterrows():
        if len(file_content) > 2:
            file_content.append("")
        
        for tag in wos_field_order:
            if tag in row.index and pd.notna(row[tag]) and str(row[tag]).strip() and str(row[tag]).strip().lower() != 'nan':
                value = str(row[tag]).strip()
                
                if tag in multi_line_fields:
                    items = [item.strip() for item in value.split(';') if item.strip()]
                    if items:
                        file_content.append(f"{tag} {items[0]}")
                        for item in items[1:]:
                            file_content.append(f"   {item}")
                else:
                    file_content.append(f"{tag} {value}")
        
        file_content.append("ER")
    
    return "\n".join(file_content).encode('utf-8-sig')

# --- ë©”ì¸ í—¤ë” ---
st.markdown("""
<div style="text-align: center; padding: 2rem 0; background: linear-gradient(135deg, #3182f6, #1c64f2); color: white; border-radius: 8px; margin-bottom: 1.5rem;">
    <h1 style="font-size: 3rem; font-weight: 700; margin-bottom: 0.3rem;">WOS PREP</h1>
    <p style="font-size: 1.1rem; margin: 0; font-weight: 500;">SCIMAT Edition - Natureê¸‰ í•™ìˆ  ê¸°ì¤€ ì ìš©</p>
</div>
""", unsafe_allow_html=True)

# --- í•µì‹¬ ê¸°ëŠ¥ ì†Œê°œ ---
st.markdown("""
<div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 12px; margin: 20px 0;">
    <div style="background: white; border-radius: 8px; padding: 20px; text-align: center; border: 1px solid #e5e8eb;">
        <div style="font-size: 32px; margin-bottom: 12px;">ğŸ”—</div>
        <div style="font-weight: 600; margin-bottom: 8px;">ë‹¤ì¤‘ íŒŒì¼ ìë™ ë³‘í•©</div>
        <div style="font-size: 13px; color: #8b95a1;">ì—¬ëŸ¬ WOS íŒŒì¼ì„ í•œ ë²ˆì— ë³‘í•© ì²˜ë¦¬</div>
    </div>
    <div style="background: white; border-radius: 8px; padding: 20px; text-align: center; border: 1px solid #e5e8eb;">
        <div style="font-size: 32px; margin-bottom: 12px;">ğŸ¯</div>
        <div style="font-weight: 600; margin-bottom: 8px;">ê· í˜•ì¡íŒ í•™ìˆ  í•„í„°ë§</div>
        <div style="font-size: 13px; color: #8b95a1;">Natureê¸‰ ì €ë„ ê¸°ì¤€ ì ìš©</div>
    </div>
    <div style="background: white; border-radius: 8px; padding: 20px; text-align: center; border: 1px solid #e5e8eb;">
        <div style="font-size: 32px; margin-bottom: 12px;">ğŸ“Š</div>
        <div style="font-weight: 600; margin-bottom: 8px;">SCIMAT ì™„ë²½ í˜¸í™˜</div>
        <div style="font-size: 13px; color: #8b95a1;">100% í˜¸í™˜ WOS í˜•ì‹ ì¶œë ¥</div>
    </div>
</div>
""", unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
st.markdown("""
<div class="section-header">
    <div class="section-title">ğŸ“‚ ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ì—…ë¡œë“œ</div>
    <div class="section-subtitle">500ê°œ ë‹¨ìœ„ë¡œ ë‚˜ë‰œ ì—¬ëŸ¬ WOS íŒŒì¼ì„ ëª¨ë‘ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”</div>
</div>
""", unsafe_allow_html=True)

uploaded_files = st.file_uploader(
    "WOS Plain Text íŒŒì¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
    type=['txt'],
    accept_multiple_files=True,
    label_visibility="collapsed",
    help="WOS Plain Text íŒŒì¼ë“¤ì„ ë“œë˜ê·¸í•˜ì—¬ ë†“ê±°ë‚˜ í´ë¦­í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”"
)

if uploaded_files:
    st.markdown(f"ğŸ“‹ **ì„ íƒëœ íŒŒì¼ ê°œìˆ˜:** {len(uploaded_files)}ê°œ")
    
    with st.spinner(f"ğŸ”„ {len
