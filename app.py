import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt
import io

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="WOS Prep", page_icon="âš™ï¸", layout="centered")

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True); nltk.download('wordnet', quiet=True); nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ì´ì „ê³¼ ë™ì¼) ---
def load_data(uploaded_file):
    file_bytes = uploaded_file.getvalue()
    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'cp949']
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(io.StringIO(file_bytes.decode(encoding)), sep='\t', lineterminator='\n')
            if df.shape[1] > 1: return df
        except Exception: continue
    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(io.StringIO(file_bytes.decode(encoding)))
            if df.shape[1] > 1: return df
        except Exception: continue
    return None

def normalize_keyword(keyword):
    normalization_dict = {'ml': 'machine learning', 'ai': 'artificial intelligence', 'live streaming': 'streaming',
                          'livestreaming': 'streaming', 'live stream': 'streaming', 'user behavior': 'consumer behavior',
                          'user engagement': 'consumer engagement', 'pls-sem': 'sem', 'structural equation modeling': 'sem'}
    return normalization_dict.get(keyword, keyword)

def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    if pd.isna(keywords_str): return ""
    cleaned_keywords = set()
    for keyword in str(keywords_str).lower().split(';'):
        keyword = keyword.strip().replace('-', ' '); keyword = re.sub(r'[^a-z\s]', '', keyword)
        words = [lemmatizer.lemmatize(w) for w in keyword.split() if w not in stop_words and len(w) > 2]
        if not words: continue
        normalized_keyword = normalize_keyword(" ".join(words)); cleaned_keywords.add(normalized_keyword)
    return '; '.join(sorted(list(cleaned_keywords)))

def classify_article(row):
    inclusion_keywords = ['user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'behavior', 'experience', 'engagement', 'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 'society', 'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization', 'education', 'learning', 'influencer']
    exclusion_keywords = ['protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer', 'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput', 'sensor data', 'geoscience', 'environmental data', 'wlan', 'ofdm', 'error correction', 'tcp', 'udp', 'network traffic']
    text_to_check = ' '.join(str(row.get(c, '')).lower() for c in ['TI', 'SO', 'DE', 'ID', 'AB'])
    if any(keyword in text_to_check for keyword in exclusion_keywords): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in text_to_check for keyword in inclusion_keywords): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def convert_df_to_scimat_format(df_to_convert):
    wos_field_order = [
        'FN', 'VR', 'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP', 'EM', 'RI',
        'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA', 'SN', 'EI', 'J9', 'JI',
        'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'AR', 'DI', 'PG', 'WC', 'SC', 'GA', 'UT', 'PM', 'OA', 'EA', 'DA'
    ]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']
    all_records_text = []
    for _, row in df_to_convert.iterrows():
        record_lines = []
        for tag in wos_field_order:
            if tag in row.index and pd.notna(row[tag]) and str(row[tag]).strip() != '':
                value = str(row[tag])
                if tag in multi_line_fields:
                    items = [item.strip() for item in value.split(';') if item.strip()]
                    if items:
                        record_lines.append(f"{tag} {items[0]}")
                        record_lines.extend([f"   {item}" for item in items[1:]])
                else:
                    record_lines.append(f"{tag} {value}")
        if record_lines:
            record_lines.append("ER")
            all_records_text.append("\n".join(record_lines))
    header = "FN Clarivate Analytics Web of Science\nVR 1.0"
    final_content = header + "\n" + "\n".join(all_records_text)
    return final_content.encode('utf-8')

# --- Streamlit UI ë° ì‹¤í–‰ ë¡œì§ ---

# --- í—¤ë” ì„¹ì…˜ ---
st.title("âš™ï¸ WOS Prep")
st.subheader("Web of Science ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¶„ì„ ë„êµ¬")
st.caption("Streamlining the preparation of your bibliometric data for analysis.")
st.markdown("---")

# --- íŒŒì¼ ì—…ë¡œë“œ ì „ ì´ˆê¸° í™”ë©´ ---
if 'analysis_done' not in st.session_state:
    st.session_state.analysis_done = False

# íŒŒì¼ ì—…ë¡œë”
uploaded_file = st.file_uploader(
    "WOSì—ì„œ 'Tab-delimited' í˜•ì‹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•œ TXT ë˜ëŠ” CSV íŒŒì¼ì„ ì—¬ê¸°ì— ì—…ë¡œë“œí•˜ì„¸ìš”.",
    type=['csv', 'txt']
)

# íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ë©´ ì¦‰ì‹œ ë¶„ì„ ì‹œì‘
if uploaded_file is not None and not st.session_state.analysis_done:
    st.session_state.analysis_done = True # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
    # ë¶„ì„ ë¡œì§ì„ ì´ ë¸”ë¡ ì•ˆìœ¼ë¡œ ì´ë™
    with st.spinner("íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
        df = load_data(uploaded_file)
        if df is None:
            st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ 'Tab-delimited' ë˜ëŠ” 'Plain Text' í˜•ì‹ì˜ íŒŒì¼ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()

        df.columns = [col.upper().strip() for col in df.columns]
        column_mapping = {'AUTHORS': 'AU', 'ARTICLE TITLE': 'TI', 'SOURCE TITLE': 'SO', 'AUTHOR KEYWORDS': 'DE', 'KEYWORDS PLUS': 'ID', 'ABSTRACT': 'AB', 'CITED REFERENCES': 'CR', 'PUBLICATION YEAR': 'PY', 'TIMES CITED, ALL DATABASES': 'TC'}
        df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns}, inplace=True)
        
        # ì›ë³¸ ë°ì´í„° ë° ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥
        st.session_state.original_df = df.copy()
        
        if 'DE' in df.columns: df['DE_Original'] = df['DE']
        if 'ID' in df.columns: df['ID_Original'] = df['ID']

        df['Classification'] = df.apply(classify_article, axis=1)

        df_processed = df.copy()
        stop_words, lemmatizer = set(stopwords.words('english')), WordNetLemmatizer()
        custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article'}
        stop_words.update(custom_stop_words)
        include_mask = df_processed['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'
        if 'DE' in df_processed.columns: df_processed.loc[include_mask, 'DE'] = df_processed.loc[include_mask, 'DE'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))
        if 'ID' in df_processed.columns: df_processed.loc[include_mask, 'ID'] = df_processed.loc[include_mask, 'ID'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))
        
        st.session_state.processed_df = df_processed

# --- íŒŒì¼ ì—…ë¡œë“œ ì „/í›„ ê³µí†µ í‘œì‹œ ì˜ì—­ ---
if not st.session_state.analysis_done:
    st.markdown(
        """
        **WOS Prepì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤.**

        ì´ ë„êµ¬ëŠ” Web of Science(WOS)ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì„œì§€ ë°ì´í„°ë¥¼ **ë¶„ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì‹ ì†í•˜ê²Œ ì •ì œí•˜ê³  ë¶„ë¥˜**í•˜ê¸° ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
        ë³µì¡í•œ ë°ì´í„° ì¤€ë¹„ ê³¼ì •ì„ ìë™í™”í•˜ì—¬ ì—°êµ¬ ìƒì‚°ì„±ì„ ë†’ì´ê³ , **SciMATê³¼ ê°™ì€ ì „ë¬¸ ë¶„ì„ ë„êµ¬ì™€ì˜ ì™„ë²½í•œ í˜¸í™˜ì„±**ì„ ì œê³µí•©ë‹ˆë‹¤.
        """
    )

# --- ë¶„ì„ ì™„ë£Œ í›„ ê²°ê³¼ í‘œì‹œ ---
if st.session_state.analysis_done:
    st.success("âœ… ë¶„ì„ ë° ë³€í™˜ ì™„ë£Œ!")
    st.markdown("#### ë‹¨ê³„ 2: ë¶„ì„ ê²°ê³¼ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ")

    df = st.session_state.original_df
    df_processed = st.session_state.processed_df
    include_mask = df_processed['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'

    st.subheader("ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    
    # ìˆ˜ì •: ë…¼ë¬¸ ë¶„ë¥˜ ê²°ê³¼ì™€ ê·¸ë˜í”„ë¥¼ ìˆ˜ì§ìœ¼ë¡œ ë°°ì¹˜
    st.write("##### ë…¼ë¬¸ ë¶„ë¥˜ ê²°ê³¼")
    classification_counts = df['Classification'].value_counts().reset_index().rename(columns={'index': 'Classification', 'Classification': 'Count'})
    st.dataframe(classification_counts, use_container_width=True)

    # ìˆ˜ì •: Altair ì°¨íŠ¸ ì¤‘ì•™ì— ì „ì²´ ë…¼ë¬¸ ìˆ˜ ì¶”ê°€
    total_papers = len(df)
    base = alt.Chart(classification_counts).encode(
        theta=alt.Theta(field="Count", type="quantitative", stack=True),
        color=alt.Color(field="Classification", type="nominal", title="ë¶„ë¥˜"),
        tooltip=['Classification', 'Count']
    )
    pie = base.mark_arc(outerRadius=120, innerRadius=70)
    text = alt.Chart(pd.DataFrame({'total': [total_papers]})).mark_text(
        align='center', baseline='middle', fontSize=24, fontWeight='bold', color="#4A4A4A"
    ).encode(text='total:N').properties(title=alt.TitleParams(text="ë…¼ë¬¸ ë¶„ë¥˜ ë¶„í¬", anchor='middle'))
    st.altair_chart(pie + text, use_container_width=True)
    
    st.markdown("---")

    st.write("##### 'ê´€ë ¨ì—°êµ¬(Include)' ì£¼ìš” í‚¤ì›Œë“œ (ì „ì²˜ë¦¬ í›„)")
    all_keywords = []
    if 'DE' in df_processed.columns: all_keywords.extend([kw.strip() for text in df_processed.loc[include_mask, 'DE'].dropna() for kw in text.split(';') if kw.strip()])
    if 'ID' in df_processed.columns: all_keywords.extend([kw.strip() for text in df_processed.loc[include_mask, 'ID'].dropna() for kw in text.split(';') if kw.strip()])
    if all_keywords:
        df_keywords = pd.DataFrame(Counter(all_keywords).most_common(20), columns=['Keyword', 'Frequency'])
        keyword_chart = alt.Chart(df_keywords).mark_bar().encode(x=alt.X('Frequency:Q', title='ë¹ˆë„'), y=alt.Y('Keyword:N', title='í‚¤ì›Œë“œ', sort='-x')).properties(title='ìƒìœ„ 20 í‚¤ì›Œë“œ ë¹ˆë„')
        st.altair_chart(keyword_chart, use_container_width=True)
    
    st.markdown("---")

    # ìˆ˜ì •: st.expander ì œê±°í•˜ê³  í•­ìƒ ë³´ì´ë„ë¡ ë³€ê²½
    st.write("##### ğŸ”¬ ì •ê·œí™” ì „/í›„ í‚¤ì›Œë“œ ë¹„êµ (ìƒìœ„ 5ê°œ ê´€ë ¨ ì—°êµ¬)")
    compare_cols = []
    if 'DE_Original' in df_processed.columns: compare_cols.extend(['DE_Original', 'DE'])
    if 'ID_Original' in df_processed.columns: compare_cols.extend(['ID_Original', 'ID'])
    if compare_cols:
        st.dataframe(df_processed.loc[include_mask, compare_cols].head(5), use_container_width=True)
    else:
        st.info("ë¹„êµí•  í‚¤ì›Œë“œ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ìˆ˜ì •: ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ìµœí•˜ë‹¨ìœ¼ë¡œ ì´ë™ ë° ìˆ˜ì§ ë°°ì¹˜
    st.markdown("---")
    st.subheader("ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
    
    st.write("**1. SciMAT ë¶„ì„ìš© íŒŒì¼ (ê¶Œì¥)**")
    st.caption("Scimatì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì›ë³¸ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
    df_for_scimat = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
    scimat_output_df = df_for_scimat.drop(columns=[col for col in ['DE_Original', 'ID_Original', 'Classification'] if col in df_for_scimat.columns])
    text_data_scimat = convert_df_to_scimat_format(scimat_output_df)
    st.download_button(label="ğŸ“¥ **ì›ë³¸ íŒŒì¼** ë‹¤ìš´ë¡œë“œ (.txt for SciMAT)", data=text_data_scimat, file_name="wos_original_for_scimat.txt", mime="text/plain", key="scimat_download", use_container_width=True)

    st.write("") # ê°„ê²© ì¶”ê°€

    st.write("**2. ì „ì²˜ë¦¬ ì™„ë£Œ íŒŒì¼**")
    st.caption("í‚¤ì›Œë“œ ì •ì œ/ì •ê·œí™”ê°€ ì™„ë£Œëœ ë°ì´í„°ì…ë‹ˆë‹¤. ì™¸ë¶€ ì‹œê°í™”ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    df_for_viz = df_processed[df_processed['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
    viz_output_df = df_for_viz.drop(columns=[col for col in ['DE_Original', 'ID_Original', 'Classification'] if col in df_for_viz.columns])
    text_data_viz = convert_df_to_scimat_format(viz_output_df)
    st.download_button(label="ğŸ“¥ **ì „ì²˜ë¦¬ëœ íŒŒì¼** ë‹¤ìš´ë¡œë“œ (.txt)", data=text_data_viz, file_name="wos_preprocessed.txt", mime="text/plain", key="preprocessed_download", use_container_width=True)

# --- ìˆ˜ì •: ì•± í•˜ë‹¨ì— ê°œë°œì ì •ë³´ ë° ë²„ì „ í‘œê¸° ---
st.markdown("---")
st.caption("Developed by ì„íƒœê²½, í•œì–‘ëŒ€í•™êµ ê¸°ìˆ ê²½ì˜ì „ë¬¸ëŒ€í•™ì› | Version 1.0.0")
