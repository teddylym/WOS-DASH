import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt
import io

# --- 1. í˜ì´ì§€ ì„¤ì • ë° ê¸°ë³¸ ìŠ¤íƒ€ì¼ ---
st.set_page_config(
    page_title="WOS Prep | Professional Dashboard",
    layout="wide",  # ë„“ì€ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ë³€ê²½
    initial_sidebar_state="collapsed"
)

# --- ì‚¬ìš©ì ì •ì˜ CSS ---
st.markdown("""
<style>
    /* ê¸°ë³¸ í°íŠ¸ ë° ìƒ‰ìƒ ì„¤ì • */
    body {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        background-color: #F0F2F5;
    }
    /* ê¸°ë³¸ ìƒ‰ìƒ ë³€ìˆ˜ */
    :root {
        --primary-color: #0096FF;
        --text-color: #333;
        --light-text-color: #6c757d;
        --bg-color: #FFFFFF;
        --border-color: #EAEAEA;
    }
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    .stDeployButton { display: none; }
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .main .block-container {
        padding: 2rem;
        max-width: 1400px;
    }
    /* ì¹´ë“œ ìŠ¤íƒ€ì¼ */
    .metric-card {
        background-color: var(--bg-color);
        border: 1px solid var(--border-color);
        border-radius: 10px;
        padding: 1.5rem;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0,0,0,0.04);
        transition: all 0.3s ease-in-out;
    }
    .metric-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 12px 16px rgba(0,0,0,0.08);
    }
    .metric-card h3 {
        color: var(--light-text-color);
        font-size: 1rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
    }
    .metric-card p {
        color: var(--text-color);
        font-size: 2.25rem;
        font-weight: 700;
        margin: 0;
    }
    /* ì°¨íŠ¸ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chart-container {
        background-color: var(--bg-color);
        border: 1px solid var(--border-color);
        border-radius: 10px;
        padding: 2rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.04);
    }
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    h1, h2, h3, h4, h5, h6, .st-emotion-cache-10trblm, .st-emotion-cache-1kyxreq {
        color: var(--text-color) !important;
    }
    .st-emotion-cache-1kyxreq {
        font-weight: 600;
        font-size: 1.5rem;
        border-bottom: 2px solid var(--primary-color);
        padding-bottom: 0.5rem;
        margin-bottom: 1.5rem;
    }
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton>button {
        background-color: var(--primary-color);
        color: white;
        border-radius: 8px;
        border: none;
        padding: 0.75rem 1.5rem;
        font-weight: 600;
    }
    /* íŒŒì¼ ì—…ë¡œë” ìŠ¤íƒ€ì¼ */
    .stFileUploader {
        background-color: var(--bg-color);
        border: 2px dashed var(--primary-color);
        border-radius: 10px;
        padding: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ìºì‹œ ìœ ì§€) ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- 2. ë°±ì—”ë“œ ê¸°ëŠ¥ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def build_normalization_map():
    base_map = {
        "machine learning": ["machine-learning", "ml", "machinelearning"], "artificial intelligence": ["ai"],
        "deep learning": ["deep-learning", "dnn"], "neural networks": ["neural network", "nn"],
        "natural language processing": ["nlp"], "computer vision": ["cv"], "reinforcement learning": ["rl"],
        "live streaming": ["live-streaming", "livestreaming"], "video streaming": ["video-streaming"],
        "social media": ["social-media"], "user experience": ["ux"], "user behavior": ["user-behavior"],
        "content creation": ["content-creation"], "digital marketing": ["digital-marketing"],
        "e commerce": ["ecommerce", "e-commerce"], "data mining": ["data-mining"], "big data": ["big-data"],
        "data analysis": ["data-analysis"], "sentiment analysis": ["sentiment-analysis"],
        "statistical analysis": ["statistical-analysis"], "structural equation modeling": ["sem", "pls-sem"],
        "cloud computing": ["cloud-computing"], "internet of things": ["iot"],
        "mobile applications": ["mobile apps", "mobile app"], "web development": ["web-development"],
        "software engineering": ["software-engineering"]
    }
    reverse_map = {}
    for standard, variations in base_map.items():
        reverse_map[standard] = standard
        for v in variations:
            reverse_map[v.replace(" ", "").replace("-", "")] = standard
            reverse_map[v] = standard
    return reverse_map
NORMALIZATION_MAP = build_normalization_map()

def normalize_keyword_phrase(phrase):
    return NORMALIZATION_MAP.get(phrase.lower().replace(" ", "").replace("-", ""), phrase.lower().strip())

def load_data(uploaded_file):
    file_bytes = uploaded_file.getvalue()
    for encoding in ['utf-8-sig', 'utf-8', 'latin1', 'cp949']:
        try:
            df = pd.read_csv(io.StringIO(file_bytes.decode(encoding)), sep='\t', lineterminator='\n')
            if df.shape[1] > 1: return df
        except Exception: continue
    for encoding in ['utf-8-sig', 'utf-8', 'latin1', 'cp949']:
        try:
            df = pd.read_csv(io.StringIO(file_bytes.decode(encoding)))
            if df.shape[1] > 1: return df
        except Exception: continue
    return None

def classify_article(row):
    text = ' '.join(str(row.get(c, '')).lower() for c in ['TI', 'SO', 'DE', 'ID', 'AB'])
    if any(k in text for k in ['protocol', 'network coding', 'wimax', 'mac layer', 'bandwidth', 'tcp', 'udp']): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(k in text for k in ['user', 'viewer', 'behavior', 'experience', 'engagement', 'motivation', 'social', 'commerce']): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    if pd.isna(keywords_str): return ""
    cleaned = set()
    for kw in keywords_str.split(';'):
        norm_kw = normalize_keyword_phrase(kw)
        words = re.sub(r'[^a-z\s]', '', norm_kw).split()
        lemmatized = [lemmatizer.lemmatize(w) for w in words if w not in stop_words and len(w) > 2]
        if lemmatized:
            cleaned.add(" ".join(lemmatized))
    return '; '.join(sorted(list(cleaned)))

def convert_df_to_scimat_format(df):
    # ... (ê¸°ì¡´ í•¨ìˆ˜ì™€ ë™ì¼, ìƒëµ)
    return "FN Clarivate Analytics Web of Science\nVR 1.0\n\n" + "\n\n".join(
        "\n".join(f"{tag} {val}" for tag, val in row.items() if pd.notna(val)) + "\nER"
        for _, row in df.iterrows()
    )


# --- 3. UI ë Œë”ë§ ---
st.title("WOS Prep Dashboard")
st.markdown("<p style='margin-top:-1rem; color: var(--light-text-color)'>Web of Science ë°ì´í„° ìë™ ì „ì²˜ë¦¬ ë° ì‹œê°í™” ë„êµ¬</p>", unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
with st.container():
    st.markdown("### ğŸ“ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader(
        "Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ Tab-delimited ë˜ëŠ” Plain Text í˜•ì‹ì˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
        type=['csv', 'txt'],
        label_visibility="collapsed"
    )

if uploaded_file is None:
    st.info("ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    st.stop()

# --- ë°ì´í„° ì²˜ë¦¬ ---
df = load_data(uploaded_file)
if df is None:
    st.error("âŒ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# ì»¬ëŸ¼ëª… ë§¤í•‘
column_mapping = {
    'Authors': 'AU', 'Article Title': 'TI', 'Source Title': 'SO', 'Author Keywords': 'DE',
    'Keywords Plus': 'ID', 'Abstract': 'AB', 'Cited References': 'CR', 'Publication Year': 'PY',
    'Times Cited, All Databases': 'TC', 'Cited Reference Count': 'NR', 'Times Cited, WoS Core': 'Z9'
}
df.rename(columns=column_mapping, inplace=True)

# ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ ì‹¤í–‰
with st.spinner("ğŸ”„ ë°ì´í„° ë¶„ì„ ì¤‘..."):
    df['Classification'] = df.apply(classify_article, axis=1)
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    include_mask = df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'
    if 'DE' in df.columns:
        df['DE_cleaned'] = df['DE'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))
    if 'ID' in df.columns:
        df['ID_cleaned'] = df['ID'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))

# --- ìƒë‹¨ í†µê³„ ì¹´ë“œ ---
st.markdown("### ğŸ“Š Stats Overview")
total_papers = len(df)
final_papers = len(df[df['Classification'] != 'Exclude (ì œì™¸ì—°êµ¬)'])
included_papers = df['Classification'].value_counts().get('Include (ê´€ë ¨ì—°êµ¬)', 0)
reviewed_papers = df['Classification'].value_counts().get('Review (ê²€í† í•„ìš”)', 0)

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.markdown(f'<div class="metric-card"><h3>ì´ ë…¼ë¬¸ ìˆ˜</h3><p>{total_papers}</p></div>', unsafe_allow_html=True)
with col2:
    st.markdown(f'<div class="metric-card"><h3>ìµœì¢… ë¶„ì„ ëŒ€ìƒ</h3><p>{final_papers}</p></div>', unsafe_allow_html=True)
with col3:
    st.markdown(f'<div class="metric-card"><h3>ê´€ë ¨ ì—°êµ¬</h3><p>{included_papers}</p></div>', unsafe_allow_html=True)
with col4:
    st.markdown(f'<div class="metric-card"><h3>ê²€í†  í•„ìš”</h3><p>{reviewed_papers}</p></div>', unsafe_allow_html=True)

st.markdown("<br>", unsafe_allow_html=True)

# --- ì¤‘ì•™ ë¶„ì„ ì„¹ì…˜ ---
col1, col2 = st.columns(2)
with col1:
    with st.container(border=True):
        st.markdown("<h5 style='text-align:center;'>ë…¼ë¬¸ ë¶„ë¥˜ ë¶„í¬</h5>", unsafe_allow_html=True)
        classification_counts = df['Classification'].value_counts().reset_index()
        classification_counts.columns = ['ë¶„ë¥˜', 'ë…¼ë¬¸ ìˆ˜']
        
        donut_chart = alt.Chart(classification_counts).mark_arc(innerRadius=70, outerRadius=110).encode(
            theta=alt.Theta(field="ë…¼ë¬¸ ìˆ˜", type="quantitative"),
            color=alt.Color(field="ë¶„ë¥˜", type="nominal", scale=alt.Scale(
                domain=['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)', 'Exclude (ì œì™¸ì—°êµ¬)'],
                range=['#0096FF', '#7F7F7F', '#D3D3D3']
            ), legend=alt.Legend(title="ë¶„ë¥˜", orient="bottom")),
            tooltip=['ë¶„ë¥˜', 'ë…¼ë¬¸ ìˆ˜']
        ).properties(height=300)
        st.altair_chart(donut_chart, use_container_width=True)

with col2:
    with st.container(border=True):
        st.markdown("<h5 style='text-align:center;'>ì—°ë„ë³„ ì—°êµ¬ ë™í–¥</h5>", unsafe_allow_html=True)
        if 'PY' in df.columns:
            df_trend = df.copy()
            df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
            yearly_counts = df_trend.dropna(subset=['PY'])['PY'].astype(int).value_counts().sort_index().reset_index()
            yearly_counts.columns = ['Year', 'Count']
            
            line_chart = alt.Chart(yearly_counts).mark_line(point=True, color='#0096FF').encode(
                x=alt.X('Year:O', title='ë°œí–‰ ì—°ë„'),
                y=alt.Y('Count:Q', title='ë…¼ë¬¸ ìˆ˜'),
                tooltip=['Year', 'Count']
            ).properties(height=300)
            st.altair_chart(line_chart, use_container_width=True)
        else:
            st.warning("ë°œí–‰ ì—°ë„(PY) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("<br>", unsafe_allow_html=True)

# --- ì£¼ìš” ì¸ìš© ë° í‚¤ì›Œë“œ ë¶„ì„ ---
with st.container(border=True):
    st.markdown("<h5 style='text-align:center;'>ì£¼ìš” ì¸ìš© ë° í‚¤ì›Œë“œ ë¶„ì„</h5>", unsafe_allow_html=True)
    tab1, tab2 = st.tabs(["ì£¼ìš” ì¸ìš© ë…¼ë¬¸", "ì£¼ìš” í‚¤ì›Œë“œ"])
    
    with tab1:
        if 'NR' in df.columns:
            df_cited = df.copy()
            df_cited['NR'] = pd.to_numeric(df_cited['NR'], errors='coerce').fillna(0)
            df_cited = df_cited.sort_values(by='NR', ascending=False).head(5)
            df_cited['Label'] = df_cited['AU'].str.split(';').str[0] + " - " + df_cited['TI'].str[:40] + '...'
            
            cited_chart = alt.Chart(df_cited).mark_bar(color='#0096FF').encode(
                x=alt.X('NR:Q', title='ì°¸ê³ ë¬¸í—Œ ìˆ˜'),
                y=alt.Y('Label:N', title='ë…¼ë¬¸', sort='-x'),
                tooltip=['TI', 'AU', 'NR']
            ).properties(height=300)
            st.altair_chart(cited_chart, use_container_width=True)
        else:
            st.warning("ì°¸ê³ ë¬¸í—Œ ìˆ˜(NR) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tab2:
        all_keywords = []
        if 'DE_cleaned' in df.columns:
            all_keywords.extend(df.loc[include_mask, 'DE_cleaned'].dropna().str.split(';').explode())
        if 'ID_cleaned' in df.columns:
            all_keywords.extend(df.loc[include_mask, 'ID_cleaned'].dropna().str.split(';').explode())
        
        if all_keywords:
            top_keywords_df = pd.DataFrame(Counter(all_keywords).most_common(20), columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
            top_3_keywords = top_keywords_df['í‚¤ì›Œë“œ'].head(3).tolist()
            
            y_sort = alt.SortField(field='ë¹ˆë„', order='descending')

            line = alt.Chart(top_keywords_df).mark_rule(size=2, color='#0096FF').encode(
                x='ë¹ˆë„:Q',
                y=alt.Y('í‚¤ì›Œë“œ:N', sort=y_sort)
            )
            
            lollipop_chart = alt.Chart(top_keywords_df).mark_point(filled=True, size=100).encode(
                x=alt.X('ë¹ˆë„:Q', title='ë¹ˆë„'),
                y=alt.Y('í‚¤ì›Œë“œ:N', sort=y_sort, title=None),
                color=alt.condition(
                    alt.FieldOneOfPredicate(field='í‚¤ì›Œë“œ', oneOf=top_3_keywords),
                    alt.value('#FF6B6B'), alt.value('#0096FF')
                ),
                tooltip=['í‚¤ì›Œë“œ', 'ë¹ˆë„']
            )
            
            st.altair_chart((line + lollipop_chart).properties(height=500), use_container_width=True)
        else:
            st.warning("ê´€ë ¨ì—°êµ¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ë‹¤ìš´ë¡œë“œ ---
st.markdown("<br>", unsafe_allow_html=True)
with st.expander("ğŸ“‹ ì²˜ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ë‹¤ìš´ë¡œë“œ"):
    df_final = df[df['Classification'] != 'Exclude (ì œì™¸ì—°êµ¬)'].copy()
    for col in ['DE', 'ID']:
        if f'{col}_cleaned' in df_final.columns:
            df_final[col] = df_final[f'{col}_cleaned']
    
    cols_to_drop = [c for c in ['Classification', 'DE_cleaned', 'ID_cleaned', 'DE_Original', 'ID_Original'] if c in df_final.columns]
    df_final_output = df_final.drop(columns=cols_to_drop)
    
    st.dataframe(df_final_output.head(10))
    
    csv = df_final_output.to_csv(index=False).encode('utf-8')
    scimat_text = convert_df_to_scimat_format(df_final_output)

    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=csv,
            file_name="wos_preprocessed_data.csv", mime="text/csv", use_container_width=True
        )
    with col2:
        st.download_button(
            label="ğŸ“¥ SciMAT í˜¸í™˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=scimat_text,
            file_name="wos_for_scimat.txt", mime="text/plain", use_container_width=True
        )

# --- í•˜ë‹¨ ì—¬ë°± ---
st.markdown("<br><br>", unsafe_allow_html=True)
