import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import io
import altair as alt

# --- í˜ì´ì§€ ì„¤ì • (Page Config) ---
st.set_page_config(page_title="WOS Analysis Dashboard", layout="centered")

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ìºì‹œ í™œìš©) ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ì¸ì½”ë”© ë¬¸ì œ í•´ê²°) ---
@st.cache_data
def load_data(uploaded_file):
    encodings_to_try = ['utf-8', 'latin1', 'cp949', 'utf-8-sig']
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, sep='\t', encoding=encoding)
            if df.shape[1] > 2: return df
        except Exception:
            continue
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding=encoding)
            if df.shape[1] > 2: return df
        except Exception:
            continue
    return None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ì •ì˜ (Core Functions) ---
def classify_article(row):
    inclusion_keywords = [
        'user', 'viewer', 'audience', 'streamer', 'consumer', 'participant',
        'behavior', 'experience', 'engagement', 'interaction', 'motivation',
        'psychology', 'social', 'community', 'cultural', 'society',
        'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization',
        'education', 'learning', 'influencer'
    ]
    exclusion_keywords = [
        'protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer',
        'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput',
        'sensor data', 'geoscience', 'environmental data', 'wlan',
        'ofdm', 'error correction', 'tcp', 'udp', 'network traffic'
    ]
    title = str(row.get('Article Title', row.get('TI', ''))).lower()
    abstract = str(row.get('Abstract', row.get('AB', ''))).lower()
    author_keywords = str(row.get('Author Keywords', row.get('DE', ''))).lower()
    keywords_plus = str(row.get('Keywords Plus', row.get('ID', ''))).lower()
    full_text = ' '.join([title, abstract, author_keywords, keywords_plus])
    if any(keyword in full_text for keyword in exclusion_keywords):
        return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in full_text for keyword in inclusion_keywords):
        return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def preprocess_keywords(row, stop_words, lemmatizer):
    if row['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)':
        author_keywords = str(row.get('Author Keywords', row.get('DE', ''))).lower()
        keywords_plus = str(row.get('Keywords Plus', row.get('ID', ''))).lower()
        all_keywords_str = author_keywords + ';' + keywords_plus
        all_keywords = all_keywords_str.split(';')
        cleaned_keywords = set()
        for keyword in all_keywords:
            keyword = keyword.strip().replace('-', ' ')
            keyword = re.sub(r'[^a-z\s]', '', keyword)
            final_words = [lemmatizer.lemmatize(w) for w in keyword.split() if w not in stop_words and len(w) > 2]
            if final_words:
                cleaned_keywords.add(" ".join(final_words))
        return '; '.join(sorted(list(cleaned_keywords)))
    return None

# --- ì¹´ë“œ ìŠ¤íƒ€ì¼ì„ ìœ„í•œ CSS ---
st.markdown("""
<style>
    div[data-testid="stVerticalBlock"] > [data-testid="stHorizontalBlock"] > div[data-testid="stVerticalBlock"] {
        border: 1px solid #e6e6e6; border-radius: 10px; padding: 25px; 
        box-shadow: 0 4px 8px rgba(0,0,0,0.05); background-color: #ffffff;
    }
</style>
""", unsafe_allow_html=True)

# --- Streamlit UI ë° ì‹¤í–‰ ë¡œì§ ---
st.title("WOS ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.caption("WOS Data Analysis Dashboard")

uploaded_file = st.file_uploader(
    "WoS Raw Data íŒŒì¼(CSV/TXT)ì„ ì—…ë¡œë“œí•˜ì„¸ìš” / Upload your WoS Raw Data file (CSV/TXT)", 
    type=['csv', 'txt']
)

# --- íŒŒì¼ ì—…ë¡œë“œ ì‹œ ìë™ ë¶„ì„ ì‹¤í–‰ ---
if uploaded_file is not None:
    df_raw = load_data(uploaded_file)
    
    if df_raw is None:
        st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    with st.spinner("íŒŒì¼ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... / Analyzing file..."):
        # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬í•˜ì—¬ ë¶„ì„ ì§„í–‰
        df = df_raw.copy()

        # 1. ë…¼ë¬¸ ë¶„ë¥˜ ì‹¤í–‰
        df['Classification'] = df.apply(classify_article, axis=1)
        
        # 2. í‚¤ì›Œë“œ ì „ì²˜ë¦¬ ì‹¤í–‰
        stop_words = set(stopwords.words('english'))
        custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article', 'articles', 'based', 'using'}
        stop_words.update(custom_stop_words)
        lemmatizer = WordNetLemmatizer()
        df['Cleaned_Keywords'] = df.apply(lambda row: preprocess_keywords(row, stop_words, lemmatizer), axis=1)

    st.success("âœ… ë¶„ì„ ì™„ë£Œ! / Analysis Complete!")
    
    # --- ê²°ê³¼ í‘œì‹œ ---
    st.markdown("---")
    with st.container():
        st.subheader("1. ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        st.caption("Analysis Summary")
        
        classification_summary = df['Classification'].value_counts()
        
        # 'ê´€ë ¨ì—°êµ¬'ì—ì„œ ì •ì œëœ í‚¤ì›Œë“œ ìˆ˜ ê³„ì‚°
        all_cleaned_keywords = []
        df[df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)']['Cleaned_Keywords'].dropna().apply(lambda x: all_cleaned_keywords.extend(x.split(';') if isinstance(x, str) else []))
        all_cleaned_keywords = [kw.strip() for kw in all_cleaned_keywords if kw.strip()]
        num_unique_keywords = len(set(all_cleaned_keywords))

        # 2x2 ê·¸ë¦¬ë“œë¡œ ìš”ì•½ ì •ë³´ í‘œì‹œ
        col1, col2 = st.columns(2)
        col3, col4 = st.columns(2)
        
        with col1:
            st.metric("â–¶ï¸ ê´€ë ¨ì—°êµ¬ / Include", classification_summary.get('Include (ê´€ë ¨ì—°êµ¬)', 0))
        with col2:
            st.metric("â–¶ï¸ ì œì™¸ì—°êµ¬ / Exclude", classification_summary.get('Exclude (ì œì™¸ì—°êµ¬)', 0))
        with col3:
            st.metric("â–¶ï¸ ê²€í† í•„ìš” / Review", classification_summary.get('Review (ê²€í† í•„ìš”)', 0))
        with col4:
            st.metric("â–¶ï¸ ì •ì œëœ í‚¤ì›Œë“œ ìˆ˜ / Unique Keywords", num_unique_keywords)


    st.markdown("---")
    with st.container():
        st.subheader("2. 'ê´€ë ¨ì—°êµ¬' í•µì‹¬ í‚¤ì›Œë“œ")
        st.caption("Core Keywords from 'Include' Group (Top 20)")
        
        keyword_counts = Counter(all_cleaned_keywords)
        
        if keyword_counts:
            top_keywords_df = pd.DataFrame(keyword_counts.most_common(20), columns=['Keyword', 'Frequency'])
            
            chart = alt.Chart(top_keywords_df).mark_bar(
                cornerRadius=3, height=20 
            ).encode(
                x=alt.X('Frequency:Q', title='ë¹ˆë„ (Frequency)'),
                y=alt.Y('Keyword:N', title='í‚¤ì›Œë“œ (Keyword)', sort='-x'),
                color=alt.value('#4F8BFF'), 
                tooltip=['Keyword', 'Frequency']
            ).properties(
                title='í•µì‹¬ í‚¤ì›Œë“œ ìƒìœ„ 20ê°œ (Top 20 Core Keywords)'
            )
            st.altair_chart(chart, use_container_width=True)
        else:
            st.write("'ê´€ë ¨ì—°êµ¬'ë¡œ ë¶„ë¥˜ëœ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # --- ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ---
    st.markdown("---")
    @st.cache_data
    def convert_df_to_csv(df_to_convert):
        return df_to_convert.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')

    csv_data = convert_df_to_csv(df)
    
    st.download_button(
       label="ğŸ“¥ ì²˜ë¦¬ëœ ì „ì²´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ / Download Processed File (CSV)",
       data=csv_data,
       file_name="wos_processed_data.csv",
       mime="text/csv",
    )
