# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import altair as alt
import io

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Prep | SCIMAT Edition",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
    /* ì—¬ê¸°ì— ê¸°ì¡´ CSS ì½”ë“œë¥¼ ëª¨ë‘ ë¶™ì—¬ë„£ìœ¼ì„¸ìš” */
    .main-container {
        background: #f8f9fa;
        min-height: 100vh;
    }
    .metric-card {
        background: white;
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        margin-bottom: 16px;
        transition: all 0.3s ease;
    }
    .metric-card:hover {
        box-shadow: 0 4px 20px rgba(0,56,117,0.15);
        border-color: #003875;
    }
    .metric-value {
        font-size: 2.8rem;
        font-weight: 700;
        color: #003875;
        margin: 0;
        line-height: 1;
    }
    .metric-label {
        font-size: 1rem;
        color: #6c757d;
        margin: 8px 0 0 0;
        font-weight: 500;
    }
    .metric-icon {
        background: linear-gradient(135deg, #003875, #0056b3);
        color: white;
        width: 48px;
        height: 48px;
        border-radius: 12px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.5rem;
        margin-bottom: 16px;
    }
    .chart-container {
        background: white;
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        margin: 16px 0;
    }
    .chart-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #212529;
        margin-bottom: 16px;
        border-bottom: 2px solid #003875;
        padding-bottom: 8px;
    }
    .section-header {
        background: linear-gradient(135deg, #003875, #0056b3);
        color: white;
        padding: 20px 24px;
        border-radius: 12px;
        margin: 24px 0 16px 0;
        box-shadow: 0 4px 16px rgba(0,56,117,0.2);
    }
    .section-title {
        font-size: 1.5rem;
        font-weight: 600;
        margin: 0;
    }
    .section-subtitle {
        font-size: 1rem;
        opacity: 0.9;
        margin: 4px 0 0 0;
    }
    .upload-zone {
        background: white;
        border: 2px dashed #003875;
        border-radius: 12px;
        padding: 40px 20px;
        text-align: center;
        margin: 20px 0;
        transition: all 0.3s ease;
    }
    .upload-zone:hover {
        background: #f8f9fa;
        border-color: #0056b3;
    }
</style>
""", unsafe_allow_html=True)


# --- ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---

def parse_wos_format(content):
    """WOS Plain Text í˜•ì‹ì„ DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    records = []
    current_record = {}
    current_field = None
    # 'PT', 'AU' ë“± WOS íƒœê·¸ëŠ” ë‘ ê¸€ìë¡œ ê³ ì •ë˜ì–´ ìˆìŒ
    field_tag_pattern = re.compile(r"^([A-Z0-9]{2})\s(.*)$")

    lines = content.split('\n')
    for line in lines:
        line = line.rstrip()
        if not line:
            continue

        if line == 'ER':
            if current_record:
                records.append(current_record)
            current_record = {}
            current_field = None
            continue

        if line.startswith('   '): # ì—°ì†ë˜ëŠ” í•„ë“œ ê°’ ì²˜ë¦¬
            if current_field in current_record:
                # ì—¬ëŸ¬ ì €ì, í‚¤ì›Œë“œ ë“±ì„ ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ì¶”ê°€
                current_record[current_field] += '; ' + line.strip()
        else:
            match = field_tag_pattern.match(line)
            if match:
                current_field, value = match.groups()
                current_record[current_field] = value.strip()

    if current_record: # ë§ˆì§€ë§‰ ë ˆì½”ë“œ ì¶”ê°€
        records.append(current_record)

    if not records:
        return None
    return pd.DataFrame(records)


def load_and_merge_wos_files(uploaded_files):
    """ë‹¤ì¤‘ WOS íŒŒì¼ì„ ë³‘í•©í•˜ê³ , ê°•í™”ëœ ì¤‘ë³µ ì œê±° ë¡œì§ì„ ì ìš©í•©ë‹ˆë‹¤."""
    all_dataframes = []
    file_status = []

    for uploaded_file in uploaded_files:
        try:
            file_bytes = uploaded_file.getvalue()
            detected_encoding = None
            decoded_content = None

            for encoding in ['utf-8-sig', 'utf-8', 'latin1', 'iso-8859-1']:
                try:
                    decoded_content = file_bytes.decode(encoding)
                    # íŒŒì¼ ì‹œì‘ ë¶€ë¶„ì´ WOS í˜•ì‹ì¸ì§€ ê°„ë‹¨íˆ í™•ì¸
                    if decoded_content.strip().startswith('FN '):
                        detected_encoding = encoding
                        break
                except UnicodeDecodeError:
                    continue

            if not decoded_content or not detected_encoding:
                file_status.append({
                    'filename': uploaded_file.name, 'status': 'ERROR', 'message': 'âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ë˜ëŠ” ì¸ì½”ë”©ì…ë‹ˆë‹¤.'
                })
                continue

            df = parse_wos_format(decoded_content)
            if df is not None and not df.empty:
                all_dataframes.append(df)
                file_status.append({
                    'filename': uploaded_file.name, 'status': 'SUCCESS', 'message': f'âœ… {len(df)}í¸ ë…¼ë¬¸ ë¡œë”© ì„±ê³µ (ì¸ì½”ë”©: {detected_encoding})'
                })
            else:
                 file_status.append({
                    'filename': uploaded_file.name, 'status': 'ERROR', 'message': 'âŒ WOS Plain Text í˜•ì‹ì´ ì•„ë‹ˆê±°ë‚˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.'
                })

        except Exception as e:
            file_status.append({
                'filename': uploaded_file.name, 'status': 'ERROR', 'message': f'âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}'
            })

    if not all_dataframes:
        return None, file_status, 0

    merged_df = pd.concat(all_dataframes, ignore_index=True)
    original_count = len(merged_df)

    # 1ë‹¨ê³„: UT (ê³ ìœ  ì‹ë³„ì) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    if 'UT' in merged_df.columns:
        # UT ê°’ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ NaNì„ ê³ ìœ í•œ ê°’ìœ¼ë¡œ ì²˜ë¦¬
        merged_df['UT'] = merged_df['UT'].fillna(pd.NA)
        merged_df.dropna(subset=['UT'], inplace=True)
        merged_df.drop_duplicates(subset=['UT'], keep='first', inplace=True)

    # 2ë‹¨ê³„: UTê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì œëª©(TI)ê³¼ ì €ì(AU) ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µ ì œê±°
    if 'TI' in merged_df.columns and 'AU' in merged_df.columns:
        # ì €ì ëª©ë¡ì˜ ìˆœì„œë‚˜ ê³µë°± ì°¨ì´ë¥¼ ë¬´ì‹œí•˜ê¸° ìœ„í•´ ì •ê·œí™”
        merged_df['AU_normalized'] = merged_df['AU'].str.lower().str.replace(r'[^a-z]', '', regex=True)
        merged_df.drop_duplicates(subset=['TI', 'AU_normalized'], keep='first', inplace=True)
        merged_df.drop(columns=['AU_normalized'], inplace=True)

    final_count = len(merged_df)
    duplicates_removed = original_count - final_count

    return merged_df, file_status, duplicates_removed

def classify_article(row):
    """ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ë¥¼ ìœ„í•œ í¬ê´„ì  ë¶„ë¥˜ í•¨ìˆ˜."""
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì†Œë¬¸ì ë³€í™˜
    def extract_text(value):
        return str(value).lower() if pd.notna(value) else ""

    title = extract_text(row.get('TI'))
    keywords = extract_text(row.get('DE')) + " " + extract_text(row.get('ID'))
    abstract = extract_text(row.get('AB'))
    full_text = title + " " + keywords + " " + abstract

    # ë¶„ë¥˜ í‚¤ì›Œë“œ ì •ì˜ (ë” ì •êµí•˜ê²Œ)
    core_streaming = ['live stream', 'livestream', 'live video', 'real-time stream', 'live commerce', 'streamer']
    related_digital = ['social media', 'e-commerce', 'influencer', 'digital marketing', 'online education', 'telemedicine']
    technical_exclude = ['routing protocol', 'network topology', 'vlsi design', 'antenna design', 'signal processing']

    # ë¶„ë¥˜ ë¡œì§
    if any(keyword in full_text for keyword in technical_exclude):
        return 'Exclude (ê¸°ìˆ ì  ë¹„ê´€ë ¨)'
    if any(keyword in full_text for keyword in core_streaming):
        return 'Include (í•µì‹¬ì—°êµ¬)'
    if any(keyword in full_text for keyword in related_digital):
        # 'live', 'real-time' ë“±ê³¼ í•¨ê»˜ ë‚˜íƒ€ë‚˜ë©´ ê´€ë ¨ ì—°êµ¬ë¡œ ê°„ì£¼
        if 'live' in full_text or 'real-time' in full_text or 'interactive' in full_text:
             return 'Include (ê´€ë ¨ì—°êµ¬)'
        else:
             return 'Review (ê´€ë ¨ì„± ê²€í† )'
    return 'Review (ë¶„ë¥˜ ë¶ˆí™•ì‹¤)'

def convert_to_scimat_wos_format(df_to_convert):
    """DataFrameì„ SCIMAT í˜¸í™˜ WOS Plain Text í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # SCIMATì´ ì¸ì‹í•˜ëŠ” í‘œì¤€ í•„ë“œ ìˆœì„œ
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'RP',
        'EM', 'FU', 'FX', 'CR', 'NR', 'TC', 'PU', 'PI', 'PA', 'SN', 'J9', 'JI',
        'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'PG', 'WC', 'SC', 'GA', 'UT'
    ]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'CR']

    content_lines = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    for _, row in df_to_convert.iterrows():
        # ë ˆì½”ë“œ ì‚¬ì´ì— ê³µë°± ë¼ì¸ ì¶”ê°€
        if len(content_lines) > 2:
            content_lines.append("")

        for tag in wos_field_order:
            if tag in row.index and pd.notna(row[tag]):
                value = str(row[tag]).strip()
                if not value: continue

                if tag in multi_line_fields:
                    # ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„ëœ í•­ëª©ë“¤ì„ ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ëˆ”
                    items = [item.strip() for item in value.split(';') if item.strip()]
                    if items:
                        content_lines.append(f"{tag:<2} {items[0]}")
                        for item in items[1:]:
                            content_lines.append(f"   {item}")
                else:
                    content_lines.append(f"{tag:<2} {value}")
        content_lines.append("ER")

    # íŒŒì¼ ëì— EF(End of File) ì¶”ê°€
    content_lines.append("EF")
    return "\n".join(content_lines).encode('utf-8-sig')


# --- UI ë Œë”ë§ ì‹œì‘ ---

st.markdown("""
<div style="text-align: center; padding: 2rem 0; background: linear-gradient(135deg, #003875, #0056b3); color: white; border-radius: 16px; margin-bottom: 2rem;">
    <h1 style="font-size: 3.5rem; font-weight: 700;">WOS PREP</h1>
    <p style="font-size: 1.3rem;">SCIMAT Edition</p>
</div>
""", unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
st.markdown("""
<div class="section-header">
    <div class="section-title">ğŸ“ ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ì—…ë¡œë“œ</div>
    <div class="section-subtitle">500ê°œ ë‹¨ìœ„ë¡œ ë‚˜ë‰œ ì—¬ëŸ¬ WOS íŒŒì¼ì„ ëª¨ë‘ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”</div>
</div>
""", unsafe_allow_html=True)

st.markdown("""
<div class="upload-zone">
    <div style="font-size: 3rem; margin-bottom: 16px; color: #003875;">ğŸ“¤</div>
    <h3 style="color: #212529;">WOS Plain Text íŒŒì¼ë“¤ì„ ì—¬ê¸°ì— ë“œë˜ê·¸í•˜ê±°ë‚˜ í´ë¦­í•˜ì„¸ìš”</h3>
    <p style="color: #6c757d;">Ctrl í‚¤ë¥¼ ëˆ„ë¥´ê³  ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
</div>
""", unsafe_allow_html=True)

uploaded_files = st.file_uploader(
    "WOS Plain Text íŒŒì¼ ì„ íƒ",
    type=['txt'],
    accept_multiple_files=True,
    label_visibility="collapsed"
)

if uploaded_files:
    with st.spinner(f"ğŸ”„ {len(uploaded_files)}ê°œ WOS íŒŒì¼ ë³‘í•© ë° ë¶„ì„ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        merged_df, file_status, duplicates_removed = load_and_merge_wos_files(uploaded_files)

    if merged_df is None or merged_df.empty:
        st.error("âš ï¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ WOS Plain Text íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        # íŒŒì¼ë³„ ìƒì„¸ ìƒíƒœ í‘œì‹œ
        st.markdown("### ğŸ“„ íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ")
        for status in file_status:
            st.warning(f"**{status['filename']}**: {status['message']}")
        st.stop()

    # --- ë°ì´í„° ì²˜ë¦¬ ë° í†µê³„ ê³„ì‚° (í•œ ë²ˆì— ìˆ˜í–‰) ---
    merged_df['Classification'] = merged_df.apply(classify_article, axis=1)

    # í†µê³„ì¹˜ ì¤‘ì•™ ê³„ì‚°
    successful_files = len([s for s in file_status if s['status'] == 'SUCCESS'])
    total_papers = len(merged_df)
    classification_counts = merged_df['Classification'].value_counts()
    include_papers = classification_counts[classification_counts.index.str.contains('Include', na=False)].sum()
    review_papers_count = classification_counts[classification_counts.index.str.contains('Review', na=False)].sum()
    exclude_papers_count = classification_counts[classification_counts.index.str.contains('Exclude', na=False)].sum()

    st.success(f"âœ… ë³‘í•© ë° ë¶„ì„ ì™„ë£Œ! {successful_files}ê°œ íŒŒì¼ì—ì„œ {total_papers:,}í¸ì˜ ê³ ìœ í•œ ë…¼ë¬¸ì„ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    if duplicates_removed > 0:
        st.info(f"ğŸ”„ ì¤‘ë³µ ë…¼ë¬¸ {duplicates_removed}í¸ì´ ìë™ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- ë¶„ì„ ê²°ê³¼ ìš”ì•½ ëŒ€ì‹œë³´ë“œ ---
    st.markdown("---")
    st.markdown("## ğŸ“ˆ ë³‘í•© ë°ì´í„° ë¶„ì„ ê²°ê³¼")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ì´ ë…¼ë¬¸ ìˆ˜", f"{total_papers:,}í¸")
    with col2:
        st.metric("í•µì‹¬ ì—°êµ¬ (Include)", f"{include_papers:,}í¸")
    with col3:
        st.metric("ê²€í†  ëŒ€ìƒ (Review)", f"{review_papers_count:,}í¸")
    with col4:
        st.metric("ì œì™¸ ëŒ€ìƒ (Exclude)", f"{exclude_papers_count:,}í¸")
    st.markdown("---")


    # --- ë…¼ë¬¸ ë¶„ë¥˜ í˜„í™© (ì°¨íŠ¸ì™€ í…Œì´ë¸”) ---
    st.markdown("## ğŸ“Š ë…¼ë¬¸ ë¶„ë¥˜ í˜„í™©")
    col1, col2 = st.columns([0.4, 0.6])

    with col1:
        st.dataframe(classification_counts.reset_index(), use_container_width=True, hide_index=True)

    with col2:
        chart_data = classification_counts.reset_index()
        chart_data.columns = ['Classification', 'Count']
        donut_chart = alt.Chart(chart_data).mark_arc(innerRadius=70).encode(
            theta=alt.Theta(field="Count", type="quantitative"),
            color=alt.Color(field="Classification", type="nominal", title="ë¶„ë¥˜"),
            tooltip=['Classification', 'Count']
        ).properties(
            width=500,
            height=350
        )
        st.altair_chart(donut_chart, use_container_width=True)


    # --- ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ ---
    if 'PY' in merged_df.columns:
        st.markdown("## ğŸ—“ï¸ ì—°ë„ë³„ ì—°êµ¬ ë™í–¥")
        try:
            df_trend = merged_df.copy()
            df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
            df_trend.dropna(subset=['PY'], inplace=True)
            df_trend['PY'] = df_trend['PY'].astype(int)

            yearly_counts = df_trend['PY'].value_counts().reset_index()
            yearly_counts.columns = ['Year', 'Count']
            yearly_counts = yearly_counts.sort_values('Year')

            line_chart = alt.Chart(yearly_counts).mark_line(point=True).encode(
                x=alt.X('Year:O', title='ë°œí–‰ ì—°ë„'),
                y=alt.Y('Count:Q', title='ë…¼ë¬¸ ìˆ˜'),
                tooltip=['Year', 'Count']
            ).properties(height=300)
            st.altair_chart(line_chart, use_container_width=True)
        except Exception as e:
            st.warning(f"ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


    # --- ë¶„ë¥˜ë³„ ë…¼ë¬¸ ìƒì„¸ ëª©ë¡ (Expander) ---
    st.markdown("---")
    st.markdown("## ğŸ“‚ ë¶„ë¥˜ë³„ ë…¼ë¬¸ ìƒì„¸ ëª©ë¡")

    # Review ë…¼ë¬¸ ëª©ë¡
    if review_papers_count > 0:
        with st.expander(f"ğŸ“ Review (ê²€í†  í•„ìš”) - {review_papers_count}í¸"):
            review_df = merged_df[merged_df['Classification'].str.contains('Review', na=False)]
            st.dataframe(review_df[['TI', 'AU', 'PY', 'SO', 'Classification']].rename(columns={
                'TI':'ì œëª©', 'AU':'ì €ì', 'PY':'ì—°ë„', 'SO':'ì €ë„', 'Classification':'ë¶„ë¥˜'
            }), hide_index=True)

    # Exclude ë…¼ë¬¸ ëª©ë¡
    if exclude_papers_count > 0:
        with st.expander(f"âŒ Exclude (ì œì™¸ ëŒ€ìƒ) - {exclude_papers_count}í¸"):
            exclude_df = merged_df[merged_df['Classification'].str.contains('Exclude', na=False)]
            st.dataframe(exclude_df[['TI', 'AU', 'PY', 'SO']].rename(columns={
                'TI':'ì œëª©', 'AU':'ì €ì', 'PY':'ì—°ë„', 'SO':'ì €ë„'
            }), hide_index=True)


    # --- ìµœì¢… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
    st.markdown("---")
    st.markdown("## ğŸ“¥ ìµœì¢… íŒŒì¼ ë‹¤ìš´ë¡œë“œ")

    # ë¶„ì„ ëŒ€ìƒ ë°ì´í„° (Exclude ì œì™¸)
    df_final_output = merged_df[~merged_df['Classification'].str.contains('Exclude', na=False)].copy()
    # WOS í˜•ì‹ ìœ ì§€ë¥¼ ìœ„í•´ ì¶”ê°€í–ˆë˜ Classification ì»¬ëŸ¼ ì œê±°
    df_final_output.drop(columns=['Classification'], errors='ignore', inplace=True)

    st.info(f"**ì´ {len(df_final_output):,}í¸**ì˜ ë…¼ë¬¸(Include ë° Review)ì´ SCIMAT ë¶„ì„ìš© íŒŒì¼ì— í¬í•¨ë©ë‹ˆë‹¤.")

    # SCIMAT í˜¸í™˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    text_data = convert_to_scimat_wos_format(df_final_output)
    st.download_button(
        label="ğŸš€ SCIMAT ë¶„ì„ìš© íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.txt)",
        data=text_data,
        file_name="scimat_ready_merged_data.txt",
        mime="text/plain",
        type="primary",
        use_container_width=True,
    )

    # ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ëª¨ë“  ì •ë³´ í¬í•¨)
    excel_buffer = io.BytesIO()
    with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
        merged_df.to_excel(writer, sheet_name='Merged_Data', index=False)
    st.download_button(
        label="ğŸ“Š ì „ì²´ ê²°ê³¼ ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ (.xlsx)",
        data=excel_buffer.getvalue(),
        file_name="wos_classification_results.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True
    )
