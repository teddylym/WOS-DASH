import streamlit as st
import pandas as pd
import altair as alt
import io
import re
from collections import Counter

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Multi-File Merger | SCIMAT Edition",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- í† ìŠ¤ ìŠ¤íƒ€ì¼ CSS ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@400;500;600;700&display=swap');
    
    .main-container {
        background: #f2f4f6;
        min-height: 100vh;
        font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, sans-serif;
    }
    
    .metric-card {
        background: white;
        border-radius: 8px;
        padding: 20px;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        margin-bottom: 12px;
        transition: all 0.2s ease;
        position: relative; /* For button positioning */
        height: 100%; /* ë™ì¼í•œ ë†’ì´ë¥¼ ìœ„í•œ ìˆ˜ì • */
    }
    
    .metric-card:hover {
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        transform: translateY(-1px);
        border-color: #3182f6;
    }
    
    .metric-value {
        font-size: 2.2rem;
        font-weight: 700;
        color: #191f28;
        margin: 0;
        line-height: 1.2;
        letter-spacing: -0.02em;
    }
    
    .metric-label {
        font-size: 13px;
        color: #8b95a1;
        margin: 6px 0 0 0;
        font-weight: 500;
        letter-spacing: -0.01em;
    }
    
    .metric-icon {
        background: #3182f6;
        color: white;
        width: 32px;
        height: 32px;
        border-radius: 6px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 16px;
        margin-bottom: 12px;
    }
    
    .chart-container {
        background: white;
        border-radius: 8px;
        padding: 24px;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        margin: 16px 0;
    }
    
    .chart-title {
        font-size: 16px;
        font-weight: 600;
        color: #191f28;
        margin-bottom: 16px;
        letter-spacing: -0.01em;
    }
    
    .section-header {
        background: white;
        color: #191f28;
        padding: 16px 20px;
        border-radius: 8px;
        margin: 20px 0 12px 0;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        position: relative;
        overflow: hidden;
    }
    
    .section-header::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        width: 4px;
        height: 100%;
        background: #3182f6;
    }
    
    .section-title {
        font-size: 16px;
        font-weight: 600;
        margin: 0;
        letter-spacing: -0.01em;
        color: #191f28;
    }
    
    .section-subtitle {
        font-size: 13px;
        color: #8b95a1;
        margin: 4px 0 0 0;
        font-weight: 400;
        letter-spacing: 0;
    }
    
    .info-panel {
        background: #f8fafe;
        border: 1px solid #e1f2ff;
        border-radius: 8px;
        padding: 16px;
        margin: 12px 0;
        position: relative;
    }
    
    .success-panel {
        background: #f0fdf4;
        border: 1px solid #dcfce7;
        border-radius: 8px;
        padding: 16px;
        margin: 12px 0;
        position: relative;
    }
    
    .warning-panel {
        background: #fffbeb;
        border: 1px solid #fed7aa;
        border-radius: 8px;
        padding: 16px;
        margin: 12px 0;
        position: relative;
    }
    
    .feature-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: 12px;
        margin: 20px 0;
    }
    
    .feature-card {
        background: white;
        border-radius: 8px;
        padding: 20px;
        text-align: center;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        transition: all 0.2s ease;
        position: relative;
        overflow: hidden;
    }
    
    .feature-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        border-color: #3182f6;
    }
    
    .feature-icon {
        font-size: 32px;
        margin-bottom: 12px;
        color: #3182f6;
    }
    
    .feature-title {
        font-size: 15px;
        font-weight: 600;
        color: #191f28;
        margin-bottom: 8px;
        letter-spacing: -0.01em;
    }
    
    .feature-desc {
        font-size: 13px;
        color: #8b95a1;
        line-height: 1.5;
        letter-spacing: 0;
    }
    
    .upload-zone {
        background: white;
        border: 1px dashed #d1d5db;
        border-radius: 8px;
        padding: 24px;
        text-align: center;
        margin: 16px 0;
        transition: all 0.2s ease;
    }
    
    .upload-zone:hover {
        background: #f8fafe;
        border-color: #3182f6;
    }
    
    .progress-indicator {
        background: #3182f6;
        height: 3px;
        border-radius: 2px;
        margin: 12px 0;
        animation: pulse 2s infinite;
    }
    
    .file-status {
        background: #f8fafe;
        border-radius: 6px;
        padding: 12px;
        margin: 8px 0;
        border-left: 3px solid #3182f6;
        font-family: 'Pretendard', sans-serif;
    }
    
    .stDownloadButton > button {
        background: #3182f6 !important;
        color: white !important;
        border: none !important;
        border-radius: 8px !important;
        padding: 12px 24px !important;
        font-weight: 600 !important;
        font-size: 14px !important;
        letter-spacing: -0.01em !important;
        transition: all 0.2s ease !important;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1) !important;
        font-family: 'Pretendard', sans-serif !important;
    }
    
    .stDownloadButton > button:hover {
        background: #1c64f2 !important;
        transform: translateY(-1px) !important;
        box-shadow: 0 2px 6px rgba(0,0,0,0.15) !important;
    }
    
    .streamlit-expanderHeader {
        background: white !important;
        border-radius: 8px !important;
        border: 1px solid #e5e8eb !important;
        font-weight: 500 !important;
        color: #191f28 !important;
        font-family: 'Pretendard', sans-serif !important;
        font-size: 14px !important;
    }
    
    .stDataFrame {
        border-radius: 8px !important;
        overflow: hidden !important;
        border: 1px solid #e5e8eb !important;
    }
    
    .stSpinner {
        color: #3182f6 !important;
    }
    
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.6; }
    }
    
    .main .block-container {
        padding-top: 1.5rem;
        padding-bottom: 1.5rem;
        max-width: 1200px;
    }
    
    .stAlert {
        border-radius: 8px !important;
        border: none !important;
        font-family: 'Pretendard', sans-serif !important;
        font-size: 14px !important;
    }
    
    .stSuccess {
        background: #f0fdf4 !important;
        color: #15803d !important;
    }
    
    .stInfo {
        background: #f8fafe !important;
        color: #1d4ed8 !important;
    }
    
    .stWarning {
        background: #fffbeb !important;
        color: #d97706 !important;
    }
    
    .stError {
        background: #fef2f2 !important;
        color: #dc2626 !important;
    }
    
    .main-text {
        font-size: 14px;
        color: #191f28;
        line-height: 1.5;
    }
    
    .sub-text {
        font-size: 13px;
        color: #8b95a1;
        line-height: 1.4;
    }
    
    .small-text {
        font-size: 12px;
        color: #8b95a1;
        line-height: 1.3;
    }
</style>
""", unsafe_allow_html=True)

# --- ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ë¡œë”© ë° ë³‘í•© í•¨ìˆ˜ ---
def load_and_merge_wos_files(uploaded_files):
    """ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ì„ ë¡œë”©í•˜ê³  ë³‘í•© - ì¤‘ë³µ ì œê±° ì™„ì „ ìˆ˜ì •"""
    all_dataframes = []
    file_status = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            file_bytes = uploaded_file.getvalue()
            encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'iso-8859-1']
            
            df = None
            encoding_used = None
            
            for encoding in encodings_to_try:
                try:
                    file_content = file_bytes.decode(encoding)
                    
                    if not file_content.strip().startswith('FN '):
                        continue
                        
                    df = parse_wos_format(file_content)
                    if df is not None and len(df) > 0:
                        encoding_used = encoding
                        break
                        
                except Exception:
                    continue
            
            if df is not None:
                all_dataframes.append(df)
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'SUCCESS',
                    'papers': len(df),
                    'encoding': encoding_used,
                    'message': f'âœ… {len(df)}í¸ ë…¼ë¬¸ ë¡œë”© ì„±ê³µ'
                })
            else:
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'ERROR',
                    'papers': 0,
                    'encoding': 'N/A',
                    'message': 'âŒ WOS Plain Text í˜•ì‹ì´ ì•„ë‹˜'
                })
                
        except Exception as e:
            file_status.append({
                'filename': uploaded_file.name,
                'status': 'ERROR',
                'papers': 0,
                'encoding': 'N/A',
                'message': f'âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}'
            })
    
    if all_dataframes:
        merged_df = pd.concat(all_dataframes, ignore_index=True)
        
        duplicates_removed = 0
        if 'UT' in merged_df.columns:
            def is_meaningful_ut(value):
                if pd.isna(value): return False
                str_value = str(value).strip()
                if len(str_value) < 10 or str_value.lower() in ['nan', 'none', 'null', '']: return False
                return True
            
            meaningful_ut_mask = merged_df['UT'].apply(is_meaningful_ut)
            rows_with_meaningful_ut = merged_df[meaningful_ut_mask]
            rows_without_meaningful_ut = merged_df[~meaningful_ut_mask]
            
            if not rows_with_meaningful_ut.empty:
                before_dedup = len(rows_with_meaningful_ut)
                deduplicated_meaningful = rows_with_meaningful_ut.drop_duplicates(subset=['UT'], keep='first')
                duplicates_removed = before_dedup - len(deduplicated_meaningful)
                merged_df = pd.concat([deduplicated_meaningful, rows_without_meaningful_ut], ignore_index=True)
        
        if duplicates_removed == 0 and 'TI' in merged_df.columns and 'AU' in merged_df.columns:
            subset_cols = ['TI', 'AU']
            valid_rows_mask = merged_df[subset_cols].notna().all(axis=1)
            valid_rows = merged_df[valid_rows_mask]
            invalid_rows = merged_df[~valid_rows_mask]
            
            if not valid_rows.empty:
                before_dedup = len(valid_rows)
                deduplicated_valid = valid_rows.drop_duplicates(subset=subset_cols, keep='first')
                duplicates_removed = before_dedup - len(deduplicated_valid)
                merged_df = pd.concat([deduplicated_valid, invalid_rows], ignore_index=True)
        
        return merged_df, file_status, duplicates_removed
    else:
        return None, file_status, 0

def parse_wos_format(content):
    """WOS Plain Text í˜•ì‹ì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    lines = content.split('\n')
    records = []
    current_record = {}
    current_field = None
    
    for line in lines:
        line = line.rstrip()
        if not line: continue
        if line == 'ER':
            if current_record:
                records.append(current_record.copy())
            current_record = {}
            current_field = None
            continue
        if line.startswith(('FN ', 'VR ')): continue
            
        if not line.startswith('   ') and ' ' in line:
            parts = line.split(' ', 1)
            if len(parts) == 2:
                current_field, field_value = parts
                current_record[current_field] = field_value.strip()
        elif line.startswith('   ') and current_field in current_record:
            current_record[current_field] += '; ' + line[3:].strip()
    
    if current_record: records.append(current_record)
    return pd.DataFrame(records) if records else None

# --- ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼ ìƒíƒœê³„ ì—°êµ¬ ë¶„ë¥˜ í•¨ìˆ˜ (2ë‹¨ê³„ í•„í„°ë§ ì ìš© ìµœì¢…ë³¸) ---
def classify_article(row):
    """2ë‹¨ê³„ í•„í„°ë§(Two-Step Filtering)ì„ ì ìš©í•œ ë…¼ë¬¸ ë¶„ë¥˜ í•¨ìˆ˜"""
    
    # --- í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ì¶œ ë° ê²°í•© (ì†Œë¬¸ì ë³€í™˜) ---
    def extract_text(value):
        return str(value).lower().strip() if pd.notna(value) else ""
    
    title = extract_text(row.get('TI', ''))
    abstract = extract_text(row.get('AB', ''))
    author_keywords = extract_text(row.get('DE', ''))
    wos_keywords = extract_text(row.get('ID', ''))
    
    full_text_for_keywords = ' '.join([title, abstract, author_keywords, wos_keywords])
    document_type = extract_text(row.get('DT', ''))

    # --- í‚¤ì›Œë“œ ì…‹ ì •ì˜ ---
    # IC1-A (í•µì‹¬ í‚¤ì›Œë“œ)
    core_keywords = [
        'live stream', 'livestream', 'live-streaming', 'twitch', 'live commerce', 'streamer', 
        'real-time interaction', 'youtube live', 'facebook live', 'tiktok live', 
        'periscope', 'bilibili live', 'afreecatv', 'chzzk', 'kick live'
    ]
    
    # IC1-B (ì—°êµ¬ ì°¨ì›)
    dimension_keywords = {
        'Technical': ['latency', 'qos', 'quality of service', 'qoe', 'quality of experience', 'protocol', 'bandwidth', 'codec', 'network', 'infrastructure'],
        'Platform': ['ecosystem', 'governance', 'algorithm', 'business model', 'platform'],
        'User': ['behavior', 'motivation', 'engagement', 'psychology', 'user', 'viewer', 'audience', 'parasocial', 'participation'],
        'Commercial': ['commerce', 'marketing', 'sales', 'roi', 'purchase', 'influencer', 'advertising', 'monetization'],
        'Social': ['culture', 'identity', 'social impact', 'fandom', 'community', 'cultural'],
        'Educational': ['learning', 'teaching', 'virtual classroom', 'education']
    }

    # EC1 (ë„ë©”ì¸ ê´€ë ¨ì„± ë¶€ì¬)
    irrelevant_domain_keywords = [
        'remote surgery', 'medical signal', 'military', 'satellite image', 'astronomy',
        'seismic', 'geological', 'telemedicine', 'vehicular network', 'drone video'
    ]
    
    # EC3 (í•™ìˆ ì  í˜•íƒœ ë¶€ì í•©)
    non_academic_types = ['editorial', 'news', 'correction', 'short commentary', 'conference abstract', 'letter', 'book review']
    
    # EC4 (ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬)
    non_interactive_keywords = ['vod', 'video on demand', 'asynchronous', 'pre-recorded', 'one-way video']

    # 2B ë‹¨ê³„ (ì—°êµ¬ ë°©ë²•ë¡ )
    methodology_keywords = [
        'survey', 'experiment', 'interview', 'case study', 'qualitative', 'quantitative', 
        'model', 'ethnography', 'empirical', 'framework', 'algorithm', 'protocol', 'analysis'
    ]

    # --- 1ë‹¨ê³„ í•„í„°ë§ (ê¸°ì´ˆ ì„ ë³„) ---
    
    # 1. EC3 (í•™ìˆ ì  í˜•íƒœ ë¶€ì í•©)
    if any(doc_type in document_type for doc_type in non_academic_types):
        return 'Exclude - EC3 (í•™ìˆ ì  í˜•íƒœ ë¶€ì í•©)'

    # 2. IC2 (í•™ìˆ ì  ê¸°ì—¬ - í˜•íƒœ)
    if not any(doc_type in document_type for doc_type in ['article', 'review']):
        return 'Exclude - IC2 ìœ„ë°° (í•™ìˆ ì  ê¸°ì—¬ ë¶€ì¡±)'

    # 3. EC1 (ë„ë©”ì¸ ê´€ë ¨ì„± ë¶€ì¬)
    if any(kw in full_text_for_keywords for kw in irrelevant_domain_keywords):
        return 'Exclude - EC1 (ë„ë©”ì¸ ê´€ë ¨ì„± ë¶€ì¬)'
    
    # 4. EC4 (ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬)
    if any(kw in full_text_for_keywords for kw in non_interactive_keywords):
        return 'Exclude - EC4 (ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬)'
        
    # 5. IC1-A (í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€)
    if not any(kw in full_text_for_keywords for kw in core_keywords):
        return 'Exclude - IC1 ìœ„ë°° (í•µì‹¬ í‚¤ì›Œë“œ ë¶€ì¬)'

    # 6. EC2 (ë¶€ì°¨ì  ì–¸ê¸‰)
    abstract_keyword_count = sum(abstract.count(kw) for kw in core_keywords)
    if abstract_keyword_count < 2:
        return 'Exclude - EC2 (ë¶€ì°¨ì  ì–¸ê¸‰)'

    # --- 2ë‹¨ê³„ í•„í„°ë§ (í•µì‹¬ ì—°êµ¬ ì†ì„± ê²€ì¦) ---
    
    # ì—°êµ¬ ì°¨ì› ë§¤ì¹­
    matched_dimensions = []
    for dim, kws in dimension_keywords.items():
        if any(kw in full_text_for_keywords for kw in kws):
            matched_dimensions.append(dim)
    
    # 7. (2A) ì—°êµ¬ ì°¨ì› ì¡°í•© í•„í„°ë§
    if len(matched_dimensions) < 2:
        return 'Exclude - 2A (ì—°êµ¬ ì°¨ì› ë‹¨ì¼)'
        
    # 8. (2B) ì—°êµ¬ ë°©ë²•ë¡  í‚¤ì›Œë“œ í•„í„°ë§
    if not any(kw in full_text_for_keywords for kw in methodology_keywords):
        return 'Exclude - 2B (ì—°êµ¬ ë°©ë²•ë¡  ë¶€ì¬)'

    # --- ìµœì¢… ë¶„ë¥˜ ---
    # 2ë‹¨ê³„ í•„í„°ë§ì„ í†µê³¼í•œ ë…¼ë¬¸ì€ ë‹¤í•™ì œì  ì—°êµ¬ë¡œ ê°„ì£¼
    return 'Include - Multidisciplinary'


# --- WOS Plain Text í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ---
def convert_to_scimat_wos_format(df_to_convert):
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']
    for _, row in df_to_convert.iterrows():
        if len(file_content) > 2: file_content.append("")
        for tag in wos_field_order:
            if tag in row.index and pd.notna(row[tag]) and str(row[tag]).strip():
                value = str(row[tag]).strip()
                if tag in multi_line_fields:
                    items = [item.strip() for item in value.split(';') if item.strip()]
                    if items:
                        file_content.append(f"{tag} {items[0]}")
                        for item in items[1:]: file_content.append(f"   {item}")
                else:
                    file_content.append(f"{tag} {value}")
        file_content.append("ER")
    return "\n".join(file_content).encode('utf-8-sig')

# --- ë©”ì¸ í—¤ë” ---
st.markdown("""
<div style="position: relative; text-align: center; padding: 2.5rem 0 3rem 0; background: linear-gradient(135deg, #3182f6, #1c64f2); color: white; border-radius: 8px; margin-bottom: 1.5rem; box-shadow: 0 2px 8px rgba(49,130,246,0.15); overflow: hidden;">
    <div style="position: absolute; top: 1rem; left: 1.5rem; color: white;">
        <div style="font-size: 12px; font-weight: 600; margin-bottom: 3px; letter-spacing: 0.3px;">HANYANG UNIVERSITY</div>
        <div style="font-size: 11px; opacity: 0.9; font-weight: 500;">Technology Management Research</div>
        <div style="font-size: 10px; opacity: 0.8; margin-top: 4px; font-weight: 400;">mot.hanyang.ac.kr</div>
    </div>
    <div style="position: absolute; top: 1rem; right: 1.5rem; text-align: right; color: rgba(255,255,255,0.9); font-size: 11px;">
        <p style="margin: 0; font-weight: 500;">Developed by: ì„íƒœê²½ (Teddy Lym)</p>
    </div>
    <h1 style="font-size: 3rem; font-weight: 700; margin-bottom: 0.3rem; letter-spacing: -0.02em;">WOS PREP</h1>
    <p style="font-size: 1.1rem; margin: 0; font-weight: 500; opacity: 0.95; letter-spacing: -0.01em;">SCIMAT Edition</p>
    <div style="width: 80px; height: 3px; background-color: rgba(255,255,255,0.3); margin: 1.5rem auto; border-radius: 2px;"></div>
</div>
""", unsafe_allow_html=True)

# --- í•µì‹¬ ê¸°ëŠ¥ ì†Œê°œ ---
st.markdown("""
<div class="feature-grid">
    <div class="feature-card"><div class="feature-icon">ğŸ”—</div><div class="feature-title">ë‹¤ì¤‘ íŒŒì¼ ìë™ ë³‘í•©</div><div class="feature-desc">ì—¬ëŸ¬ WOS íŒŒì¼ì„ í•œ ë²ˆì— ë³‘í•© ì²˜ë¦¬</div></div>
    <div class="feature-card"><div class="feature-icon">ğŸš«</div><div class="feature-title">ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì œê±°</div><div class="feature-desc">UT ê¸°ì¤€ ìë™ ì¤‘ë³µ ë…¼ë¬¸ ê°ì§€ ë° ì œê±°</div></div>
    <div class="feature-card"><div class="feature-icon">ğŸ¯</div><div class="feature-title">í•™ìˆ ì  ì—„ë°€ì„±</div><div class="feature-desc">2ë‹¨ê³„ í•„í„°ë§ìœ¼ë¡œ í•µì‹¬ ì—°êµ¬ ì„ ë³„</div></div>
</div>
""", unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
st.markdown("""
<div class="section-header">
    <div class="section-title">ğŸ“‚ ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ì—…ë¡œë“œ</div>
    <div class="section-subtitle">500ê°œ ë‹¨ìœ„ë¡œ ë‚˜ë‰œ ì—¬ëŸ¬ WOS íŒŒì¼ì„ ëª¨ë‘ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”</div>
</div>
""", unsafe_allow_html=True)

uploaded_files = st.file_uploader("WOS Plain Text íŒŒì¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)", type=['txt'], accept_multiple_files=True, label_visibility="collapsed")

if 'show_exclude_details' not in st.session_state:
    st.session_state['show_exclude_details'] = False

if uploaded_files:
    st.markdown(f"ğŸ“‹ **ì„ íƒëœ íŒŒì¼ ê°œìˆ˜:** {len(uploaded_files)}ê°œ")
    st.markdown('<div class="progress-indicator"></div>', unsafe_allow_html=True)
    
    with st.spinner(f"ğŸ”„ {len(uploaded_files)}ê°œ WOS íŒŒì¼ ë³‘í•© ë° 2ë‹¨ê³„ í•™ìˆ ì  ì •ì œ ì ìš© ì¤‘..."):
        merged_df, file_status, duplicates_removed = load_and_merge_wos_files(uploaded_files)
        
        if merged_df is None:
            st.error("âš ï¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ WOS Plain Text íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()
        
        merged_df['Classification'] = merged_df.apply(classify_article, axis=1)

    successful_files = sum(1 for s in file_status if s['status'] == 'SUCCESS')
    total_papers_before_filter = len(merged_df)
    
    df_excluded = merged_df[merged_df['Classification'].str.startswith('Exclude', na=False)]
    df_included = merged_df[~merged_df['Classification'].str.startswith('Exclude', na=False)].copy()
    
    st.success(f"âœ… ë³‘í•© ë° ì •ì œ ì™„ë£Œ! {successful_files}ê°œ íŒŒì¼ì—ì„œ ìµœì¢… {len(df_included):,}í¸ì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    if duplicates_removed > 0:
        st.info(f"ğŸ”„ ì¤‘ë³µ ë…¼ë¬¸ {duplicates_removed}í¸ì´ ìë™ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- ë¶„ì„ ê²°ê³¼ ìš”ì•½ ---
    st.markdown(f"""
    <div class="section-header">
        <div class="section-title">ğŸ“ˆ í•™ìˆ ì  ì •ì œ ê²°ê³¼ (Academic Refinement Results)</div>
        <div class="section-subtitle">2ë‹¨ê³„ í•„í„°ë§ ì ìš© í›„ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼ ìƒíƒœê³„ ì—°êµ¬ ë¶„ë¥˜ ê²°ê³¼</div>
    </div>
    """, unsafe_allow_html=True)

    total_excluded = len(df_excluded)
    df_final_output = df_included.copy() # ì´ì œ Review ì¹´í…Œê³ ë¦¬ê°€ ì—†ìœ¼ë¯€ë¡œ ë°”ë¡œ ë³µì‚¬
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.markdown(f"""<div class="metric-card"><div class="metric-icon">ğŸ“‹</div><div class="metric-value">{len(df_final_output):,}</div><div class="metric-label">ìµœì¢… ë¶„ì„ ëŒ€ìƒ<br><small>(Final Papers)</small></div></div>""", unsafe_allow_html=True)
    with col2:
        st.markdown(f"""<div class="metric-card"><div class="metric-icon">âœ…</div><div class="metric-value">{len(df_included):,}</div><div class="metric-label">í•µì‹¬ í¬í•¨ ì—°êµ¬<br><small>(Included Papers)</small></div></div>""", unsafe_allow_html=True)
    with col3:
        processing_rate = (len(df_included) / total_papers_before_filter * 100) if total_papers_before_filter > 0 else 0
        st.markdown(f"""<div class="metric-card"><div class="metric-icon">ğŸ“Š</div><div class="metric-value">{processing_rate:.1f}%</div><div class="metric-label">ìµœì¢… í¬í•¨ ë¹„ìœ¨<br><small>(Inclusion Rate)</small></div></div>""", unsafe_allow_html=True)
    with col4:
        st.markdown(f"""<div class="metric-card" style="padding-bottom: 50px;"><div class="metric-icon" style="background-color: #ef4444;">â›”</div><div class="metric-value">{total_excluded:,}</div><div class="metric-label">í•™ìˆ ì  ë°°ì œ<br><small>(Excluded Papers)</small></div><div style="position: absolute; bottom: 10px; right: 15px;">""", unsafe_allow_html=True)
        if st.button("ìƒì„¸ë³´ê¸°", key="exclude_details_button"): 
            st.session_state['show_exclude_details'] = not st.session_state.get('show_exclude_details', False)
        st.markdown("</div></div>", unsafe_allow_html=True) 

    # --- ì„ ì • ê¸°ì¤€ ì„¤ëª… UI (2ë‹¨ê³„ í•„í„°ë§ ì ìš©) ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ğŸ“œ 2ë‹¨ê³„ í•„í„°ë§ ì„ ì • ê¸°ì¤€ (Two-Step Filtering Criteria)</div>
    """, unsafe_allow_html=True)
    ic_col, ec_col = st.columns(2)
    with ic_col:
        st.markdown('<h5 style="color: #10b981;">âœ… í¬í•¨ ê¸°ì¤€ (Inclusion Criteria)</h5>', unsafe_allow_html=True)
        st.markdown("""
        - **IC (í•µì‹¬ ì—°êµ¬):** ì•„ë˜ **ëª¨ë“  ë°°ì œ ê¸°ì¤€ì„ í†µê³¼**í•˜ê³ , **ìµœì†Œ 2ê°œ ì´ìƒì˜ ì—°êµ¬ ì°¨ì›**ì— ê±¸ì³ìˆìœ¼ë©°, **ëª…í™•í•œ ì—°êµ¬ ë°©ë²•ë¡ **ì„ ì œì‹œí•˜ëŠ” ê¹Šì´ ìˆëŠ” ì—°êµ¬.
        """)
    with ec_col:
        st.markdown('<h5 style="color: #ef4444;">â›”ï¸ ë°°ì œ ê¸°ì¤€ (Exclusion Criteria)</h5>', unsafe_allow_html=True)
        st.markdown("""
        **1ë‹¨ê³„: ê¸°ì´ˆ ì„ ë³„**
        - **EC1 (ë„ë©”ì¸ ê´€ë ¨ì„± ë¶€ì¬):** ì›ê²© ìˆ˜ìˆ , êµ°ì‚¬ ë“± ë¬´ê´€ ë„ë©”ì¸ ì—°êµ¬.
        - **EC2 (ë¶€ì°¨ì  ì–¸ê¸‰):** ì´ˆë¡ ë‚´ í•µì‹¬ í‚¤ì›Œë“œ 2íšŒ ë¯¸ë§Œ ì–¸ê¸‰ ì—°êµ¬.
        - **EC3 (í•™ìˆ ì  í˜•íƒœ ë¶€ì í•©):** ì‚¬ì„¤, ë‰´ìŠ¤ ë“± ë¹„í•™ìˆ  ìë£Œ.
        - **EC4 (ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬):** VOD, ë¹„ë™ê¸° ì˜ìƒ ë“± ì—°êµ¬.
        
        **2ë‹¨ê³„: í•µì‹¬ ì†ì„± ê²€ì¦**
        - **EC5 (ì—°êµ¬ ì°¨ì› ë‹¨ì¼):** 1ê°œì˜ ì—°êµ¬ ì°¨ì›ì—ë§Œ êµ­í•œëœ ì—°êµ¬.
        - **EC6 (ì—°êµ¬ ë°©ë²•ë¡  ë¶€ì¬):** ì‹¤ì¦ì  ì—°êµ¬ ë°©ë²•ë¡ ì´ ì—†ëŠ” ì—°êµ¬.
        """)
    st.markdown("</div>", unsafe_allow_html=True)


    # --- ë°°ì œëœ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ---
    if st.session_state.get('show_exclude_details', False) and total_excluded > 0:
        st.markdown("""<div class="chart-container"><div class="chart-title">ë°°ì œ ì‚¬ìœ ë³„ ë¶„í¬ (Distribution by Exclusion Reason)</div>""", unsafe_allow_html=True)
        
        exclusion_reason_df = df_excluded['Classification'].str.replace('Exclude - ', '').value_counts().reset_index()
        exclusion_reason_df.columns = ['Exclusion Reason', 'Count']

        bar_chart = alt.Chart(exclusion_reason_df).mark_bar().encode(
            x=alt.X('Count:Q', title='ë…¼ë¬¸ ìˆ˜'),
            y=alt.Y('Exclusion Reason:N', title='ë°°ì œ ì‚¬ìœ ', sort='-x'),
            tooltip=['Exclusion Reason', 'Count']
        ).properties(height=300)
        
        st.altair_chart(bar_chart, use_container_width=True)

        with st.expander("ë°°ì œ ë…¼ë¬¸ ìƒì„¸ ëª©ë¡ ë³´ê¸° (ìµœëŒ€ 100ê°œ ìƒ˜í”Œ)"):
            st.dataframe(df_excluded[['TI', 'PY', 'SO', 'Classification']].head(100), use_container_width=True)
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                df_excluded[['TI', 'AU', 'PY', 'SO', 'AB', 'Classification']].to_excel(writer, sheet_name='Excluded_Papers', index=False)
            st.download_button(label="ì „ì²´ ë°°ì œ ëª©ë¡ ë‹¤ìš´ë¡œë“œ (Excel)", data=excel_buffer.getvalue(), file_name="excluded_papers.xlsx", mime="application/vnd.ms-excel", use_container_width=True)


    # --- ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ ---
    if 'PY' in df_final_output.columns:
        st.markdown("""<div class="chart-container"><div class="chart-title">ìµœì¢… ì„ ì • ë…¼ë¬¸ì˜ ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ (Publication Trend of Final Papers)</div>""", unsafe_allow_html=True)
        df_trend = df_final_output.copy()
        df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
        df_trend.dropna(subset=['PY'], inplace=True)
        yearly_counts = df_trend['PY'].astype(int).value_counts().reset_index()
        yearly_counts.columns = ['Year', 'Count']
        
        line_chart = alt.Chart(yearly_counts).mark_line(point=True, strokeWidth=3, color='#0064ff').encode(
            x=alt.X('Year:O', title='Publication Year (ë°œí–‰ ì—°ë„)'),
            y=alt.Y('Count:Q', title='Number of Papers (ë…¼ë¬¸ ìˆ˜)'),
            tooltip=['Year', 'Count']
        ).properties(height=300)
        st.altair_chart(line_chart, use_container_width=True)
        st.markdown("</div>", unsafe_allow_html=True)

    # --- ìµœì¢… ìš”ì•½ íŒ¨ë„ ---
    st.markdown(f"""
    <div class="info-panel">
        <h4 style="color: #0064ff; margin-bottom: 16px; font-weight: 700;">ğŸ¯ 2ë‹¨ê³„ í•™ìˆ ì  ë°ì´í„° ì •ì œ ì™„ë£Œ</h4>
        <p style="color: #191f28; margin: 6px 0;"><strong>íŒŒì¼ í†µí•©:</strong> {successful_files}ê°œì˜ WOS íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë³‘í•©</p>
        <p style="color: #191f28; margin: 6px 0;"><strong>ì¤‘ë³µ ì œê±°:</strong> {duplicates_removed}í¸ì˜ ì¤‘ë³µ ë…¼ë¬¸ ìë™ ê°ì§€ ë° ì œê±°</p>
        <p style="color: #191f28; margin: 6px 0;"><strong>í•™ìˆ ì  ì—„ë°€ì„±:</strong> 2ë‹¨ê³„ í•„í„°ë§ìœ¼ë¡œ {total_excluded}í¸ ì œì™¸</p>
        <p style="color: #191f28; margin: 6px 0;"><strong>ìµœì¢… ê·œëª¨:</strong> {len(df_final_output):,}í¸ì˜ ê³ í’ˆì§ˆ í•µì‹¬ ì—°êµ¬ ë°ì´í„°ì…‹</p>
        <p style="color: #191f28; margin: 6px 0;"><strong>SCIMAT í˜¸í™˜:</strong> ì™„ë²½í•œ WOS Plain Text í˜•ì‹ìœ¼ë¡œ 100% í˜¸í™˜ì„± ë³´ì¥</p>
        <div style="margin-top: 16px; padding: 12px; background: rgba(0,100,255,0.1); border-radius: 8px;">
            <p style='color: #0064ff; margin: 0; font-weight: 600; font-size: 14px;'>
            ğŸ’¡ <strong>ìµœì¢… í¬í•¨ ë¹„ìœ¨:</strong> {(len(df_final_output)/total_papers_before_filter*100 if total_papers_before_filter > 0 else 0):.1f}% 
            - ì—°êµ¬ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê¸°ì—¬í•˜ëŠ” ê¹Šì´ ìˆëŠ” í•µì‹¬ ì—°êµ¬ë§Œì„ ì„ ë³„í–ˆìŠµë‹ˆë‹¤.
            </p>
        </div>
    </div>
    """, unsafe_allow_html=True)


    # --- ìµœì¢… ë‹¤ìš´ë¡œë“œ ---
    st.markdown("""<div class="section-header"><div class="section-title">ğŸ“¥ ìµœì¢… íŒŒì¼ ë‹¤ìš´ë¡œë“œ (Final File Download)</div><div class="section-subtitle">ì •ì œëœ ê³ í’ˆì§ˆ WOS Plain Text íŒŒì¼</div></div>""", unsafe_allow_html=True)
    text_data = convert_to_scimat_wos_format(df_final_output)
    st.download_button(label="ğŸ“¥ ë‹¤ìš´ë¡œë“œ (Download)", data=text_data, file_name=f"scimat_filtered_{len(df_final_output)}papers.txt", mime="text/plain", use_container_width=True)

# --- í•˜ë‹¨ ê³ ì • ì •ë³´ ---
with st.expander("â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)", expanded=False):
    st.markdown("""
    **Q: ì—¬ëŸ¬ WOS íŒŒì¼ì„ ì–´ë–»ê²Œ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë‚˜ìš”?**
    A: WOSì—ì„œ ì—¬ëŸ¬ ë²ˆ Plain Text ë‹¤ìš´ë¡œë“œí•œ í›„, ëª¨ë“  .txt íŒŒì¼ì„ í•œ ë²ˆì— ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
    
    **Q: ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ë…¼ë¬¸ì´ ë°°ì œë˜ë‚˜ìš”?**
    A: 2ë‹¨ê³„ í•„í„°ë§ì„ í†µí•´ ì£¼ì œ ê´€ë ¨ì„±ì´ ë‚®ê±°ë‚˜(1ë‹¨ê³„), ì—°êµ¬ì˜ ê¹Šì´(ì—°êµ¬ ì°¨ì› ë‹¨ì¼) ë˜ëŠ” í•™ìˆ ì  ì—„ë°€ì„±(ë°©ë²•ë¡  ë¶€ì¬)ì´ ë¶€ì¡±í•œ(2ë‹¨ê³„) ì—°êµ¬ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë°°ì œí•©ë‹ˆë‹¤.
    
    **Q: SCIMATì—ì„œ í‚¤ì›Œë“œ ì •ë¦¬ë¥¼ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**
    A: Group set â†’ Word â†’ Find similar words by distances (Maximum distance: 1)ë¡œ ìœ ì‚¬ í‚¤ì›Œë“œë¥¼ ìë™ í†µí•©í•˜ê³ , Word Group manual setì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ê·¸ë£¹í™”í•˜ì„¸ìš”.
    
    **Q: SCIMAT ë¶„ì„ ì„¤ì •ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**
    A: Unit of Analysis: "Author's words + Source's words", Network Type: "Co-occurrence", Normalization: "Equivalence Index", Clustering: "Simple Centers Algorithm" (Maximum network size: 50)ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    """)

with st.expander("ğŸ“Š WOS â†’ SciMAT ë¶„ì„ ì‹¤í–‰ ê°€ì´ë“œ", expanded=False):
    st.markdown("""
    ### í•„ìš”í•œ ê²ƒ
    - SciMAT ì†Œí”„íŠ¸ì›¨ì–´ (ë¬´ë£Œ ë‹¤ìš´ë¡œë“œ)
    - ë‹¤ìš´ë¡œë“œëœ WOS Plain Text íŒŒì¼
    - Java 1.8 ì´ìƒ
    
    ### 1ë‹¨ê³„: SciMAT ì‹œì‘í•˜ê¸°
    
    **ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±**
    ```
    1. SciMAT ì‹¤í–‰ (SciMAT.jar ë”ë¸”í´ë¦­)
    2. File â†’ New Project
    3. Path: ì €ì¥í•  í´ë” ì„ íƒ
    4. File name: í”„ë¡œì íŠ¸ ì´ë¦„ ì…ë ¥
    5. Accept
    ```
    
    **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**
    ```
    1. File â†’ Add Files
    2. "ISI WoS" ì„ íƒ
    3. ë‹¤ìš´ë¡œë“œí•œ txt íŒŒì¼ ì„ íƒ
    4. ë¡œë”© ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    ```
    
    ### 2ë‹¨ê³„: í‚¤ì›Œë“œ ì •ë¦¬í•˜ê¸°
    
    **ìœ ì‚¬ í‚¤ì›Œë“œ ìë™ í†µí•©**
    ```
    1. Group set â†’ Word â†’ Find similar words by distances
    2. Maximum distance: 1 (í•œ ê¸€ì ì°¨ì´)
    3. ê°™ì€ ì˜ë¯¸ ë‹¨ì–´ë“¤ í™•ì¸í•˜ê³  Moveë¡œ í†µí•©
    ```
    
    **ìˆ˜ë™ìœ¼ë¡œ í‚¤ì›Œë“œ ì •ë¦¬**
    ```
    1. Group set â†’ Word â†’ Word Group manual set
    2. Words without group ëª©ë¡ í™•ì¸
    3. ê´€ë ¨ í‚¤ì›Œë“œë“¤ ì„ íƒ í›„ New groupìœ¼ë¡œ ë¬¶ê¸°
    4. ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
    ```
    
    ### 3ë‹¨ê³„: ì‹œê°„ êµ¬ê°„ ì„¤ì •
    
    **Period ë§Œë“¤ê¸°**
    ```
    1. Knowledge base â†’ Periods â†’ Periods manager
    2. Add ë²„íŠ¼ìœ¼ë¡œ ì‹œê°„ êµ¬ê°„ ìƒì„±:
       - Period 1: 2000-2006 (íƒœë™ê¸°)
       - Period 2: 2007-2016 (í˜•ì„±ê¸°)
       - Period 3: 2017-2021 (í™•ì‚°ê¸°)
       - Period 4: 2022-2025 (ìœµí•©ê¸°)
    ```
    
    ### 4ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰
    
    **ë¶„ì„ ë§ˆë²•ì‚¬ ì‹œì‘**
    ```
    1. Analysis â†’ Make Analysis
    2. ëª¨ë“  Period ì„ íƒ â†’ Next
    ```
    
    **Step 1-8: ë¶„ì„ ì„¤ì •**
    - Unit of Analysis: "Author's words + Source's words"
    - Data Reduction: Minimum frequency 2
    - Network Type: "Co-occurrence"
    - Normalization: "Equivalence Index"
    - Clustering: "Simple Centers Algorithm" (Max network size: 50)
    - Document Mapper: "Core Mapper"
    - Performance Measures: G-index, Sum Citations
    - Evolution Map: "Jaccard Index"
    ```
    
st.markdown("<br><br>", unsafe_allow_html=True)
