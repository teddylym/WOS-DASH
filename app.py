import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt
import io

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="WOS Prep | Professional Edition", layout="wide", initial_sidebar_state="collapsed")

# --- ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
    /* ê¸°ì¡´ CSS ìœ ì§€, ìƒëµ */
</style>
""", unsafe_allow_html=True)

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- í‚¤ì›Œë“œ ì •ê·œí™” ì‚¬ì „ í™•ì¥ ---
@st.cache_data
def build_normalization_map():
    base_map = {
        "live commerce": ["live shopping", "social commerce", "livestream shopping", "live video commerce", "e-commerce live streaming"],
        "live streaming": ["live-streaming", "livestreaming", "real time streaming", "live broadcast"],
        "user engagement": ["consumer engagement", "viewer engagement", "audience engagement", "customer engagement"],
        "purchase intention": ["purchase intentions", "buying intention", "purchase behavior"],
        "user experience": ["consumer experience", "viewer experience", "ux"],
        "social presence": ["perceived social presence"],
        "influencer marketing": ["influencer", "digital celebrities", "wanghong"],
        "platform technology": ["streaming technology", "platform architecture", "streaming media"],
        "peer-to-peer": ["p2p", "peer to peer"],
        "artificial intelligence": ["ai"],
        "user behavior": ["consumer behavior"],
        "vulnerability analysis": ["service quality", "platform adoption"]  # ì¶”ê°€: ìƒê±°ë˜ ê´€ë ¨ ì¬ë¶„ë¥˜
    }
    reverse_map = {}
    for standard_form, variations in base_map.items():
        for variation in variations:
            reverse_map[variation.strip().lower()] = standard_form
        reverse_map[standard_form.strip().lower()] = standard_form
    return reverse_map

NORMALIZATION_MAP = build_normalization_map()

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
def load_data(uploaded_file):
    # ê¸°ì¡´ ë¡œì§ ìœ ì§€, ìƒëµ
    pass

# --- [ì¬ìˆ˜ì •] ë¶„ë¥˜ í•¨ìˆ˜: ìƒê±°ë˜ ì¤‘ì‹¬ìœ¼ë¡œ ì„¸ë¶„í™” ---
def classify_article(row):
    strong_inclusion_keywords = [
        'live streaming commerce', 'social commerce', 'livestreaming commerce', 'purchase intention', 
        'customer engagement', 'consumer behavior', 'influencer marketing', 'brand engagement', 
        'online shopping', 'digital marketing', 'e-commerce', 'viewer engagement', 'user experience',
        'social motivations', 'parasocial interaction', 'virtual gift', 'fan engagement',
        'vulnerability analysis', 'service quality', 'platform adoption'  # ì¶”ê°€: ìƒê±°ë˜ í¬ì°©
    ]
    
    inclusion_keywords = [
        'user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'experience', 
        'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 
        'society', 'marketing', 'business', 'brand', 'monetization', 'education', 'learning'
    ]
    
    exclusion_keywords = [  # ì„¸ë¶„í™”: ë³µí•© í‚¤ì›Œë“œë¡œ ì œí•œ
        'protocol optimization', 'network coding scheme', 'wimax technology', 'ieee 802.16 standard',
        'mac layer protocol', 'packet dropping algorithm', 'bandwidth optimization', 
        'forward error correction scheme', 'sensor data processing', 'geoscience application'
    ]
    
    medical_keywords = ['surgical education', 'medical education', 'surgery', 'clinical learning']  # ì¶”ê°€: ì˜ë£Œ ì¬ê²€í† 
    
    title = str(row.get('TI', '')).lower()
    source_title = str(row.get('SO', '')).lower()
    author_keywords = str(row.get('DE', '')).lower()
    keywords_plus = str(row.get('ID', '')).lower()
    abstract = str(row.get('AB', '')).lower()
    
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    
    # 1. ê°•ë ¥ í¬í•¨ í™•ì¸
    if any(keyword in full_text for keyword in strong_inclusion_keywords):
        return 'Include (ê´€ë ¨ì—°êµ¬)'
    
    # 2. ì˜ë£Œ êµìœ¡ ì¬ê²€í† 
    if any(kw in full_text for kw in medical_keywords):
        if any(kw in full_text for kw in ['user experience', 'consumer engagement', 'e-commerce']):
            return 'Review (ê²€í† í•„ìš”)'
        return 'Exclude (ì œì™¸ì—°êµ¬)'
    
    # 3. ëª…í™• ì œì™¸ í™•ì¸ (ë³µí•© í‚¤ì›Œë“œ ë§¤ì¹­)
    if any(keyword in full_text for keyword in exclusion_keywords):
        return 'Exclude (ì œì™¸ì—°êµ¬)'

    # 4. ì¼ë°˜ í¬í•¨ í™•ì¸
    if sum(1 for keyword in inclusion_keywords if keyword in full_text) >= 2:
        return 'Include (ê´€ë ¨ì—°êµ¬)'
    
    return 'Review (ê²€í† í•„ìš”)'

# --- í‚¤ì›Œë“œ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def clean_keyword_string(keywords_str, stop_words, lemmatizer, normalization_map):
    # ê¸°ì¡´ ë¡œì§ ìœ ì§€, ìƒëµ
    pass

# --- SCIMAT í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ---
def convert_df_to_scimat_format(df_to_convert):
    # ê¸°ì¡´ ë¡œì§ ìœ ì§€, ìƒëµ
    pass

# --- ë©”ì¸ í—¤ë” ë° ê¸°ëŠ¥ ì†Œê°œ ---
# ê¸°ì¡´ HTML ìœ ì§€, ìƒëµ

# --- íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬ ---
uploaded_file = st.file_uploader(  # ê¸°ì¡´ ì—…ë¡œë”
    "Tab-delimited, Plain Text, ë˜ëŠ” Excel í˜•ì‹ì˜ WOS ë°ì´í„° íŒŒì¼ì„ ì—¬ê¸°ì— ë“œë˜ê·¸í•˜ê±°ë‚˜ ì„ íƒí•˜ì„¸ìš”.",
    type=['csv', 'txt', 'xlsx', 'xls'],
    label_visibility="collapsed"
)

if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is None:
        st.error("âš ï¸ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    # ì»¬ëŸ¼ ë§¤í•‘ ê¸°ì¡´ ìœ ì§€

    with st.spinner("ğŸ”„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # ê¸°ì¡´ ì²˜ë¦¬ ë¡œì§

    # --- ë¶„ì„ ê²°ê³¼ ìš”ì•½ ê¸°ì¡´ ìœ ì§€ ---

    # --- [ì¶”ê°€] Review ë…¼ë¬¸ UI ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ” Review Needed Papers</div>
        <div class="section-subtitle">ê²€í† ê°€ í•„ìš”í•œ ë…¼ë¬¸ ëª©ë¡</div>
    </div>
    """, unsafe_allow_html=True)
    
    review_papers = df[df['Classification'] == 'Review (ê²€í† í•„ìš”)'].head(30)
    if not review_papers.empty:
        display_data = []
        for idx, row in review_papers.iterrows():
            display_data.append({
                'ë…¼ë¬¸ ì œëª©': str(row.get('TI', 'No Title'))[:80],
                'ì €ì': str(row.get('AU', 'Unknown')).split(';')[0],
                'ì—°ë„': str(row.get('PY', 'N/A')),
                'ì €ì í‚¤ì›Œë“œ': str(row.get('DE', 'N/A'))[:50]
            })
        st.dataframe(pd.DataFrame(display_data), use_container_width=True)
        for idx, row in review_papers.iterrows():
            col1, col2 = st.columns(2)
            with col1:
                if st.button(f"Include: {row['TI'][:50]}", key=f"include_{idx}"):
                    df.loc[idx, 'Classification'] = 'Include (ê´€ë ¨ì—°êµ¬)'
            with col2:
                if st.button(f"Exclude: {row['TI'][:50]}", key=f"exclude_{idx}"):
                    df.loc[idx, 'Classification'] = 'Exclude (ì œì™¸ì—°êµ¬)'
    else:
        st.info("ê²€í† ê°€ í•„ìš”í•œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    # --- ìµœì¢… íŒŒì¼ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ ê¸°ì¡´ ìœ ì§€ ---
