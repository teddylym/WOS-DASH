import streamlit as st
import pandas as pd
import altair as alt
import io
import re
from collections import Counter

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Multi-File Merger | SCIMAT Edition",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- í† ìŠ¤ ìŠ¤íƒ€ì¼ CSS ---
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Pretendard:wght@400;500;600;700&display=swap');
    
    .main-container {
        background: #f2f4f6;
        min-height: 100vh;
        font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, sans-serif;
    }
    
    .metric-card {
        background: white;
        border-radius: 8px;
        padding: 20px;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        margin-bottom: 12px;
        transition: all 0.2s ease;
    }
    
    .metric-card:hover {
        box-shadow: 0 2px 8px rgba(0,0,0,0.08);
        transform: translateY(-1px);
        border-color: #3182f6;
    }
    
    .metric-value {
        font-size: 2.2rem;
        font-weight: 700;
        color: #191f28;
        margin: 0;
        line-height: 1.2;
        letter-spacing: -0.02em;
    }
    
    .metric-label {
        font-size: 13px;
        color: #8b95a1;
        margin: 6px 0 0 0;
        font-weight: 500;
        letter-spacing: -0.01em;
    }
    
    .metric-icon {
        background: #3182f6;
        color: white;
        width: 32px;
        height: 32px;
        border-radius: 6px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 16px;
        margin-bottom: 12px;
    }
    
    .chart-container {
        background: white;
        border-radius: 8px;
        padding: 24px;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        margin: 16px 0;
    }
    
    .chart-title {
        font-size: 16px;
        font-weight: 600;
        color: #191f28;
        margin-bottom: 16px;
        letter-spacing: -0.01em;
    }
    
    .section-header {
        background: white;
        color: #191f28;
        padding: 16px 20px;
        border-radius: 8px;
        margin: 20px 0 12px 0;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        position: relative;
        overflow: hidden;
    }
    
    .section-header::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        width: 4px;
        height: 100%;
        background: #3182f6;
    }
    
    .section-title {
        font-size: 16px;
        font-weight: 600;
        margin: 0;
        letter-spacing: -0.01em;
        color: #191f28;
    }
    
    .section-subtitle {
        font-size: 13px;
        color: #8b95a1;
        margin: 4px 0 0 0;
        font-weight: 400;
        letter-spacing: 0;
    }
    
    .info-panel {
        background: #f8fafe;
        border: 1px solid #e1f2ff;
        border-radius: 8px;
        padding: 16px;
        margin: 12px 0;
        position: relative;
    }
    
    .success-panel {
        background: #f0fdf4;
        border: 1px solid #dcfce7;
        border-radius: 8px;
        padding: 16px;
        margin: 12px 0;
        position: relative;
    }
    
    .warning-panel {
        background: #fffbeb;
        border: 1px solid #fed7aa;
        border-radius: 8px;
        padding: 16px;
        margin: 12px 0;
        position: relative;
    }
    
    .feature-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: 12px;
        margin: 20px 0;
    }
    
    .feature-card {
        background: white;
        border-radius: 8px;
        padding: 20px;
        text-align: center;
        box-shadow: none;
        border: 1px solid #e5e8eb;
        transition: all 0.2s ease;
        position: relative;
        overflow: hidden;
    }
    
    .feature-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        border-color: #3182f6;
    }
    
    .feature-icon {
        font-size: 32px;
        margin-bottom: 12px;
        color: #3182f6;
    }
    
    .feature-title {
        font-size: 15px;
        font-weight: 600;
        color: #191f28;
        margin-bottom: 8px;
        letter-spacing: -0.01em;
    }
    
    .feature-desc {
        font-size: 13px;
        color: #8b95a1;
        line-height: 1.5;
        letter-spacing: 0;
    }
    
    .upload-zone {
        background: white;
        border: 1px dashed #d1d5db;
        border-radius: 8px;
        padding: 24px;
        text-align: center;
        margin: 16px 0;
        transition: all 0.2s ease;
    }
    
    .upload-zone:hover {
        background: #f8fafe;
        border-color: #3182f6;
    }
    
    .progress-indicator {
        background: #3182f6;
        height: 3px;
        border-radius: 2px;
        margin: 12px 0;
        animation: pulse 2s infinite;
    }
    
    .file-status {
        background: #f8fafe;
        border-radius: 6px;
        padding: 12px;
        margin: 8px 0;
        border-left: 3px solid #3182f6;
        font-family: 'Pretendard', sans-serif;
    }
    
    .stDownloadButton > button {
        background: #3182f6 !important;
        color: white !important;
        border: none !important;
        border-radius: 8px !important;
        padding: 12px 24px !important;
        font-weight: 600 !important;
        font-size: 14px !important;
        letter-spacing: -0.01em !important;
        transition: all 0.2s ease !important;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1) !important;
        font-family: 'Pretendard', sans-serif !important;
    }
    
    .stDownloadButton > button:hover {
        background: #1c64f2 !important;
        transform: translateY(-1px) !important;
        box-shadow: 0 2px 6px rgba(0,0,0,0.15) !important;
    }
    
    .streamlit-expanderHeader {
        background: white !important;
        border-radius: 8px !important;
        border: 1px solid #e5e8eb !important;
        font-weight: 500 !important;
        color: #191f28 !important;
        font-family: 'Pretendard', sans-serif !important;
        font-size: 14px !important;
    }
    
    .stDataFrame {
        border-radius: 8px !important;
        overflow: hidden !important;
        border: 1px solid #e5e8eb !important;
    }
    
    .stSpinner {
        color: #3182f6 !important;
    }
    
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.6; }
    }
    
    .main .block-container {
        padding-top: 1.5rem;
        padding-bottom: 1.5rem;
        max-width: 1200px;
    }
    
    .stAlert {
        border-radius: 8px !important;
        border: none !important;
        font-family: 'Pretendard', sans-serif !important;
        font-size: 14px !important;
    }
    
    .stSuccess {
        background: #f0fdf4 !important;
        color: #15803d !important;
    }
    
    .stInfo {
        background: #f8fafe !important;
        color: #1d4ed8 !important;
    }
    
    .stWarning {
        background: #fffbeb !important;
        color: #d97706 !important;
    }
    
    .stError {
        background: #fef2f2 !important;
        color: #dc2626 !important;
    }
    
    .main-text {
        font-size: 14px;
        color: #191f28;
        line-height: 1.5;
    }
    
    .sub-text {
        font-size: 13px;
        color: #8b95a1;
        line-height: 1.4;
    }
    
    .small-text {
        font-size: 12px;
        color: #8b95a1;
        line-height: 1.3;
    }
</style>
""", unsafe_allow_html=True)

# --- ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ë¡œë”© ë° ë³‘í•© í•¨ìˆ˜ ---
def load_and_merge_wos_files(uploaded_files):
    """ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ì„ ë¡œë”©í•˜ê³  ë³‘í•© - ì¤‘ë³µ ì œê±° ì™„ì „ ìˆ˜ì •"""
    all_dataframes = []
    file_status = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            file_bytes = uploaded_file.getvalue()
            encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'iso-8859-1']
            
            df = None
            encoding_used = None
            
            for encoding in encodings_to_try:
                try:
                    file_content = file_bytes.decode(encoding)
                    
                    # WOS ì›ë³¸ í˜•ì‹ ê²€ì¦ (FNìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•¨)
                    if not file_content.strip().startswith('FN '):
                        continue
                        
                    # WOS í˜•ì‹ íŒŒì‹±
                    df = parse_wos_format(file_content)
                    if df is not None and len(df) > 0:
                        encoding_used = encoding
                        break
                        
                except Exception:
                    continue
            
            if df is not None:
                all_dataframes.append(df)
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'SUCCESS',
                    'papers': len(df),
                    'encoding': encoding_used,
                    'message': f'âœ… {len(df)}í¸ ë…¼ë¬¸ ë¡œë”© ì„±ê³µ'
                })
            else:
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'ERROR',
                    'papers': 0,
                    'encoding': 'N/A',
                    'message': 'âŒ WOS Plain Text í˜•ì‹ì´ ì•„ë‹˜'
                })
                
        except Exception as e:
            file_status.append({
                'filename': uploaded_file.name,
                'status': 'ERROR',
                'papers': 0,
                'encoding': 'N/A',
                'message': f'âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}'
            })
    
    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ë³‘í•©
    if all_dataframes:
        merged_df = pd.concat(all_dataframes, ignore_index=True)
        original_count = len(merged_df)
        
        # ì¤‘ë³µ ì œê±° ë¡œì§ - ì™„ì „íˆ ìƒˆë¡œ ì‘ì„±
        duplicates_removed = 0
        
        if 'UT' in merged_df.columns:
            # UT í•„ë“œì˜ ì‹¤ì œ ê°’ë“¤ í™•ì¸
            ut_series = merged_df['UT'].copy()
            
            # ìœ íš¨í•œ UT ê°’ë§Œ í•„í„°ë§ (ë” ì—„ê²©í•œ ì¡°ê±´)
            def is_meaningful_ut(value):
                if pd.isna(value):
                    return False
                str_value = str(value).strip()
                # ë¹ˆ ë¬¸ìì—´, 'nan', 'None', ë§¤ìš° ì§§ì€ ê°’ë“¤ ì œì™¸
                if len(str_value) == 0 or str_value.lower() in ['nan', 'none', 'null', '']:
                    return False
                # WOS UTëŠ” ì¼ë°˜ì ìœ¼ë¡œ 'WOS:' ë˜ëŠ” ë¬¸ì+ìˆ«ì ì¡°í•©
                # ìµœì†Œ 10ì ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ê°’ë§Œ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
                if len(str_value) < 10:
                    return False
                # 'WOS:' ë¡œ ì‹œì‘í•˜ê±°ë‚˜ ì¶©ë¶„íˆ ê¸´ ì˜ìˆ«ì ì¡°í•©ì¸ ê²½ìš°ë§Œ ìœ íš¨
                if str_value.startswith('WOS:') or (len(str_value) >= 15 and any(c.isalnum() for c in str_value)):
                    return True
                return False
            
            # ìœ íš¨í•œ UTë¥¼ ê°€ì§„ í–‰ë“¤ë§Œ ì„ ë³„
            meaningful_ut_mask = ut_series.apply(is_meaningful_ut)
            rows_with_meaningful_ut = merged_df[meaningful_ut_mask]
            rows_without_meaningful_ut = merged_df[~meaningful_ut_mask]
            
            # ì‹¤ì œë¡œ ì˜ë¯¸ìˆëŠ” UTê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¤‘ë³µ ê²€ì‚¬
            if len(rows_with_meaningful_ut) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒ ìˆì–´ì•¼ ì¤‘ë³µ ê²€ì‚¬ ì˜ë¯¸
                # ì¤‘ë³µ ì œê±° ì „í›„ ë¹„êµ
                before_dedup = len(rows_with_meaningful_ut)
                deduplicated_meaningful = rows_with_meaningful_ut.drop_duplicates(subset=['UT'], keep='first')
                after_dedup = len(deduplicated_meaningful)
                
                actual_duplicates = before_dedup - after_dedup
                
                if actual_duplicates > 0:
                    duplicates_removed = actual_duplicates
                    
                    # ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ì™€ UT ì—†ëŠ” í–‰ë“¤ ì¬ê²°í•©
                    merged_df = pd.concat([deduplicated_meaningful, rows_without_meaningful_ut], ignore_index=True)
        
        # ëŒ€ì•ˆ: UTê°€ ì—†ê±°ë‚˜ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì œëª©+ì €ì ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        if duplicates_removed == 0 and 'TI' in merged_df.columns:
            # ì œëª©ê³¼ ì²« ë²ˆì§¸ ì €ì ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸ (ë§¤ìš° ë³´ìˆ˜ì )
            title_author_before = len(merged_df)
            
            # ì œëª©ì´ ìˆê³  ì €ìê°€ ìˆëŠ” í–‰ë“¤ë§Œ ëŒ€ìƒ
            has_title = merged_df['TI'].notna() & (merged_df['TI'].str.strip() != '')
            has_author = merged_df.get('AU', pd.Series()).notna() if 'AU' in merged_df.columns else pd.Series([False] * len(merged_df))
            
            complete_rows = merged_df[has_title & has_author] if 'AU' in merged_df.columns else merged_df[has_title]
            incomplete_rows = merged_df[~(has_title & has_author)] if 'AU' in merged_df.columns else merged_df[~has_title]
            
            if len(complete_rows) > 1:
                dedup_columns = ['TI', 'AU'] if 'AU' in merged_df.columns else ['TI']
                deduplicated_complete = complete_rows.drop_duplicates(subset=dedup_columns, keep='first')
                title_author_removed = len(complete_rows) - len(deduplicated_complete)
                
                if title_author_removed > 0:
                    duplicates_removed = title_author_removed
                    merged_df = pd.concat([deduplicated_complete, incomplete_rows], ignore_index=True)
        
        return merged_df, file_status, duplicates_removed
    else:
        return None, file_status, 0

def parse_wos_format(content):
    """WOS Plain Text í˜•ì‹ì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    lines = content.split('\n')
    records = []
    current_record = {}
    current_field = None
    
    for line in lines:
        line = line.rstrip()
        
        if not line:
            continue
            
        # ë ˆì½”ë“œ ì¢…ë£Œ
        if line == 'ER':
            if current_record:
                records.append(current_record.copy())
                current_record = {}
                current_field = None
            continue
            
        # í—¤ë” ë¼ì¸ ê±´ë„ˆë›°ê¸°
        if line.startswith(('FN ', 'VR ')):
            continue
            
        # ìƒˆ í•„ë“œ ì‹œì‘
        if not line.startswith('   ') and ' ' in line:
            parts = line.split(' ', 1)
            if len(parts) == 2:
                field_tag, field_value = parts
                current_field = field_tag
                current_record[field_tag] = field_value.strip()
        
        # ê¸°ì¡´ í•„ë“œ ì—°ì†
        elif line.startswith('   ') and current_field and current_field in current_record:
            continuation_value = line[3:].strip()
            if continuation_value:
                current_record[current_field] += '; ' + continuation_value
    
    # ë§ˆì§€ë§‰ ë ˆì½”ë“œ ì²˜ë¦¬
    if current_record:
        records.append(current_record)
    
    if not records:
        return None
        
    return pd.DataFrame(records)

# --- ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° íŠ¹í™” ë¶„ë¥˜ í•¨ìˆ˜ (ì‚¬ìš©ì ìš”ì²­ ê¸°ì¤€ ì ìš©) ---
def classify_article(row):
    """ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¥¸ í¬í•¨/ë°°ì œ ê¸°ì¤€(IC/EC)ì„ ì ìš©í•œ ë…¼ë¬¸ ë¶„ë¥˜ í•¨ìˆ˜"""
    
    # --- í‚¤ì›Œë“œ ì…‹ ì •ì˜ ---
    # IC1: ì£¼ì œ ì¤‘ì‹¬ì„± (ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° í•µì‹¬)
    core_streaming_keywords = [
        'live stream', 'livestream', 'live video', 'live broadcast', 'live e-commerce',
        'real-time stream', 'streaming platform', 'streaming service', 'live shopping',
        'live commerce', 'shoppertainment', 'streamer', 'viewer', 'broadcaster',
        'twitch', 'youtube live', 'facebook live', 'tiktok live', 'douyin', 'kuaishou', 'taobao live'
    ]
    
    # EC3: ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬
    interaction_keywords = [
        'interactive', 'interaction', 'real-time', 'real time', 'two-way', 'bidirectional',
        'synchronous', 'live chat', 'audience participation', 'user engagement', 'parasocial',
        'viewer engagement', 'community', 'social presence'
    ]

    # EC2: ì‚¬íšŒ-ê¸°ìˆ ì  ë§¥ë½ ë¶€ì¬ (ìˆœìˆ˜ ê¸°ìˆ )
    pure_tech_exclusions = [
        'routing protocol', 'network topology', 'mac protocol', 'tcp/ip', 'udp', 'rtmp', 'hls', 'dash',
        'video codec', 'audio codec', 'h.264', 'h.265', 'hevc', 'mpeg', 'video compression',
        'cdn architecture', 'server load balancing', 'edge server', 'bandwidth allocation', 'qos',
        'vlsi', 'fpga', 'asic', 'signal processing', 'modulation', 'mimo', 'ofdm', 'latency optimization'
    ]
    # ì‚¬íšŒ-ê¸°ìˆ ì  ë§¥ë½ì„ ë‚˜íƒ€ë‚´ëŠ” í‚¤ì›Œë“œ (EC2ë¥¼ í”¼í•˜ê¸° ìœ„í•¨)
    socio_tech_context_keywords = [
        'user behavior', 'psychology', 'motivation', 'engagement', 'addiction', 'parasocial', 
        'social presence', 'trust', 'social impact', 'cultural', 'community', 'identity', 
        'online culture', 'social capital', 'digital labor', 'commerce', 'marketing', 'influencer', 
        'brand', 'purchase intention', 'advertising', 'e-commerce', 'social commerce', 'creator economy'
    ]

    # EC1: ì£¼ì œ ê´€ë ¨ì„± ë¶€ì¡± (ë¶€ì°¨ì  ë§¥ë½)
    peripheral_mention_indicators = [
        'for example', 'such as', 'including', 'future work', 'future research', 'future direction'
    ]

    # EC4: í•™ìˆ ì  ì—„ë°€ì„± ë¶€ì¡±
    methodological_exclusion_types = [
        'editorial material', 'letter', 'book review', 'meeting abstract', 
        'proceedings paper', 'correction', 'errata', 'note', 'short survey', 'white paper'
    ]
    
    # EC5: ì¤‘ë³µ ê²Œì¬
    duplicate_indicators = [
        'extended version', 'preliminary version', 'conference version', 'short version'
    ]

    # --- í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ì¶œ ë° ê²°í•© ---
    def extract_text(value):
        return str(value).lower().strip() if pd.notna(value) else ""
    
    title = extract_text(row.get('TI', ''))
    abstract = extract_text(row.get('AB', ''))
    author_keywords = extract_text(row.get('DE', ''))
    keywords_plus = extract_text(row.get('ID', ''))
    document_type = extract_text(row.get('DT', ''))
    language = extract_text(row.get('LA', ''))
    
    full_text = ' '.join([title, abstract, author_keywords, keywords_plus])
    
    # --- ë¶„ë¥˜ ë¡œì§ (ê³„ì¸µì  í•„í„°ë§) ---
    
    # [EC4/IC2] í•™ìˆ ì  í˜•íƒœ ë° ì—„ë°€ì„± ê²€ì¦
    if any(doc_type in document_type for doc_type in methodological_exclusion_types):
        return 'EC4 - í•™ìˆ ì  ì—„ë°€ì„± ë¶€ì¡±'
    if not any(doc in document_type for doc in ['article', 'review']):
        return 'EC4 - í•™ìˆ ì  ì—„ë°€ì„± ë¶€ì¡±'

    # [IC3] ì–¸ì–´ ê²€ì¦
    if language and language != 'english':
        return 'EC - Non-English'

    # [EC5] ì¤‘ë³µ ê²Œì¬ ê²€ì¦
    if any(indicator in full_text for indicator in duplicate_indicators):
        return 'EC5 - ì¤‘ë³µ ê²Œì¬'

    # [IC1/EC1] ì£¼ì œ ì¤‘ì‹¬ì„± ë° ê´€ë ¨ì„± ë¶€ì¡± ê²€ì¦
    has_core_keyword = any(kw in full_text for kw in core_streaming_keywords)
    if not has_core_keyword:
        return 'EC1 - ì£¼ì œ ê´€ë ¨ì„± ë¶€ì¡±'
    
    is_peripheral = any(indicator in full_text for indicator in peripheral_mention_indicators)
    core_keyword_count = sum(1 for kw in core_streaming_keywords if kw in full_text)
    if is_peripheral and core_keyword_count <= 2:
        return 'EC1 - ì£¼ì œ ê´€ë ¨ì„± ë¶€ì¡±'

    # [EC3] ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬ ê²€ì¦
    has_interaction = any(kw in full_text for kw in interaction_keywords)
    if not has_interaction:
        return 'EC3 - ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬'

    # [EC2] ì‚¬íšŒ-ê¸°ìˆ ì  ë§¥ë½ ë¶€ì¬ ê²€ì¦
    is_pure_tech = any(kw in full_text for kw in pure_tech_exclusions)
    has_socio_context = any(kw in full_text for kw in socio_tech_context_keywords)
    if is_pure_tech and not has_socio_context:
        return 'EC2 - ì‚¬íšŒ-ê¸°ìˆ ì  ë§¥ë½ ë¶€ì¬'

    # ëª¨ë“  ì œì™¸ ê¸°ì¤€ì„ í†µê³¼í•œ ê²½ìš°, í¬í•¨
    return 'Include'


# --- ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ í•¨ìˆ˜ ---
def diagnose_merged_quality(df, file_count, duplicates_removed):
    """ë³‘í•©ëœ WOS ë°ì´í„°ì˜ í’ˆì§ˆ ì§„ë‹¨ - ìˆ˜ì •ëœ ë²„ì „"""
    issues = []
    recommendations = []
    
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    required_fields = ['TI', 'AU', 'SO', 'PY']
    keyword_fields = ['DE', 'ID']
    
    for field in required_fields:
        if field not in df.columns:
            issues.append(f"âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        else:
            valid_count = df[field].notna().sum()
            total_count = len(df)
            missing_rate = (total_count - valid_count) / total_count * 100
            
            if missing_rate > 10:
                issues.append(f"âš ï¸ {field} í•„ë“œì˜ {missing_rate:.1f}%ê°€ ëˆ„ë½ë¨")
    
    # í‚¤ì›Œë“œ í•„ë“œ í’ˆì§ˆ í™•ì¸
    has_keywords = False
    for field in keyword_fields:
        if field in df.columns:
            has_keywords = True
            valid_keywords = df[field].notna() & (df[field] != '') & (df[field] != 'nan')
            valid_count = valid_keywords.sum()
            total_count = len(df)
            
            if valid_count < total_count * 0.7:
                issues.append(f"âš ï¸ {field} í•„ë“œì˜ {((total_count-valid_count)/total_count*100):.1f}%ê°€ ë¹„ì–´ìˆìŒ")
    
    if not has_keywords:
        issues.append("âŒ í‚¤ì›Œë“œ í•„ë“œ ì—†ìŒ: DE ë˜ëŠ” ID í•„ë“œ í•„ìš”")
    
    # ë³‘í•© ê´€ë ¨ ì •ë³´ - ì‹¤ì œ ê²°ê³¼ë§Œ ë°˜ì˜ (ì™„ì „ ìˆ˜ì •)
    recommendations.append(f"âœ… {file_count}ê°œ íŒŒì¼ ì„±ê³µì ìœ¼ë¡œ ë³‘í•©ë¨")
    
    # ì¤‘ë³µ ì œê±° ê²°ê³¼ë§Œ ì‹¤ì œ ë°ì´í„°ì— ë”°ë¼ í‘œì‹œ
    if duplicates_removed > 0:
        recommendations.append(f"ğŸ”„ ì¤‘ë³µ ë…¼ë¬¸ {duplicates_removed}í¸ ìë™ ì œê±°ë¨")
    else:
        recommendations.append("âœ… ì¤‘ë³µ ë…¼ë¬¸ ì—†ìŒ - ëª¨ë“  ë…¼ë¬¸ì´ ê³ ìœ  ë°ì´í„°")
    
    recommendations.append("âœ… WOS Plain Text í˜•ì‹ - SCIMAT ìµœì  í˜¸í™˜ì„± í™•ë³´")
    
    return issues, recommendations

# --- WOS Plain Text í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ---
def convert_to_scimat_wos_format(df_to_convert):
    """SCIMAT ì™„ì „ í˜¸í™˜ WOS Plain Text í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']

    for _, row in df_to_convert.iterrows():
        if len(file_content) > 2:
            file_content.append("")
        
        for tag in wos_field_order:
            if tag in row.index and pd.notna(row[tag]) and str(row[tag]).strip() and str(row[tag]).strip().lower() != 'nan':
                value = str(row[tag]).strip()
                
                if tag in multi_line_fields:
                    items = [item.strip() for item in value.split(';') if item.strip()]
                    if items:
                        file_content.append(f"{tag} {items[0]}")
                        for item in items[1:]:
                            file_content.append(f"   {item}")
                else:
                    file_content.append(f"{tag} {value}")
        
        file_content.append("ER")
    
    return "\n".join(file_content).encode('utf-8-sig')

# --- ë©”ì¸ í—¤ë” ---
st.markdown("""
<div style="position: relative; text-align: center; padding: 2.5rem 0 3rem 0; background: linear-gradient(135deg, #3182f6, #1c64f2); color: white; border-radius: 8px; margin-bottom: 1.5rem; box-shadow: 0 2px 8px rgba(49,130,246,0.15); overflow: hidden;">
    <div style="position: absolute; top: 1rem; left: 1.5rem; color: white;">
        <div style="font-size: 12px; font-weight: 600; margin-bottom: 3px; letter-spacing: 0.3px;">HANYANG UNIVERSITY</div>
        <div style="font-size: 11px; opacity: 0.9; font-weight: 500;">Technology Management Research</div>
        <div style="font-size: 10px; opacity: 0.8; margin-top: 4px; font-weight: 400;">mot.hanyang.ac.kr</div>
    </div>
    <div style="position: absolute; top: 1rem; right: 1.5rem; text-align: right; color: rgba(255,255,255,0.9); font-size: 11px;">
        <p style="margin: 0; font-weight: 500;">Developed by: ì„íƒœê²½ (Teddy Lym)</p>
    </div>
    <h1 style="font-size: 3rem; font-weight: 700; margin-bottom: 0.3rem; letter-spacing: -0.02em;">
        WOS PREP
    </h1>
    <p style="font-size: 1.1rem; margin: 0; font-weight: 500; opacity: 0.95; letter-spacing: -0.01em;">
        SCIMAT Edition
    </p>
    <div style="width: 80px; height: 3px; background-color: rgba(255,255,255,0.3); margin: 1.5rem auto; border-radius: 2px;"></div>
</div>
""", unsafe_allow_html=True)

# --- í•µì‹¬ ê¸°ëŠ¥ ì†Œê°œ ---
st.markdown("""
<div class="feature-grid">
    <div class="feature-card">
        <div class="feature-icon">ğŸ”—</div>
        <div class="feature-title">ë‹¤ì¤‘ íŒŒì¼ ìë™ ë³‘í•©</div>
        <div class="feature-desc">ì—¬ëŸ¬ WOS íŒŒì¼ì„ í•œ ë²ˆì— ë³‘í•© ì²˜ë¦¬</div>
    </div>
    <div class="feature-card">
        <div class="feature-icon">ğŸš«</div>
        <div class="feature-title">ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì œê±°</div>
        <div class="feature-desc">UT ê¸°ì¤€ ìë™ ì¤‘ë³µ ë…¼ë¬¸ ê°ì§€ ë° ì œê±°</div>
    </div>
    <div class="feature-card">
        <div class="feature-icon">ğŸ¯</div>
        <div class="feature-title">í•™ìˆ ì  ì—„ë°€ì„±</div>
        <div class="feature-desc">ê°œë… ê¸°ë°˜ í•™ìˆ ì  ì •ì œ ì ìš©</div>
    </div>
</div>
""", unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
st.markdown("""
<div class="section-header">
    <div class="section-title">ğŸ“‚ ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ì—…ë¡œë“œ</div>
    <div class="section-subtitle">500ê°œ ë‹¨ìœ„ë¡œ ë‚˜ë‰œ ì—¬ëŸ¬ WOS íŒŒì¼ì„ ëª¨ë‘ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”</div>
</div>
""", unsafe_allow_html=True)

uploaded_files = st.file_uploader(
    "WOS Plain Text íŒŒì¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
    type=['txt'],
    accept_multiple_files=True,
    label_visibility="collapsed",
    help="WOS Plain Text íŒŒì¼ë“¤ì„ ë“œë˜ê·¸í•˜ì—¬ ë†“ê±°ë‚˜ í´ë¦­í•˜ì—¬ ì„ íƒí•˜ì„¸ìš”"
)

if uploaded_files:
    st.markdown(f"ğŸ“‹ **ì„ íƒëœ íŒŒì¼ ê°œìˆ˜:** {len(uploaded_files)}ê°œ")
    
    # í”„ë¡œê·¸ë ˆìŠ¤ ì¸ë””ì¼€ì´í„°
    st.markdown('<div class="progress-indicator"></div>', unsafe_allow_html=True)
    
    with st.spinner(f"ğŸ”„ {len(uploaded_files)}ê°œ WOS íŒŒì¼ ë³‘í•© ë° í•™ìˆ ì  ì •ì œ ì ìš© ì¤‘..."):
        # íŒŒì¼ ë³‘í•©
        merged_df, file_status, duplicates_removed = load_and_merge_wos_files(uploaded_files)
        
        if merged_df is None:
            st.error("âš ï¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ WOS Plain Text íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ë“¤ì´ Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì •í’ˆ Plain Text íŒŒì¼ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            # íŒŒì¼ë³„ ìƒíƒœ í‘œì‹œ
            st.markdown("### ğŸ“„ íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ")
            for status in file_status:
                st.markdown(f"""
                <div class="file-status">
                    <strong>{status['filename']}</strong><br>
                    {status['message']}
                </div>
                """, unsafe_allow_html=True)
            st.stop()
        
        # ë…¼ë¬¸ ë¶„ë¥˜ ìˆ˜í–‰ - ê°•í™”ëœ ê¸°ì¤€ ì ìš©
        merged_df['Classification'] = merged_df.apply(classify_article, axis=1)

    # ì„±ê³µì ì¸ íŒŒì¼ ê°œìˆ˜ ê³„ì‚°
    successful_files = len([s for s in file_status if s['status'] == 'SUCCESS'])
    total_papers_before_filter = len(merged_df)
    
    # ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„
    df_excluded_strict = merged_df[~merged_df['Classification'].str.startswith('Include', na=False)]
    df_for_analysis = merged_df[merged_df['Classification'].str.startswith('Include', na=False)].copy()
    
    total_papers = len(df_for_analysis)
    
    st.success(f"âœ… ë³‘í•© ë° í•™ìˆ ì  ì •ì œ ì™„ë£Œ! {successful_files}ê°œ íŒŒì¼ì—ì„œ ìµœì¢… {total_papers:,}í¸ì˜ ë…¼ë¬¸ì„ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    
    # ì¤‘ë³µ ì œê±° ê²°ê³¼ í‘œì‹œ - ì‹¤ì œ ê²°ê³¼ë§Œ
    if duplicates_removed > 0:
        st.info(f"ğŸ”„ ì¤‘ë³µ ë…¼ë¬¸ {duplicates_removed}í¸ì´ ìë™ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. (ì›ë³¸ ì´ {total_papers_before_filter + duplicates_removed:,}í¸ â†’ ì •ì œ í›„ {total_papers_before_filter:,}í¸)")
    else:
        st.info("âœ… ì¤‘ë³µ ë…¼ë¬¸ ì—†ìŒ - ëª¨ë“  ë…¼ë¬¸ì´ ê³ ìœ í•œ ë°ì´í„°ì…ë‹ˆë‹¤.")

    # --- íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ“„ íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ</div>
        <div class="section-subtitle">ì—…ë¡œë“œëœ ê° íŒŒì¼ì˜ ì²˜ë¦¬ ê²°ê³¼</div>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ğŸ“‹ íŒŒì¼ë³„ ìƒì„¸ ìƒíƒœ</div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns([0.6, 0.4])
    
    with col1:        
        for status in file_status:
            color = "#10b981" if status['status'] == 'SUCCESS' else "#ef4444"
            icon = "âœ…" if status['status'] == 'SUCCESS' else "âŒ"
            
            st.markdown(f"""
            <div style="margin: 12px 0; padding: 16px; background: white; border-left: 4px solid {color}; border-radius: 12px; box-shadow: 0 1px 3px rgba(0,0,0,0.04);">
                <strong>{icon} {status['filename']}</strong><br>
                <small style="color: #8b95a1;">{status['message']}</small>
                {f" | ì¸ì½”ë”©: {status['encoding']}" if status['encoding'] != 'N/A' else ""}
            </div>
            """, unsafe_allow_html=True)
    
    with col2:
        # íŒŒì¼ ì²˜ë¦¬ í†µê³„
        success_count = len([s for s in file_status if s['status'] == 'SUCCESS'])
        error_count = len([s for s in file_status if s['status'] == 'ERROR'])
        
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">âœ…</div>
            <div class="metric-value">{success_count}</div>
            <div class="metric-label">ì„±ê³µí•œ íŒŒì¼</div>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">âŒ</div>
            <div class="metric-value">{error_count}</div>
            <div class="metric-label">ì‹¤íŒ¨í•œ íŒŒì¼</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("</div>", unsafe_allow_html=True)

    # --- ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ê²°ê³¼ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ” ë³‘í•© ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨</div>
        <div class="section-subtitle">ë³‘í•©ëœ WOS ë°ì´í„°ì˜ í’ˆì§ˆê³¼ SCIMAT í˜¸í™˜ì„± ê²€ì¦</div>
    </div>
    """, unsafe_allow_html=True)

    with st.spinner("ğŸ” ë³‘í•© ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì¤‘..."):
        issues, recommendations = diagnose_merged_quality(df_for_analysis, successful_files, duplicates_removed)

    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ğŸ” ë³‘í•© ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ê²°ê³¼</div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown('<h5 style="color: #ef4444; margin-bottom: 16px;">ğŸš¨ ë°œê²¬ëœ ë¬¸ì œì  (Identified Issues)</h5>', unsafe_allow_html=True)
        
        if issues:
            for issue in issues:
                st.markdown(f"- {issue}")
        else:
            st.markdown("âœ… **ë¬¸ì œì  ì—†ìŒ** - ë³‘í•© ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜")
    
    with col2:
        st.markdown('<h5 style="color: #10b981; margin-bottom: 16px;">ğŸ’¡ ë³‘í•© ê²°ê³¼ (Merge Results)</h5>', unsafe_allow_html=True)
        
        if recommendations:
            for rec in recommendations:
                st.markdown(f"- {rec}")
        else:
            st.markdown("ğŸ¯ **ìµœì  ìƒíƒœ** - SCIMAT ì™„ë²½ í˜¸í™˜")
    
    st.markdown("</div>", unsafe_allow_html=True)

    # ë³‘í•© ì„±ê³µ ì•Œë¦¼
    st.markdown("""
    <div class="success-panel">
        <h4 style="color: #065f46; margin-bottom: 20px; font-weight: 700;">ğŸ¯ ë‹¤ì¤‘ íŒŒì¼ ë³‘í•© ë° í•™ìˆ ì  ì •ì œ ì„±ê³µ!</h4>
        <p style="color: #065f46; margin: 6px 0; font-weight: 500;">ì—¬ëŸ¬ WOS Plain Text íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ í•˜ë‚˜ë¡œ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        <p style="color: #065f46; margin: 6px 0; font-weight: 500;"><strong>í•™ìˆ ì  ì—„ë°€ì„±:</strong> ê°•í™”ëœ í¬í•¨/ë°°ì œ ê¸°ì¤€ì„ ì²´ê³„ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ì—°êµ¬ì˜ ì‹ ë¢°ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.</p>
        <p style="color: #065f46; margin: 6px 0; font-weight: 500;"><strong>SCIMAT í˜¸í™˜ì„±:</strong> ë³‘í•©ëœ íŒŒì¼ì€ SCIMATì—ì„œ 100% ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.</p>
    </div>
    """, unsafe_allow_html=True)

    # --- ë¶„ì„ ê²°ê³¼ ìš”ì•½ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ“ˆ í•™ìˆ ì  ì •ì œ ê²°ê³¼</div>
        <div class="section-subtitle">í•™ìˆ ì  ì •ì œ ê¸°ì¤€ ì ìš© í›„ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ ë¶„ë¥˜ ê²°ê³¼</div>
    </div>
    """, unsafe_allow_html=True)

    # ì´ ë°°ì œëœ ë…¼ë¬¸ ìˆ˜ ê³„ì‚°
    total_excluded = len(df_excluded_strict)
    
    # Classification ì»¬ëŸ¼ë§Œ ì œê±° (ì›ë³¸ WOS í˜•ì‹ ìœ ì§€)
    df_final_output = df_for_analysis.drop(columns=['Classification'], errors='ignore')
    
    # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“‹</div>
            <div class="metric-value">{len(df_final_output):,}</div>
            <div class="metric-label">ìµœì¢… ë¶„ì„ ëŒ€ìƒ (Final)<br><small style="color: #8b95a1;">(ì •ì œ ê¸°ì¤€ ì ìš©í›„)</small></div>
        </div>
        """, unsafe_allow_html=True)
    
    include_papers = len(df_for_analysis)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">âœ…</div>
            <div class="metric-value">{include_papers:,}</div>
            <div class="metric-label">í•µì‹¬ í¬í•¨ ì—°êµ¬ (Included)</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        processing_rate = (len(df_final_output) / total_papers_before_filter * 100) if total_papers_before_filter > 0 else 0
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“Š</div>
            <div class="metric-value">{processing_rate:.1f}%</div>
            <div class="metric-label">ìµœì¢… í¬í•¨ ë¹„ìœ¨ (Inclusion Rate)</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        # ë°°ì œëœ ë…¼ë¬¸ë“¤ì„ ìœ„í•œ í† ê¸€ ë²„íŠ¼ì´ ìˆëŠ” ë°•ìŠ¤
        col4_inner1, col4_inner2 = st.columns([3, 1])
        
        with col4_inner1:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-icon">â›”</div>
                <div class="metric-value">{total_excluded:,}</div>
                <div class="metric-label">í•™ìˆ ì  ë°°ì œ (Excluded)</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col4_inner2:
            st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)
            if st.button(
                "ğŸ“‹", 
                key="exclude_details_button",
                help="ë°°ì œëœ ë…¼ë¬¸ ìƒì„¸ ë³´ê¸° (View Excluded Papers)"
            ):
                st.session_state['show_exclude_details'] = not st.session_state.get('show_exclude_details', False)

    # ë°°ì œëœ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ í† ê¸€ í‘œì‹œ
    if st.session_state.get('show_exclude_details', False) and total_excluded > 0:
        st.markdown("""
        <div style="background: #fef2f2; padding: 20px; border-radius: 16px; margin: 20px 0; border: 1px solid #ef4444;">
            <h4 style="color: #dc2626; margin-bottom: 16px; font-weight: 700;">â›” í•™ìˆ ì  ë°°ì œ ê¸°ì¤€ì— ë”°ë¥¸ ì œì™¸ ë…¼ë¬¸ (Excluded Papers by Criteria)</h4>
        </div>
        """, unsafe_allow_html=True)
        
        # --- ì œì™¸ ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€ ---
        excluded_excel_data = []
        for idx, (_, paper) in enumerate(df_excluded_strict.iterrows(), 1):
            excluded_excel_data.append({
                'No (ë²ˆí˜¸)': idx,
                'Title (ë…¼ë¬¸ ì œëª©)': str(paper.get('TI', 'N/A')),
                'Year (ì¶œíŒì—°ë„)': str(paper.get('PY', 'N/A')),
                'Journal (ì €ë„ëª…)': str(paper.get('SO', 'N/A')),
                'Authors (ì €ì)': str(paper.get('AU', 'N/A')),
                'Exclusion Reason (ì œì™¸ ì‚¬ìœ )': str(paper.get('Classification', 'N/A')),
                'Author Keywords (ì €ì í‚¤ì›Œë“œ)': str(paper.get('DE', 'N/A')),
                'WOS Keywords (WOS í‚¤ì›Œë“œ)': str(paper.get('ID', 'N/A')),
                'Abstract (ì´ˆë¡)': str(paper.get('AB', 'N/A')),
                'Document Type (ë¬¸ì„œìœ í˜•)': str(paper.get('DT', 'N/A'))
            })
        
        excluded_excel_df = pd.DataFrame(excluded_excel_data)
        excel_buffer = io.BytesIO()
        with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
            excluded_excel_df.to_excel(writer, sheet_name='Excluded_Papers', index=False)
        excel_data = excel_buffer.getvalue()
        
        st.download_button(
            label="ğŸ“Š ì œì™¸ëœ ë…¼ë¬¸ ì „ì²´ ëª©ë¡ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ (Download Excluded Papers as Excel)",
            data=excel_data,
            file_name=f"excluded_papers_list_{len(df_excluded_strict)}papers.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        st.markdown("<br>", unsafe_allow_html=True)

        # ë°°ì œ ê¸°ì¤€ë³„ ë¶„ë¥˜ ë° í‘œì‹œ
        exclusion_categories = {
            'EC1': 'ì£¼ì œ ê´€ë ¨ì„± ë¶€ì¡±',
            'EC2': 'ì‚¬íšŒ-ê¸°ìˆ ì  ë§¥ë½ ë¶€ì¬',
            'EC3': 'ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬',
            'EC4': 'í•™ìˆ ì  ì—„ë°€ì„± ë¶€ì¡±',
            'EC5': 'ì¤‘ë³µ ê²Œì¬',
        }
        
        # EC ê¸°ì¤€ë³„ ë°°ì œ í˜„í™©
        for ec_code, description in exclusion_categories.items():
            ec_papers = df_excluded_strict[df_excluded_strict['Classification'].str.startswith(ec_code, na=False)]
            if len(ec_papers) > 0:
                st.markdown(f"""
                <div style="margin: 12px 0; padding: 16px; background: white; border-left: 4px solid #ef4444; border-radius: 12px;">
                    <strong style="color: #dc2626;">{ec_code}: {description}</strong> 
                    <span style="color: #8b95a1;">({len(ec_papers)}í¸)</span>
                </div>
                """, unsafe_allow_html=True)
                
                # ìƒìœ„ 5í¸ë§Œ ìƒ˜í”Œë¡œ í‘œì‹œ
                for idx, (_, paper) in enumerate(ec_papers.head(5).iterrows(), 1):
                    title = str(paper.get('TI', 'N/A'))[:80] + "..." if len(str(paper.get('TI', 'N/A'))) > 80 else str(paper.get('TI', 'N/A'))
                    year = str(paper.get('PY', 'N/A'))
                    source = str(paper.get('SO', 'N/A'))[:40] + "..." if len(str(paper.get('SO', 'N/A'))) > 40 else str(paper.get('SO', 'N/A'))
                    
                    st.markdown(f"""
                    <div style="margin: 8px 0 8px 20px; padding: 12px; background: #f9fafb; border-radius: 8px; font-size: 14px;">
                        <div style="font-weight: 500; color: #374151; margin-bottom: 4px;">{title}</div>
                        <div style="color: #6b7280; font-size: 12px;">{year} | {source}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                if len(ec_papers) > 5:
                    st.markdown(f"<p style='color: #8b95a1; text-align: right; margin: 8px 20px 16px 20px; font-size: 12px;'>... ì™¸ {len(ec_papers) - 5}í¸ ë”</p>", unsafe_allow_html=True)

    # ë°°ì œ ê¸°ì¤€ ì ìš© ê²°ê³¼ ìš”ì•½
    st.markdown(f"""
    <div class="info-panel">
        <h4 style="color: #0064ff; margin-bottom: 16px; font-weight: 700;">ğŸ“Š í•™ìˆ ì  ì—„ë°€ì„± í™•ë³´</h4>
        <p style="color: #0064ff; margin: 6px 0; font-weight: 500;"><strong>ì´ ì…ë ¥ (Total Input):</strong> {total_papers_before_filter:,}í¸ì˜ ë…¼ë¬¸</p>
        <p style="color: #0064ff; margin: 6px 0; font-weight: 500;"><strong>ë°°ì œ ì ìš© (Excluded):</strong> {total_excluded:,}í¸ ì œì™¸ ({(total_excluded/total_papers_before_filter*100):.1f}%)</p>
        <p style="color: #0064ff; margin: 6px 0; font-weight: 500;"><strong>ìµœì¢… ë¶„ì„ (Final Dataset):</strong> {len(df_final_output):,}í¸ìœ¼ë¡œ ì •ì œëœ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹</p>
        <p style="color: #0064ff; margin: 6px 0; font-weight: 500;"><strong>í•µì‹¬ ì—°êµ¬ (Core Research):</strong> {include_papers:,}í¸ì˜ ì§ì ‘ ê´€ë ¨ ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ í™•ë³´</p>
    </div>
    """, unsafe_allow_html=True)

    # --- ë…¼ë¬¸ ë¶„ë¥˜ í˜„í™© ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">í•™ìˆ ì  ì •ì œ í›„ ì—°êµ¬ ë¶„ë¥˜ ë¶„í¬ (Distribution of Research after Filtering)</div>
    """, unsafe_allow_html=True)

    classification_counts_df = df_for_analysis['Classification'].value_counts().reset_index()
    classification_counts_df.columns = ['Classification (ë¶„ë¥˜)', 'Count (ë…¼ë¬¸ ìˆ˜)']

    col1, col2 = st.columns([0.4, 0.6])
    with col1:
        st.dataframe(classification_counts_df, use_container_width=True, hide_index=True)

    with col2:
        # ë„ë„› ì°¨íŠ¸
        selection = alt.selection_point(fields=['Classification (ë¶„ë¥˜)'], on='mouseover', nearest=True)

        base = alt.Chart(classification_counts_df).encode(
            theta=alt.Theta(field="Count (ë…¼ë¬¸ ìˆ˜)", type="quantitative", stack=True),
            color=alt.Color(field="Classification (ë¶„ë¥˜)", type="nominal", title="Classification (ë¶„ë¥˜)",
                           scale=alt.Scale(scheme='tableau20'),
                           legend=alt.Legend(orient="right", titleColor="#191f28", labelColor="#8b95a1")),
            opacity=alt.condition(selection, alt.value(1), alt.value(0.8))
        ).add_params(selection)

        pie = base.mark_arc(outerRadius=150, innerRadius=90)
        text_total = alt.Chart(pd.DataFrame([{'value': f'{len(df_final_output)}'}])).mark_text(
            align='center', baseline='middle', fontSize=45, fontWeight='bold', color='#0064ff'
        ).encode(text='value:N')
        text_label = alt.Chart(pd.DataFrame([{'value': 'Included Papers'}])).mark_text(
            align='center', baseline='middle', fontSize=16, dy=30, color='#8b95a1'
        ).encode(text='value:N')

        chart = (pie + text_total + text_label).properties(
            width=350, height=350
        ).configure_view(strokeWidth=0)
        st.altair_chart(chart, use_container_width=True)

    st.markdown("</div>", unsafe_allow_html=True)

    # --- ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ ---
    if 'PY' in df_final_output.columns:
        st.markdown("""
        <div class="chart-container">
            <div class="chart-title">ì •ì œëœ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ ë™í–¥ (Publication Trend of Refined Papers)</div>
        """, unsafe_allow_html=True)
        
        df_trend = df_final_output.copy()
        df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
        df_trend.dropna(subset=['PY'], inplace=True)
        df_trend['PY'] = df_trend['PY'].astype(int)
        
        yearly_counts = df_trend['PY'].value_counts().reset_index()
        yearly_counts.columns = ['Year (ë°œí–‰ ì—°ë„)', 'Count (ë…¼ë¬¸ ìˆ˜)']
        yearly_counts = yearly_counts[yearly_counts['Year (ë°œí–‰ ì—°ë„)'] <= 2025].sort_values('Year (ë°œí–‰ ì—°ë„)')

        if len(yearly_counts) > 0:
            line_chart = alt.Chart(yearly_counts).mark_line(
                point={'size': 80, 'filled': True}, strokeWidth=3, color='#0064ff'
            ).encode(
                x=alt.X('Year (ë°œí–‰ ì—°ë„):O', title='Publication Year (ë°œí–‰ ì—°ë„)'),
                y=alt.Y('Count (ë…¼ë¬¸ ìˆ˜):Q', title='Number of Papers (ë…¼ë¬¸ ìˆ˜)'),
                tooltip=['Year (ë°œí–‰ ì—°ë„)', 'Count (ë…¼ë¬¸ ìˆ˜)']
            ).properties(height=300)
            
            st.altair_chart(line_chart, use_container_width=True)
        
        st.markdown("</div>", unsafe_allow_html=True)

    # --- í‚¤ì›Œë“œ ìƒ˜í”Œ í™•ì¸ ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ì •ì œëœ ë°ì´í„° í‚¤ì›Œë“œ í’ˆì§ˆ í™•ì¸ (Keyword Quality Check)</div>
    """, unsafe_allow_html=True)
    
    sample_data = []
    sample_rows = df_for_analysis.head(3)
    
    for idx, row in sample_rows.iterrows():
        title = str(row.get('TI', 'N/A'))[:80] + "..." if len(str(row.get('TI', 'N/A'))) > 80 else str(row.get('TI', 'N/A'))
        de_keywords = str(row.get('DE', 'N/A')) if pd.notna(row.get('DE')) else 'N/A'
        id_keywords = str(row.get('ID', 'N/A')) if pd.notna(row.get('ID')) else 'N/A'
        
        # í‚¤ì›Œë“œ ê°œìˆ˜ ê³„ì‚°
        de_count = len([k.strip() for k in de_keywords.split(';') if k.strip()]) if de_keywords != 'N/A' else 0
        id_count = len([k.strip() for k in id_keywords.split(';') if k.strip()]) if id_keywords != 'N/A' else 0
        
        sample_data.append({
            'Title (ë…¼ë¬¸ ì œëª©)': title,
            'DE Keywords': de_keywords[:100] + "..." if len(de_keywords) > 100 else de_keywords,
            'ID Keywords': id_keywords[:100] + "..." if len(id_keywords) > 100 else id_keywords,
            'DE Count': de_count,
            'ID Count': id_count
        })
    
    if sample_data:
        sample_df = pd.DataFrame(sample_data)
        st.dataframe(sample_df, use_container_width=True, hide_index=True)
        
        # í‚¤ì›Œë“œ í’ˆì§ˆ í‰ê°€
        avg_de = sum([d['DE Count'] for d in sample_data]) / len(sample_data) if sample_data else 0
        avg_id = sum([d['ID Count'] for d in sample_data]) / len(sample_data) if sample_data else 0
        
        if avg_de >= 3 and avg_id >= 3:
            st.success("âœ… í‚¤ì›Œë“œ í’ˆì§ˆ ìš°ìˆ˜ - SCIMATì—ì„œ ì›í™œí•œ ê·¸ë£¨í•‘ ì˜ˆìƒ")
        elif avg_de >= 2 or avg_id >= 2:
            st.warning("âš ï¸ í‚¤ì›Œë“œ í’ˆì§ˆ ë³´í†µ - SCIMATì—ì„œ ì¼ë¶€ ì œí•œ ê°€ëŠ¥")
        else:
            st.error("âŒ í‚¤ì›Œë“œ í’ˆì§ˆ ë¶€ì¡± - ì›ë³¸ WOS ë‹¤ìš´ë¡œë“œ ì„¤ì • í™•ì¸ í•„ìš”")

    st.markdown("</div>", unsafe_allow_html=True)

    # (ê¸°ì¡´ Review ë…¼ë¬¸ ìƒì„¸ ëª©ë¡ ì„¹ì…˜ì€ ìƒˆë¡œìš´ ë¶„ë¥˜ ê¸°ì¤€ì— ë”°ë¼ ë¶ˆí•„ìš”í•˜ì—¬ ì œê±°)

    # ë³‘í•© ì„±ê³¼ ê°•ì¡° - í•™ìˆ ì  ì—„ë°€ì„± ë°˜ì˜
    success_info = []
    success_info.append(f"<strong>íŒŒì¼ í†µí•©:</strong> {successful_files}ê°œì˜ WOS íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë³‘í•©")
    
    if duplicates_removed > 0:
        success_info.append(f"<strong>ì¤‘ë³µ ì œê±°:</strong> {duplicates_removed}í¸ì˜ ì¤‘ë³µ ë…¼ë¬¸ ìë™ ê°ì§€ ë° ì œê±°")
    
    success_info.append(f"<strong>í•™ìˆ ì  ì—„ë°€ì„±:</strong> ê°œë… ê¸°ë°˜ ë°°ì œ ê¸°ì¤€ìœ¼ë¡œ {total_excluded}í¸ ì œì™¸")
    success_info.append(f"<strong>ìµœì¢… ê·œëª¨:</strong> {len(df_final_output):,}í¸ì˜ ê³ í’ˆì§ˆ ë…¼ë¬¸ìœ¼ë¡œ ì •ì œëœ ë°ì´í„°ì…‹")
    success_info.append(f"<strong>í•µì‹¬ ì—°êµ¬:</strong> {include_papers}í¸ì˜ ì§ì ‘ ê´€ë ¨ ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ í™•ë³´")
    success_info.append("<strong>SCIMAT í˜¸í™˜:</strong> ì™„ë²½í•œ WOS Plain Text í˜•ì‹ìœ¼ë¡œ 100% í˜¸í™˜ì„± ë³´ì¥")
    
    success_content = "".join([f"<p style='color: #0064ff; margin: 6px 0; font-weight: 500;'>{info}</p>" for info in success_info])
    
    st.markdown(f"""
    <div class="info-panel">
        <h4 style="color: #0064ff; margin-bottom: 16px; font-weight: 700;">ğŸ¯ í•™ìˆ ì  ë°ì´í„° ì •ì œ ì™„ë£Œ</h4>
        {success_content}
        <div style="margin-top: 16px; padding: 12px; background: rgba(0,100,255,0.1); border-radius: 8px;">
            <p style='color: #0064ff; margin: 0; font-weight: 600; font-size: 14px;'>
            ğŸ’¡ <strong>ë°°ì œ ê¸°ì¤€ ì ìš©ë¥ :</strong> {(total_excluded/total_papers_before_filter*100):.1f}% 
            - ì—°êµ¬ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê¸°ì—¬í•˜ëŠ” ë…¼ë¬¸ë§Œì„ ì„ ë³„í•˜ì—¬ ë¶„ì„ì˜ ê¹Šì´ì™€ ì‹ ë¢°ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.
            </p>
        </div>
    </div>
    """, unsafe_allow_html=True)

    # --- ìµœì¢… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ“¥ í•™ìˆ ì  ì •ì œ ì™„ë£Œ - SCIMAT ë¶„ì„ìš© íŒŒì¼ ë‹¤ìš´ë¡œë“œ</div>
        <div class="section-subtitle">ê°•í™”ëœ í¬í•¨/ë°°ì œ ê¸°ì¤€ ì ìš© í›„ ì •ì œëœ ê³ í’ˆì§ˆ WOS Plain Text íŒŒì¼</div>
    </div>
    """, unsafe_allow_html=True)
    
    # SCIMAT í˜¸í™˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    text_data = convert_to_scimat_wos_format(df_final_output)
    
    download_clicked = st.download_button(
        label="ğŸ“¥ ë‹¤ìš´ë¡œë“œ (Download)",
        data=text_data,
        file_name=f"live_streaming_academic_filtered_scimat_{len(df_final_output)}papers.txt",
        mime="text/plain",
        type="primary",
        use_container_width=True,
        key="download_final_file",
        help="í•™ìˆ ì  ì •ì œ ê¸°ì¤€ ì ìš© í›„ SCIMATì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ WOS Plain Text íŒŒì¼"
    )

# --- í•˜ë‹¨ ì—¬ë°± ë° ì¶”ê°€ ì •ë³´ ---
st.markdown("<br>", unsafe_allow_html=True)

# ë„ì›€ë§ ì„¹ì…˜ - í•­ìƒ í‘œì‹œ
with st.expander("â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)", expanded=False):
    st.markdown("""
    **Q: ì—¬ëŸ¬ WOS íŒŒì¼ì„ ì–´ë–»ê²Œ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë‚˜ìš”?**
    A: WOSì—ì„œ ì—¬ëŸ¬ ë²ˆ Plain Text ë‹¤ìš´ë¡œë“œí•œ í›„, ëª¨ë“  .txt íŒŒì¼ì„ í•œ ë²ˆì— ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
    
    **Q: ì¤‘ë³µëœ ë…¼ë¬¸ì´ ìˆì„ê¹Œë´ ê±±ì •ë©ë‹ˆë‹¤.**
    A: UT(Unique Article Identifier) ê¸°ì¤€ìœ¼ë¡œ ìë™ ì¤‘ë³µ ì œê±°ë˜ë©°, UTê°€ ì—†ìœ¼ë©´ ì œëª©+ì €ì ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µì„ ê°ì§€í•©ë‹ˆë‹¤.
    
    **Q: WOSì—ì„œ ì–´ë–¤ ì„¤ì •ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•˜ë‚˜ìš”?**
    A: Export â†’ Record Content: "Full Record and Cited References", File Format: "Plain Text"ë¡œ ì„¤ì •í•˜ì„¸ìš”. ì¸ìš© ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ ì°¸ê³ ë¬¸í—Œ ì •ë³´ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
    
    **Q: ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ë…¼ë¬¸ì´ ë°°ì œë˜ë‚˜ìš”?**
    A: ì•„ë˜ì˜ ì²´ê³„ì ì¸ ê¸°ì¤€ì— ë”°ë¼ ì—°êµ¬ì˜ ê¹Šì´ì™€ ì‹ ë¢°ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
    - **EC1 (ì£¼ì œ ê´€ë ¨ì„± ë¶€ì¡±):** ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°ì„ ë¶€ì°¨ì  ë§¥ë½ìœ¼ë¡œë§Œ ì–¸ê¸‰í•œ ì—°êµ¬
    - **EC2 (ì‚¬íšŒ-ê¸°ìˆ ì  ë§¥ë½ ë¶€ì¬):** ì‚¬ìš©ìë‚˜ ì‚¬íšŒì  í•¨ì˜ ì—†ì´ ìˆœìˆ˜ ê¸°ìˆ ë§Œ ë‹¤ë£¬ ì—°êµ¬
    - **EC3 (ìƒí˜¸ì‘ìš©ì„± ë¶€ì¬):** ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš© ê°œë…ì´ ì—†ëŠ” ë‹¨ë°©í–¥ VOD ì—°êµ¬
    - **EC4 (í•™ìˆ ì  ì—„ë°€ì„± ë¶€ì¡±):** ë™ë£Œ ì‹¬ì‚¬ë¥¼ ê±°ì¹˜ì§€ ì•Šì€ ì‚¬ì„¤, ì„œí‰ ë“±
    - **EC5 (ì¤‘ë³µ):** ëª…ë°±íˆ ì¤‘ë³µ ê²Œì¬ëœ ì—°êµ¬
    
    **Q: SCIMATì—ì„œ í‚¤ì›Œë“œ ì •ë¦¬ë¥¼ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**
    A: Group set â†’ Word â†’ Find similar words by distances (Maximum distance: 1)ë¡œ ìœ ì‚¬ í‚¤ì›Œë“œë¥¼ ìë™ í†µí•©í•˜ê³ , Word Group manual setì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ê·¸ë£¹í™”í•˜ì„¸ìš”.
    
    **Q: SCIMAT ë¶„ì„ ì„¤ì •ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**
    A: Unit of Analysis: "Author's words + Source's words", Network Type: "Co-occurrence", Normalization: "Equivalence Index", Clustering: "Simple Centers Algorithm" (Maximum network size: 50)ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    
    **Q: ë³‘í•©ëœ íŒŒì¼ì´ SCIMATì—ì„œ ì œëŒ€ë¡œ ë¡œë”©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**
    A: ì›ë³¸ WOS íŒŒì¼ë“¤ì´ 'FN Clarivate Analytics Web of Science'ë¡œ ì‹œì‘í•˜ëŠ” ì •í’ˆ Plain Text íŒŒì¼ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
    
    **Q: SCIMATì—ì„œ PeriodëŠ” ì–´ë–»ê²Œ ì„¤ì •í•˜ë‚˜ìš”?**
    A: ì—°êµ¬ ë¶„ì•¼ì˜ ì§„í™” ë‹¨ê³„ë¥¼ ë°˜ì˜í•˜ì—¬ ì˜ë¯¸ ìˆê²Œ êµ¬ë¶„í•˜ë˜, ê° Periodë‹¹ ìµœì†Œ 50í¸ ì´ìƒì˜ ë…¼ë¬¸ì„ í¬í•¨í•˜ë„ë¡ ì„¤ì •í•˜ì„¸ìš”.
    
    **Q: ëª‡ ê°œì˜ íŒŒì¼ê¹Œì§€ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆë‚˜ìš”?**
    A: ê¸°ìˆ ì ìœ¼ë¡œëŠ” ì œí•œì´ ì—†ì§€ë§Œ, ì•ˆì •ì„±ì„ ìœ„í•´ 10ê°œ ì´í•˜ì˜ íŒŒì¼ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë§¤ìš° í° ë°ì´í„°ì…‹ì˜ ê²½ìš° ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬í•˜ì„¸ìš”.
    """)

# SciMAT ë¶„ì„ ê°€ì´ë“œ - í•­ìƒ í‘œì‹œ
with st.expander("ğŸ“Š WOS â†’ SciMAT ë¶„ì„ ì‹¤í–‰ ê°€ì´ë“œ", expanded=False):
    st.markdown("""
    ### í•„ìš”í•œ ê²ƒ
    - SciMAT ì†Œí”„íŠ¸ì›¨ì–´ (ë¬´ë£Œ ë‹¤ìš´ë¡œë“œ)
    - ë‹¤ìš´ë¡œë“œëœ WOS Plain Text íŒŒì¼
    - Java 1.8 ì´ìƒ
    
    ### 1ë‹¨ê³„: SciMAT ì‹œì‘í•˜ê¸°
    
    **ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±**
    ```
    1. SciMAT ì‹¤í–‰ (SciMAT.jar ë”ë¸”í´ë¦­)
    2. File â†’ New Project
    3. Path: ì €ì¥í•  í´ë” ì„ íƒ
    4. File name: í”„ë¡œì íŠ¸ ì´ë¦„ ì…ë ¥
    5. Accept
    ```
    
    **ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°**
    ```
    1. File â†’ Add Files
    2. "ISI WoS" ì„ íƒ
    3. ë‹¤ìš´ë¡œë“œí•œ txt íŒŒì¼ ì„ íƒ
    4. ë¡œë”© ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
    ```
    
    ### 2ë‹¨ê³„: í‚¤ì›Œë“œ ì •ë¦¬í•˜ê¸°
    
    **ìœ ì‚¬ í‚¤ì›Œë“œ ìë™ í†µí•©**
    ```
    1. Group set â†’ Word â†’ Find similar words by distances
    2. Maximum distance: 1 (í•œ ê¸€ì ì°¨ì´)
    3. ê°™ì€ ì˜ë¯¸ ë‹¨ì–´ë“¤ í™•ì¸í•˜ê³  Moveë¡œ í†µí•©
    ```
    ì˜ë¯¸: ì² ìê°€ 1ê¸€ìë§Œ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ì„œ ì œì•ˆ (ì˜ˆ: "platform" â†” "platforms")
    
    **ìˆ˜ë™ìœ¼ë¡œ í‚¤ì›Œë“œ ì •ë¦¬**
    ```
    1. Group set â†’ Word â†’ Word Group manual set
    2. Words without group ëª©ë¡ í™•ì¸
    3. ê´€ë ¨ í‚¤ì›Œë“œë“¤ ì„ íƒ í›„ New groupìœ¼ë¡œ ë¬¶ê¸°
    4. ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œ ì œê±°
    ```
    ëª©ì : ë°ì´í„° í’ˆì§ˆ í–¥ìƒ, ì˜ë¯¸ ìˆëŠ” í´ëŸ¬ìŠ¤í„° í˜•ì„±
    
    ### 3ë‹¨ê³„: ì‹œê°„ êµ¬ê°„ ì„¤ì •
    
    **Period ë§Œë“¤ê¸°**
    ```
    1. Knowledge base â†’ Periods â†’ Periods manager
    2. Add ë²„íŠ¼ìœ¼ë¡œ ì‹œê°„ êµ¬ê°„ ìƒì„±:
       - Period 1: 1996-2006 (íƒœë™ê¸°)
       - Period 2: 2007-2016 (í˜•ì„±ê¸°)
       - Period 3: 2017-2021 (í™•ì‚°ê¸°)
       - Period 4: 2022-2024 (ì„±ìˆ™ê¸°)
    ```
    ì›ë¦¬: ì—°êµ¬ ë¶„ì•¼ì˜ ì§„í™” ë‹¨ê³„ë¥¼ ë°˜ì˜í•œ ì˜ë¯¸ ìˆëŠ” êµ¬ë¶„
    
    **ê° Periodì— ë…¼ë¬¸ í• ë‹¹**
    ```
    1. Period 1 í´ë¦­ â†’ Add
    2. í•´ë‹¹ ì—°ë„ ë…¼ë¬¸ë“¤ ì„ íƒ
    3. ì˜¤ë¥¸ìª½ í™”ì‚´í‘œë¡œ ì´ë™
    4. ë‹¤ë¥¸ Periodë“¤ë„ ë™ì¼í•˜ê²Œ ë°˜ë³µ
    ```
    
    ### 4ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰
    
    **ë¶„ì„ ë§ˆë²•ì‚¬ ì‹œì‘**
    ```
    1. Analysis â†’ Make Analysis
    2. ëª¨ë“  Period ì„ íƒ â†’ Next
    ```
    
    **Step 1-8: ë¶„ì„ ì„¤ì •**
    - Unit of Analysis: "Author's words + Source's words"
    - Data Reduction: Minimum frequency 2
    - Network Type: "Co-occurrence"
    - Normalization: "Equivalence Index"
    - Clustering: "Simple Centers Algorithm" (Max network size: 50)
    - Document Mapper: "Core Mapper"
    - Performance Measures: G-index, Sum Citations
    - Evolution Map: "Jaccard Index"
    
    **ë¶„ì„ ì‹¤í–‰**
    ```
    - Finish í´ë¦­
    - ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (10-30ë¶„)
    ```
    
    ### 5ë‹¨ê³„: ê²°ê³¼ í•´ì„
    
    **ì „ëµì  ë‹¤ì´ì–´ê·¸ë¨ 4ì‚¬ë¶„ë©´**
    - ìš°ìƒë‹¨: Motor Themes (í•µì‹¬ ì£¼ì œ)
    - ì¢Œìƒë‹¨: Specialized Themes (ì „ë¬¸í™”ëœ ì£¼ì œ)
    - ì¢Œí•˜ë‹¨: Emerging/Declining Themes (ì‹ í¥/ì‡ í‡´ ì£¼ì œ)
    - ìš°í•˜ë‹¨: Basic Themes (ê¸°ì´ˆ ì£¼ì œ)
    
    **ì§„í™” ë§µ ë¶„ì„**
    - ë…¸ë“œ í¬ê¸° = ë…¼ë¬¸ ìˆ˜
    - ì—°ê²°ì„  ë‘ê»˜ = Jaccard ìœ ì‚¬ë„
    - ì‹œê°„ì— ë”°ë¥¸ ì£¼ì œ ë³€í™” ì¶”ì 
    
    ### ë¬¸ì œ í•´ê²°
    - í‚¤ì›Œë“œ ì •ë¦¬ë¥¼ ê¼¼ê¼¼íˆ (ë¶„ì„í’ˆì§ˆì˜ í•µì‹¬)
    - Periodë³„ ìµœì†Œ 50í¸ ì´ìƒ ê¶Œì¥
    - Java ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ì¬ì‹œì‘
    - ì¸ì½”ë”© ë¬¸ì œì‹œ UTF-8ë¡œ ë³€ê²½
    """)

st.markdown("<br><br>", unsafe_allow_html=True)
