import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import io
import altair as alt

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="WOS Analysis Dashboard", layout="centered")

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- 1. [í•µì‹¬ ê°œì„ ] ê°œë… ì •ê·œí™” ì‚¬ì „ (Thesaurus) ---
# Cobo(2012)ì˜ í•µì‹¬ ì›ì¹™ì¸ 'ê°œë… ë‹¨ìœ„' í†µí•©ì„ ìœ„í•œ ì‚¬ìš©ì ì‚¬ì „
@st.cache_data
def get_normalization_map():
    thesaurus = {
        "live commerce": ["live shopping", "social commerce", "livestream shopping", "e-commerce live streaming"],
        "user engagement": ["consumer engagement", "viewer engagement", "audience engagement"],
        "purchase intention": ["purchase intentions", "buying intention"],
        "user experience": ["consumer experience", "viewer experience"],
        "social presence": ["perceived social presence"],
        "influencer marketing": ["influencer", "digital celebrities", "wanghong"],
        "platform technology": ["streaming technology", "platform architecture", "streaming media"],
        "peer-to-peer": ["p2p", "peer to peer"]
    }
    # ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•œ ì—­ë°©í–¥ ë§µ ìƒì„±
    normalization_map = {}
    for standard_term, variations in thesaurus.items():
        for variation in variations:
            normalization_map[variation] = standard_term
    return normalization_map

NORMALIZATION_MAP = get_normalization_map()

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_data
def load_data(uploaded_file):
    # ... (ì´ì „ê³¼ ë™ì¼í•œ ì¸ì½”ë”© ì²˜ë¦¬ ë¡œì§) ...
    encodings_to_try = ['utf-8', 'latin1', 'cp949', 'utf-8-sig']
    df = None
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df_try = pd.read_csv(uploaded_file, sep='\t', encoding=encoding)
            if df_try.shape[1] > 2: return df_try
        except Exception: continue
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df_try = pd.read_csv(uploaded_file, encoding=encoding)
            if df_try.shape[1] > 2: return df_try
        except Exception: continue
    return None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def classify_article(row):
    # ... (ê¸°ì¡´ê³¼ ë™ì¼í•œ ë¶„ë¥˜ ë¡œì§) ...
    inclusion_keywords = ['user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'behavior', 'experience', 'engagement', 'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 'society', 'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization', 'education', 'learning', 'influencer']
    exclusion_keywords = ['protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer', 'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput', 'sensor data', 'geoscience', 'environmental data', 'wlan', 'ofdm', 'error correction', 'tcp', 'udp', 'network traffic']
    title = str(row.get('Article Title', row.get('TI', ''))).lower()
    abstract = str(row.get('Abstract', row.get('AB', ''))).lower()
    author_keywords = str(row.get('Author Keywords', row.get('DE', ''))).lower()
    keywords_plus = str(row.get('Keywords Plus', row.get('ID', ''))).lower()
    full_text = ' '.join([title, abstract, author_keywords, keywords_plus])
    if any(keyword in full_text for keyword in exclusion_keywords): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in full_text for keyword in inclusion_keywords): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def preprocess_keywords(row, stop_words, lemmatizer, normalization_map):
    if row['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)':
        author_keywords = str(row.get('Author Keywords', row.get('DE', ''))).lower()
        keywords_plus = str(row.get('Keywords Plus', row.get('ID', ''))).lower()
        all_keywords_str = author_keywords + ';' + keywords_plus
        all_keywords = all_keywords_str.split(';')
        
        cleaned_keywords = set()
        for keyword in all_keywords:
            keyword = keyword.strip()
            if not keyword: continue

            # 1ë‹¨ê³„: ê°œë… ì •ê·œí™” (Normalization)
            normalized_keyword = normalization_map.get(keyword, keyword)
            
            # 2ë‹¨ê³„: ë¬¸ì ì •ì œ ë° í‘œì œì–´ ì¶”ì¶œ (Cleaning & Lemmatization)
            normalized_keyword = normalized_keyword.replace('-', ' ')
            normalized_keyword = re.sub(r'[^a-z\s]', '', normalized_keyword)
            
            final_words = [lemmatizer.lemmatize(w) for w in normalized_keyword.split() if w not in stop_words and len(w) > 2]
            if final_words:
                cleaned_keywords.add(" ".join(final_words))
        
        return '; '.join(sorted(list(cleaned_keywords)))
    return None

# --- ì¹´ë“œ ìŠ¤íƒ€ì¼ CSS ---
st.markdown("""
<style>
    div[data-testid="stVerticalBlock"] > [data-testid="stHorizontalBlock"] > div[data-testid="stVerticalBlock"] {
        border: 1px solid #e6e6e6; border-radius: 10px; padding: 25px; 
        box-shadow: 0 4px 8px rgba(0,0,0,0.05); background-color: #ffffff;
    }
    .recommendation-box {
        border: 1px solid #cce5ff; border-radius: 10px; padding: 20px;
        background-color: #f8f9fa; margin-top: 20px;
    }
</style>
""", unsafe_allow_html=True)

# --- Streamlit UI ë° ì‹¤í–‰ ë¡œì§ ---
st.title("WOS ë°ì´í„° ë¶„ì„ ë° ì •ì œ ë„êµ¬")
st.caption("WOS Data Classifier & Preprocessor")

uploaded_file = st.file_uploader(
    "WoS Raw Data íŒŒì¼(CSV/TXT)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", 
    type=['csv', 'txt']
)

if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is None:
        st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    if st.button("ë¶„ì„ ì‹œì‘ / Start Analysis"):
        with st.spinner("ë¶„ì„ ì¤‘... / Analyzing..."):
            
            df['Classification'] = df.apply(classify_article, axis=1)
            
            stop_words = set(stopwords.words('english'))
            custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article'}
            stop_words.update(custom_stop_words)
            lemmatizer = WordNetLemmatizer()
            df['Cleaned_Keywords'] = df.apply(lambda row: preprocess_keywords(row, stop_words, lemmatizer, NORMALIZATION_MAP), axis=1)

            st.success("âœ… ë¶„ì„ ì™„ë£Œ! / Analysis Complete!")
            
            # --- ê²°ê³¼ í‘œì‹œ ---
            st.markdown("---")
            with st.container():
                st.subheader("1. 1ì°¨ ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½")
                st.caption("Initial Classification Summary")
                classification_summary = df['Classification'].value_counts()
                st.metric("â–¶ï¸ ê´€ë ¨ì—°êµ¬ (Include)", classification_summary.get('Include (ê´€ë ¨ì—°êµ¬)', 0))
                st.metric("â–¶ï¸ ì œì™¸ì—°êµ¬ (Exclude)", classification_summary.get('Exclude (ì œì™¸ì—°êµ¬)', 0))
                st.metric("â–¶ï¸ ê²€í† í•„ìš” (Review)", classification_summary.get('Review (ê²€í† í•„ìš”)', 0))

            st.markdown("---")
            with st.container():
                st.subheader("2. ì •ê·œí™”ëœ í•µì‹¬ í‚¤ì›Œë“œ ë¶„ì„")
                st.caption("Normalized Core Keywords Analysis (Top 20)")
                
                all_cleaned_keywords = []
                df[df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)']['Cleaned_Keywords'].dropna().apply(lambda x: all_cleaned_keywords.extend(x.split(';') if isinstance(x, str) else []))
                all_cleaned_keywords = [kw.strip() for kw in all_cleaned_keywords if kw.strip()]
                keyword_counts = Counter(all_cleaned_keywords)
                
                if keyword_counts:
                    top_keywords_df = pd.DataFrame(keyword_counts.most_common(20), columns=['Keyword', 'Frequency'])
                    chart = alt.Chart(top_keywords_df).mark_bar(cornerRadius=3, height=20).encode(
                        x=alt.X('Frequency:Q', title='ë¹ˆë„ (Frequency)'),
                        y=alt.Y('Keyword:N', title='í‚¤ì›Œë“œ (Keyword)', sort='-x'),
                        color=alt.value('#4F8BFF'), 
                        tooltip=['Keyword', 'Frequency']
                    ).properties(title='í•µì‹¬ ê°œë… ìƒìœ„ 20ê°œ (Top 20 Core Concepts)')
                    st.altair_chart(chart, use_container_width=True)
                else:
                    st.write("'ê´€ë ¨ì—°êµ¬'ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # --- 3. [í•µì‹¬ ê°œì„ ] ë°©ë²•ë¡ ì  ì œì–¸ ---
            st.markdown("---")
            with st.container():
                st.subheader("3. ë‹¤ìŒ ë¶„ì„ ë‹¨ê³„ë¥¼ ìœ„í•œ ì œì–¸")
                st.caption("Recommendations for Next Analysis Steps (based on Cobo et al., 2012)")
                st.markdown("""
                <div class="recommendation-box">
                <h4>í•™ìˆ ì  ì™„ì„±ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ 2ê°€ì§€ í•µì‹¬ ì „ëµ:</h4>
                <ol>
                    <li>
                        <strong>'ê´€ê³„ì˜ ê°•ë„' ì¸¡ì • (Measuring Relational Strength):</strong>
                        <p>VOSviewer ë˜ëŠ” SciMAT ë¶„ì„ ì‹œ, ë‹¨ìˆœ ë™ì‹œì¶œí˜„ ë¹ˆë„(Co-occurrence) ëŒ€ì‹  <strong>'Association Strength'</strong> ë˜ëŠ” <strong>'Equivalence Index'</strong>ë¥¼ ì •ê·œí™”(Normalization) ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. ì´ëŠ” ë‘ ê°œë…ì´ ì–¼ë§ˆë‚˜ ë…ì ì ìœ¼ë¡œ ê°•í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ë³´ì—¬ì£¼ì–´, ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ë¥¼ í¬ê²Œ ë†’ì…ë‹ˆë‹¤.</p>
                    </li>
                    <li>
                        <strong>'í´ëŸ¬ìŠ¤í„° ë‹¨ìœ„' ì—°êµ¬ ì œì™¸ (Cluster-based Exclusion):</strong>
                        <p>í˜„ì¬ì˜ 'ì œì™¸ì—°êµ¬' ë¶„ë¥˜ëŠ” íš¨ìœ¨ì ì¸ 1ì°¨ í•„í„°ë§ì…ë‹ˆë‹¤. ë” ë†’ì€ ê°ê´€ì„±ì„ ìœ„í•´, <strong>'ê´€ë ¨ì—°êµ¬'ì™€ 'ê²€í† í•„ìš”' ê·¸ë£¹ ì „ì²´ë¥¼ ëŒ€ìƒ</strong>ìœ¼ë¡œ VOSviewerì—ì„œ ì˜ˆë¹„ í´ëŸ¬ìŠ¤í„°ë§ì„ ë¨¼ì € ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤. ê·¸ ê²°ê³¼ ëª…í™•í•˜ê²Œ ë¶„ë¦¬ë˜ëŠ” 'ìˆœìˆ˜ ë„¤íŠ¸ì›Œí¬ ê¸°ìˆ ' í´ëŸ¬ìŠ¤í„°ê°€ ìˆë‹¤ë©´, "ë°ì´í„°ê°€ ë³´ì—¬ì£¼ëŠ” êµ¬ì¡°ì— ê·¼ê±°í•˜ì—¬ í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ì „ì²´ë¥¼ ìµœì¢… ë¶„ì„ì—ì„œ ì œì™¸í•œë‹¤"ê³  ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì—°êµ¬ì˜ ê²½ê³„ë¥¼ ì„¤ì •í•˜ëŠ” ê°€ì¥ ê°ê´€ì ì´ê³  ë°©ì–´í•˜ê¸° ìš©ì´í•œ ë°©ë²•ì…ë‹ˆë‹¤.</p>
                    </li>
                </ol>
                </div>
                """, unsafe_allow_html=True)

            # --- ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ---
            st.markdown("---")
            @st.cache_data
            def convert_df_to_csv(df_to_convert):
                return df_to_convert.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')

            csv_data = convert_df_to_csv(df)
            st.download_button(
               label="ğŸ“¥ ì²˜ë¦¬ëœ ì „ì²´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (CSV)",
               data=csv_data,
               file_name="wos_processed_data.csv",
               mime="text/csv",
            )
