import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt
import io

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Prep | Professional Edition",
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ìºì‹œ ìœ ì§€) ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- í‚¤ì›Œë“œ ì •ê·œí™” ì‚¬ì „ (ì—­ë°©í–¥ ë§¤í•‘ìœ¼ë¡œ ìµœì í™”) ---
def build_normalization_map():
    """ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì—­ë°©í–¥ ì •ê·œí™” ì‚¬ì „ ìƒì„±"""
    base_map = {
        # AI/ML ê´€ë ¨ (ì„¸ë¶„í™” ìœ ì§€)
        "machine learning": ["machine-learning", "machine_learning", "ml", "machinelearning"],
        "artificial intelligence": ["ai", "artificial-intelligence", "artificial_intelligence", "artificialintelligence"],
        "deep learning": ["deep-learning", "deep_learning", "deep neural networks", "deep neural network", "dnn", "deeplearning"],
        "neural networks": ["neural-networks", "neural_networks", "neuralnetworks", "neural network", "nn"],
        "natural language processing": ["nlp", "natural-language-processing", "natural_language_processing"],
        "computer vision": ["computer-vision", "computer_vision", "computervision", "cv"],
        "reinforcement learning": ["reinforcement-learning", "reinforcement_learning", "rl"],

        # ìŠ¤íŠ¸ë¦¬ë°/ë¯¸ë””ì–´ ê´€ë ¨
        "live streaming": ["live-streaming", "live_streaming", "livestreaming", "real time streaming"],
        "video streaming": ["video-streaming", "video_streaming", "videostreaming"],
        "social media": ["social-media", "social_media", "socialmedia"],
        "user experience": ["user-experience", "user_experience", "ux", "userexperience"],
        "user behavior": ["user-behavior", "user_behavior", "userbehavior"],
        "content creation": ["content-creation", "content_creation", "contentcreation"],
        "digital marketing": ["digital-marketing", "digital_marketing", "digitalmarketing"],
        "e commerce": ["ecommerce", "e-commerce", "e_commerce", "electronic commerce"],

        # ì—°êµ¬ë°©ë²•ë¡  ê´€ë ¨
        "data mining": ["data-mining", "data_mining", "datamining"],
        "big data": ["big-data", "big_data", "bigdata"],
        "data analysis": ["data-analysis", "data_analysis", "dataanalysis"],
        "sentiment analysis": ["sentiment-analysis", "sentiment_analysis", "sentimentanalysis"],
        "statistical analysis": ["statistical-analysis", "statistical_analysis", "statisticalanalysis"],
        "structural equation modeling": ["sem", "pls-sem", "pls sem", "structural equation model"],

        # ê¸°ìˆ  ê´€ë ¨
        "cloud computing": ["cloud-computing", "cloud_computing", "cloudcomputing"],
        "internet of things": ["iot", "internet-of-things", "internet_of_things"],
        "mobile applications": ["mobile-applications", "mobile_applications", "mobile apps", "mobile app"],
        "web development": ["web-development", "web_development", "webdevelopment"],
        "software engineering": ["software-engineering", "software_engineering", "softwareengineering"]
    }

    # ì—­ë°©í–¥ ë§¤í•‘ ìƒì„± (variation -> standard_form)
    reverse_map = {}
    for standard_form, variations in base_map.items():
        for variation in variations:
            reverse_map[variation.lower()] = standard_form
        # í‘œì¤€ í˜•íƒœë„ ìê¸° ìì‹ ìœ¼ë¡œ ë§¤í•‘
        reverse_map[standard_form.lower()] = standard_form

    return reverse_map

NORMALIZATION_MAP = build_normalization_map()

def normalize_keyword_phrase(phrase):
    """êµ¬ë¬¸ ë‹¨ìœ„ í‚¤ì›Œë“œ ì •ê·œí™”"""
    phrase_lower = phrase.lower().strip()
    return NORMALIZATION_MAP.get(phrase_lower, phrase_lower)

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
def load_data(uploaded_file):
    file_bytes = uploaded_file.getvalue()
    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'cp949']

    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # íƒ­ êµ¬ë¶„ì ìš°ì„  ì‹œë„
            df = pd.read_csv(io.StringIO(file_content), sep='\t', lineterminator='\n')
            if df.shape[1] > 1: return df
        except Exception:
            continue

    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # ì½¤ë§ˆ êµ¬ë¶„ì ì‹œë„
            df = pd.read_csv(io.StringIO(file_content))
            if df.shape[1] > 1: return df
        except Exception:
            continue

    return None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def classify_article(row):
    inclusion_keywords = ['user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'behavior', 'experience', 'engagement', 'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 'society', 'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization', 'education', 'learning', 'influencer']
    exclusion_keywords = ['protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer', 'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput', 'sensor data', 'geoscience', 'environmental data', 'wlan', 'ofdm', 'error correction', 'tcp', 'udp', 'network traffic']
    title = str(row.get('TI', '')).lower()
    source_title = str(row.get('SO', '')).lower()
    author_keywords = str(row.get('DE', '')).lower()
    keywords_plus = str(row.get('ID', '')).lower()
    abstract = str(row.get('AB', '')).lower()
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    if any(keyword in full_text for keyword in exclusion_keywords): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in full_text for keyword in inclusion_keywords): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì •ê·œí™” ë° ì •ì œ ì²˜ë¦¬"""
    if pd.isna(keywords_str) or not isinstance(keywords_str, str):
        return ""

    all_keywords = keywords_str.split(';')
    cleaned_keywords = set()

    for keyword in all_keywords:
        if not keyword.strip():
            continue

        # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ
        keyword_clean = keyword.strip().lower()
        keyword_clean = re.sub(r'[^a-z\s\-_]', '', keyword_clean)

        # 2ë‹¨ê³„: êµ¬ë¬¸ ë‹¨ìœ„ ì •ê·œí™”
        normalized_phrase = normalize_keyword_phrase(keyword_clean)

        # 3ë‹¨ê³„: ë‹¨ì–´ë³„ ì²˜ë¦¬
        if normalized_phrase == keyword_clean.lower():
            keyword_clean = keyword_clean.replace('-', ' ').replace('_', ' ')
            words = keyword_clean.split()

            filtered_words = []
            for word in words:
                if word and len(word) > 2 and word not in stop_words:
                    lemmatized_word = lemmatizer.lemmatize(word)
                    filtered_words.append(lemmatized_word)

            if filtered_words:
                reconstructed_phrase = " ".join(filtered_words)
                final_keyword = normalize_keyword_phrase(reconstructed_phrase)
                if final_keyword and len(final_keyword) > 2:
                    cleaned_keywords.add(final_keyword)
        else:
            if normalized_phrase and len(normalized_phrase) > 2:
                cleaned_keywords.add(normalized_phrase)

    return '; '.join(sorted(list(cleaned_keywords)))

# --- SCIMAT í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ---
def convert_df_to_scimat_format(df_to_convert):
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']

    for _, row in df_to_convert.iterrows():
        if len(file_content) > 2:
            file_content.append("")
        sorted_tags = [tag for tag in wos_field_order if tag in row.index and pd.notna(row[tag])]

        for tag in sorted_tags:
            value = row[tag]
            if pd.isna(value) or not str(value).strip():
                continue
            if not isinstance(value, str):
                value = str(value)

            if tag in multi_line_fields:
                items = [item.strip() for item in value.split(';') if item.strip()]
                if items:
                    file_content.append(f"{tag} {items[0]}")
                    for item in items[1:]:
                        file_content.append(f"  {item}")
            else:
                file_content.append(f"{tag} {value}")

        file_content.append("ER")
    return "\n".join(file_content).encode('utf-8')

# --- í—¤ë” ë° ë²„ì „ ì •ë³´ ---
st.markdown("""
<div style="text-align: center; padding-top: 1rem;">
    <h1 style="color: #212529; font-size: 2.8rem; font-weight: 700; margin-bottom: 0.5rem; letter-spacing: -0.05em;">
        WOS Prep
    </h1>
    <p style="color: #495057; font-size: 1.2rem; margin: 0; font-weight: 400;">
        A Professional Tool for Web of Science Data Pre-processing
    </p>
</div>
""", unsafe_allow_html=True)

st.markdown("""
<div style="text-align: center; color: #6c757d; font-size: 0.85rem; padding: 1rem 0 2rem 0;">
    <p style="margin-bottom: 0.25rem;">
        <strong>Developed by:</strong> ì„íƒœê²½ (Teddy Lym), Hanyang University (Graduate School of Technology & Innovation Management)
    </p>
    <p style="margin: 0; font-size: 0.8rem;">
        <strong>Version:</strong> 1.5.0 | A Tool for Academic Research
    </p>
</div>
""", unsafe_allow_html=True)

st.markdown('<div style="width: 80px; height: 4px; background-color: #0d6efd; margin: 0 auto 3rem auto; border-radius: 2px;"></div>', unsafe_allow_html=True)


# --- ì£¼ìš” ê¸°ëŠ¥ ì†Œê°œ ---
col1, col2, col3 = st.columns(3)
with col1:
    st.markdown("""
    <div style="text-align: center; padding: 1.5rem; background: #f8f9fa; border-radius: 12px; border: 1px solid #e8eaed; height: 100%;">
        <div style="color: #1a73e8; font-size: 2.5rem; margin-bottom: 0.8rem;">ğŸ“Š</div>
        <h4 style="color: #3c4043; margin-bottom: 0.5rem; font-weight: 500;">ë°ì´í„° ë¶„ë¥˜</h4>
        <p style="color: #5f6368; font-size: 0.9rem; margin: 0;">í•µì‹¬ ë…¼ë¬¸ ìë™ ë¶„ë¥˜</p>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown("""
    <div style="text-align: center; padding: 1.5rem; background: #f8f9fa; border-radius: 12px; border: 1px solid #e8eaed; height: 100%;">
        <div style="color: #34a853; font-size: 2.5rem; margin-bottom: 0.8rem;">âš™ï¸</div>
        <h4 style="color: #3c4043; margin-bottom: 0.5rem; font-weight: 500;">í‚¤ì›Œë“œ ì •ê·œí™”</h4>
        <p style="color: #5f6368; font-size: 0.9rem; margin: 0;">AI ê¸°ë°˜ í‚¤ì›Œë“œ í‘œì¤€í™”</p>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown("""
    <div style="text-align: center; padding: 1.5rem; background: #f8f9fa; border-radius: 12px; border: 1px solid #e8eaed; height: 100%;">
        <div style="color: #ea4335; font-size: 2.5rem; margin-bottom: 0.8rem;">ğŸ¯</div>
        <h4 style="color: #3c4043; margin-bottom: 0.5rem; font-weight: 500;">SciMAT í˜¸í™˜</h4>
        <p style="color: #5f6368; font-size: 0.9rem; margin: 0;">ì™„ë²½í•œ ë¶„ì„ ë„êµ¬ ì—°ë™</p>
    </div>
    """, unsafe_allow_html=True)

st.markdown("<br>", unsafe_allow_html=True)

# --- í‚¤ì›Œë“œ ì •ê·œí™” ê¸°ì¤€ ì„¤ëª… ---
with st.expander("â“˜ í‚¤ì›Œë“œ ì •ê·œí™” ê¸°ì¤€ ìƒì„¸"):
    st.markdown("""
    **ì ìš©ë˜ëŠ” ì •ê·œí™” ê·œì¹™:**
    - **AI/ML ê´€ë ¨**: machine learning â† machine-learning, ML, machinelearning
    - **ì¸ê³µì§€ëŠ¥**: artificial intelligence â† AI, artificial-intelligence
    - **ë”¥ëŸ¬ë‹**: deep learning â† deep-learning, deep neural networks, DNN
    - **ìŠ¤íŠ¸ë¦¬ë°**: live streaming â† live-streaming, livestreaming
    - **ì‚¬ìš©ì ê²½í—˜**: user experience â† user-experience, UX
    - **ì—°êµ¬ë°©ë²•ë¡ **: structural equation modeling â† SEM, PLS-SEM
    - **ì „ììƒê±°ë˜**: e commerce â† ecommerce, e-commerce, electronic commerce
    """)

# --- íŒŒì¼ ì—…ë¡œë“œ ---
st.divider()
st.subheader("ğŸ“ ë°ì´í„° ì—…ë¡œë“œ")
st.markdown("<p style='color: #495057; font-size: 1rem; margin-top: -10px;'>Web of Science Raw Data íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.</p>", unsafe_allow_html=True)

uploaded_file = st.file_uploader(
    "Tab-delimited ë˜ëŠ” Plain Text í˜•ì‹ì˜ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.",
    type=['csv', 'txt'],
    label_visibility="collapsed"
)

if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is None:
        st.error("âŒ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ 'Tab-delimited' ë˜ëŠ” 'Plain Text' í˜•ì‹ì˜ íŒŒì¼ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()

    column_mapping = {
        'Authors': 'AU', 'Article Title': 'TI', 'Source Title': 'SO', 'Author Keywords': 'DE',
        'Keywords Plus': 'ID', 'Abstract': 'AB', 'Cited References': 'CR', 'Publication Year': 'PY',
        'Times Cited, All Databases': 'TC', 'Times Cited, WoS Core': 'Z9'
    }
    for old_name, new_name in column_mapping.items():
        if old_name in df.columns:
            df.rename(columns={old_name: new_name}, inplace=True)

    with st.spinner("ğŸ”„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        # 1ë‹¨ê³„: ë¶„ë¥˜
        df['Classification'] = df.apply(classify_article, axis=1)

        # ì›ë³¸ í‚¤ì›Œë“œ ë°±ì—…
        if 'DE' in df.columns: df['DE_Original'] = df['DE'].copy()
        if 'ID' in df.columns: df['ID_Original'] = df['ID'].copy()

        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ì •ê·œí™”
        stop_words = set(stopwords.words('english'))
        custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article', 'using', 'based', 'approach', 'method', 'system', 'model'}
        stop_words.update(custom_stop_words)
        lemmatizer = WordNetLemmatizer()
        include_mask = df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'

        if 'DE' in df.columns:
            df['DE_cleaned'] = df['DE'].copy()
            df.loc[include_mask, 'DE_cleaned'] = df.loc[include_mask, 'DE'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))
        if 'ID' in df.columns:
            df['ID_cleaned'] = df['ID'].copy()
            df.loc[include_mask, 'ID_cleaned'] = df.loc[include_mask, 'ID'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))

        st.success("âœ… ë¶„ì„ ì™„ë£Œ!")

        # --- ë¶„ì„ ê²°ê³¼ ìš”ì•½ ---
        st.divider()
        st.subheader("ğŸ“ˆ ë¶„ì„ ê²°ê³¼ ìš”ì•½")

        # ì „ì²˜ë¦¬ ê²°ê³¼ ê³„ì‚°
        unique_before, unique_after = 0, 0
        if include_mask.any():
            de_original = df.loc[include_mask, 'DE_Original'].dropna().str.split(';').explode().str.strip().str.lower()
            id_original = df.loc[include_mask, 'ID_Original'].dropna().str.split(';').explode().str.strip().str.lower()
            unique_before = pd.concat([de_original, id_original]).nunique()

            de_cleaned = df.loc[include_mask, 'DE_cleaned'].dropna().str.split(';').explode().str.strip().str.lower()
            id_cleaned = df.loc[include_mask, 'ID_cleaned'].dropna().str.split(';').explode().str.strip().str.lower()
            unique_after = pd.concat([de_cleaned, id_cleaned]).nunique()
        
        # ì²˜ë¦¬ íš¨ìœ¨ ìš”ì•½
        st.markdown("###### **ì²˜ë¦¬ íš¨ìœ¨ ìš”ì•½**")
        col1, col2, col3 = st.columns(3)
        df_final = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
        col1.metric("ìµœì¢… ë¶„ì„ ë…¼ë¬¸", f"{len(df_final)} ê±´")
        col2.metric("ì •ê·œí™” ì ìš© ë…¼ë¬¸", f"{include_mask.sum()} ê±´")
        col3.metric("í‚¤ì›Œë“œ ì •ê·œí™”", f"{unique_before} â†’ {unique_after}", f"-{unique_before - unique_after} ê°œ", help="ìœ ì‚¬/ë™ì˜ì–´ í‚¤ì›Œë“œë¥¼ í‘œì¤€í™”í•˜ì—¬ ê³ ìœ  í‚¤ì›Œë“œ ê°œìˆ˜ë¥¼ ì¤„ì˜€ìŠµë‹ˆë‹¤.")
        
        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("###### **ë…¼ë¬¸ ë¶„ë¥˜ ë° í‚¤ì›Œë“œ í˜„í™©**")

        col1, col2 = st.columns([0.4, 0.6])
        with col1:
            # ë„ë„› ì°¨íŠ¸
            classification_counts = df['Classification'].value_counts().reset_index()
            classification_counts.columns = ['ë¶„ë¥˜', 'ë…¼ë¬¸ ìˆ˜']
            total_papers = classification_counts['ë…¼ë¬¸ ìˆ˜'].sum()
            domain = ['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)', 'Exclude (ì œì™¸ì—°êµ¬)']
            range_ = ['#d62728', '#1f77b4', '#7f7f7f']  # Red, Blue, Grey

            base = alt.Chart(classification_counts).encode(
                theta=alt.Theta(field="ë…¼ë¬¸ ìˆ˜", type="quantitative", stack=True),
                color=alt.Color(field="ë¶„ë¥˜", type="nominal", title="ë¶„ë¥˜",
                               scale=alt.Scale(domain=domain, range=range_),
                               legend=alt.Legend(orient="bottom", titleColor="#212529", labelColor="#495057"))
            )
            pie = base.mark_arc(outerRadius=120, innerRadius=70)
            text_total = alt.Chart(pd.DataFrame([{'value': f'{total_papers}'}])).mark_text(
                align='center', baseline='middle', fontSize=35, fontWeight='bold', color='#212529'
            ).encode(text='value:N')
            text_label = alt.Chart(pd.DataFrame([{'value': 'Total Papers'}])).mark_text(
                align='center', baseline='middle', fontSize=14, dy=-25, color='#495057'
            ).encode(text='value:N')

            chart = (pie + text_total + text_label).properties(
                title=alt.TitleParams(text='ë…¼ë¬¸ ë¶„ë¥˜ ë¶„í¬', anchor='middle', fontSize=16, fontWeight=500, color="#212529"),
            ).configure_view(strokeWidth=0)
            st.altair_chart(chart, use_container_width=True)

        with col2:
            # ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (ë¡¤ë¦¬íŒ ì°¨íŠ¸)
            all_keywords = []
            if 'DE_cleaned' in df.columns:
                all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'DE_cleaned'].dropna() for kw in text.split(';') if kw.strip()])
            if 'ID_cleaned' in df.columns:
                all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'ID_cleaned'].dropna() for kw in text.split(';') if kw.strip()])

            if all_keywords:
                keyword_counts = Counter(all_keywords)
                top_n = 20
                top_keywords_df = pd.DataFrame(keyword_counts.most_common(top_n), columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                top_3_keywords = top_keywords_df['í‚¤ì›Œë“œ'].head(3).tolist()
                
                y_encoding = alt.Y('í‚¤ì›Œë“œ:N', title=None, sort=alt.SortField(field='ë¹ˆë„', order='descending'))
                x_encoding = alt.X('ë¹ˆë„:Q', title='ë¹ˆë„', scale=alt.Scale(zero=True))
                
                line = alt.Chart(top_keywords_df).mark_rule(size=2).encode(y=y_encoding, x=x_encoding)
                point = alt.Chart(top_keywords_df).mark_point(filled=True, size=100).encode(
                    y=y_encoding, x=x_encoding, tooltip=['í‚¤ì›Œë“œ', 'ë¹ˆë„'],
                    color=alt.condition(alt.FieldOneOfPredicate(field='í‚¤ì›Œë“œ', oneOf=top_3_keywords), alt.value('#d62728'), alt.value('#4c78a8'))
                )
                lollipop_chart = (line + point).properties(
                    title=alt.TitleParams(text=f'ìƒìœ„ {top_n} í‚¤ì›Œë“œ', anchor='middle', fontSize=16, fontWeight=500, color="#212529"),
                    height=450
                ).configure_axis(grid=False).configure_view(strokeWidth=0)
                st.altair_chart(lollipop_chart, use_container_width=True)
            else:
                st.warning("âš ï¸ ê´€ë ¨ì—°êµ¬ë¡œ ë¶„ë¥˜ëœ ë…¼ë¬¸ì—ì„œ ìœ íš¨í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        st.divider()
        
        # --- ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ ê·¸ë˜í”„ ---
        st.markdown("###### **ì—°ë„ë³„ ì—°êµ¬ ë™í–¥**")
        
        # ì—°ë„ë³„ ë°ì´í„° ì²˜ë¦¬
        df_trend = df.copy()
        df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
        df_trend.dropna(subset=['PY'], inplace=True)
        df_trend['PY'] = df_trend['PY'].astype(int)
        
        yearly_counts = df_trend['PY'].value_counts().reset_index()
        yearly_counts.columns = ['Year', 'Count']
        yearly_counts = yearly_counts[yearly_counts['Year'] <= 2025].sort_values('Year')

        # 2025ë…„ ì˜ˆìƒì¹˜ ê³„ì‚°
        projection_layer = alt.Chart(pd.DataFrame([])).mark_line()
        show_projection_caption = False
        if 2025 in yearly_counts['Year'].values and 2024 in yearly_counts['Year'].values:
            count_2025_actual = yearly_counts.loc[yearly_counts['Year'] == 2025, 'Count'].iloc[0]
            count_2024_actual = yearly_counts.loc[yearly_counts['Year'] == 2024, 'Count'].iloc[0]
            count_2025_projected = count_2025_actual * 2
            
            projection_df = pd.DataFrame([
                {'Year': 2024, 'Count': count_2024_actual},
                {'Year': 2025, 'Count': count_2025_projected}
            ])
            
            projection_layer = alt.Chart(projection_df).mark_line(
                strokeDash=[5, 5], color='red', point={'color': 'red', 'filled': False, 'size': 60}
            ).encode(x='Year:O', y='Count:Q')
            show_projection_caption = True
            
        # ê¸°ë³¸ ë¼ì¸ ì°¨íŠ¸
        line_chart = alt.Chart(yearly_counts).mark_line(point=True).encode(
            x=alt.X('Year:O', title='ë°œí–‰ ì—°ë„'),
            y=alt.Y('Count:Q', title='ë…¼ë¬¸ ìˆ˜'),
            tooltip=['Year', 'Count']
        ).properties(height=300)
        
        # ì°¨íŠ¸ ê²°í•© ë° ì¶œë ¥
        trend_chart = (line_chart + projection_layer).properties(title="ì—°ë„ë³„ ë…¼ë¬¸ ë°œí–‰ ìˆ˜")
        st.altair_chart(trend_chart, use_container_width=True)
        if show_projection_caption:
            st.caption("ğŸ“ˆ ë¶‰ì€ ì ì„ ì€ 2025ë…„ ìƒë°˜ê¸° ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì—°ê°„ ë°œí–‰ëŸ‰ì„ ì¶”ì •í•œ ì˜ˆìƒì¹˜ì…ë‹ˆë‹¤.")

        # --- íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ë¯¸ë¦¬ë³´ê¸° ---
        st.divider()
        st.subheader("ğŸ“‹ ì²˜ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ë‹¤ìš´ë¡œë“œ")
        
        df_final_output = df_final.copy()
        if 'DE' in df_final_output.columns:
            df_final_output['DE'] = df_final_output['DE_cleaned']
        if 'ID' in df_final_output.columns:
            df_final_output['ID'] = df_final_output['ID_cleaned']
        cols_to_drop = ['Classification', 'DE_cleaned', 'ID_cleaned', 'DE_Original', 'ID_Original']
        df_final_output = df_final_output.drop(columns=[col for col in cols_to_drop if col in df_final_output.columns], errors='ignore')
        
        st.dataframe(df_final_output.head(10), use_container_width=True)
        
        st.markdown("<br>", unsafe_allow_html=True)
        text_data = convert_df_to_scimat_format(df_final_output)
        st.download_button(
            label="ğŸ“¥ SciMAT í˜¸í™˜ í¬ë§· íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.txt)",
            data=text_data,
            file_name="wos_prep_for_scimat.txt",
            mime="text/plain",
            type="primary",
            use_container_width=True
        )
        if st.checkbox("SciMAT ì‚¬ìš© ê°€ì´ë“œ ë³´ê¸°"):
            st.info("""
            **ğŸ’¡ SciMAT ì‚¬ìš© ê°€ì´ë“œ:**
            1. ë‹¤ìš´ë¡œë“œí•œ `wos_prep_for_scimat.txt` íŒŒì¼ì„ SciMATì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
            2. `Group set` â†’ `Words groups manager`ì—ì„œ Levenshtein distanceë¥¼ í™œìš©í•´ ìœ ì‚¬ í‚¤ì›Œë“œë¥¼ ìë™ìœ¼ë¡œ ê·¸ë£¹í•‘í•©ë‹ˆë‹¤.
            3. ìˆ˜ë™ìœ¼ë¡œ í‚¤ì›Œë“œ ê·¸ë£¹ì„ ìµœì¢… ì¡°ì •í•œ í›„ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
            """)
