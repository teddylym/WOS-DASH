import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import base64
from pathlib import Path

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Prep for SciMAT | Hanyang University Edition",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- ì´ë¯¸ì§€ íŒŒì¼ì„ Base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜ ---
# ì´ í•¨ìˆ˜ëŠ” ë¡œê³  íŒŒì¼ì„ ì½”ë“œì— ì§ì ‘ ì‚½ì…í•˜ì—¬ GitHub ë°°í¬ ì‹œ íŒŒì¼ ê²½ë¡œ ë¬¸ì œë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
def get_image_as_base64(path):
    # ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if not Path(path).exists():
        return None
    with open(path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode()

# --- ë¡œê³  ë° CSS ì„¤ì • (í•œì–‘ëŒ€ UI ì ìš©) ---
HANYANG_BLUE = "#003C71"  # í•œì–‘ëŒ€í•™êµ ê³µì‹ ë¸”ë£¨ ìƒ‰ìƒ
LOGO_PATH = "HYU_logotype_blue_eng.png" # ì½”ë“œì™€ ë™ì¼í•œ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•  ë¡œê³  íŒŒì¼

encoded_logo = get_image_as_base64(LOGO_PATH)

# ë¡œê³ ê°€ íŒŒì¼ ê²½ë¡œì— ì¡´ì¬í•  ê²½ìš°ì—ë§Œ HTML ìƒì„±
if encoded_logo:
    logo_html = f'<img src="data:image/png;base64,{encoded_logo}" style="height: 50px; margin-right: 20px;">'
else:
    logo_html = "" # ë¡œê³  íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹„ì›Œë‘ 

# ì»¤ìŠ¤í…€ CSS
st.markdown(f"""
<style>
    /* ì „ì²´ ë°°ê²½ ë° í°íŠ¸ */
    .stApp {{
        background: #f8f9fa;
        font-family: 'Malgun Gothic', 'Apple SD Gothic Neo', sans-serif;
    }}

    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    .app-header {{
        display: flex;
        align-items: center;
        justify-content: flex-start;
        background-color: white;
        padding: 1.5rem 1.5rem;
        border-radius: 12px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.08);
        margin-bottom: 2rem;
    }}

    .app-header h1 {{
        font-size: 2rem;
        font-weight: 700;
        color: {HANYANG_BLUE};
        margin: 0;
        line-height: 1.2;
    }}

    /* ë©”íŠ¸ë¦­ ì¹´ë“œ ìŠ¤íƒ€ì¼ */
    .metric-card {{
        background: white;
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        margin-bottom: 16px;
        transition: all 0.3s ease;
        text-align: center;
    }}

    .metric-card:hover {{
        box-shadow: 0 6px 20px rgba(0, 60, 113, 0.15);
        border-color: {HANYANG_BLUE};
        transform: translateY(-3px);
    }}

    .metric-value {{
        font-size: 2.5rem;
        font-weight: 700;
        color: {HANYANG_BLUE};
    }}

    .metric-label {{
        font-size: 1rem;
        color: #6c757d;
        font-weight: 500;
    }}

    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton > button {{
        font-weight: 600;
        border-radius: 8px;
        border: 2px solid {HANYANG_BLUE};
        background-color: {HANYANG_BLUE};
        color: white;
        width: 100%;
        transition: all 0.3s ease;
    }}
    .stButton > button:hover {{
        background-color: white;
        color: {HANYANG_BLUE};
    }}
    .stDownloadButton > button {{
        font-weight: 600;
        border-radius: 8px;
        border: 2px solid {HANYANG_BLUE};
        background-color: {HANYANG_BLUE};
        color: white;
        width: 100%;
        transition: all 0.3s ease;
    }}
    .stDownloadButton > button:hover {{
        background-color: white;
        color: {HANYANG_BLUE};
    }}


    /* êµ¬ë¶„ì„  ìŠ¤íƒ€ì¼ */
    hr {{
        border-top: 1px solid #dee2e6;
        margin-top: 2rem;
        margin-bottom: 2rem;
    }}
</style>
""", unsafe_allow_html=True)


# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ) ---
@st.cache_resource
def download_nltk_resources():
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet')
download_nltk_resources()


# --- í•µì‹¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_keywords(keywords):
    if not isinstance(keywords, str):
        return []
    # ì„¸ë¯¸ì½œë¡  ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    keywords = re.split(r';\s*', keywords)
    processed = []
    for keyword in keywords:
        kw_lower = keyword.lower().strip()
        # ë¶ˆìš©ì–´ ì²˜ë¦¬ ë° í‘œì œì–´ ì¶”ì¶œ
        if kw_lower and kw_lower not in stop_words:
            lemma = lemmatizer.lemmatize(kw_lower)
            processed.append(lemma)
    return processed

def convert_df_to_scimat_format(df):
    output = []
    for _, row in df.iterrows():
        # í•„ìˆ˜ í•„ë“œë§Œ ì„ íƒí•˜ì—¬ SciMAT í˜•ì‹ì— ë§ê²Œ ë³€í™˜
        required_fields = {
            'AU': row.get('Authors', ''),
            'TI': row.get('Article Title', ''),
            'SO': row.get('Source Title', ''),
            'DE': row.get('Author Keywords', ''),
            'ID': row.get('Keywords Plus', ''),
            'AB': row.get('Abstract', ''),
            'C1': row.get('Addresses', ''),
            'PY': row.get('Publication Year', ''),
            'TC': row.get('WoS Core Collection Times Cited Count', '')
        }

        entry = ""
        for key, value in required_fields.items():
            # ê°’ì´ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì„ ê²½ìš°ì—ë§Œ ì¶”ê°€
            if pd.notna(value) and str(value).strip() != '':
                entry += f"{key} {str(value)}\n"

        output.append(entry + "ER\n")

    # íŒŒì¼ ì‹œì‘ ë¶€ë¶„ì— í•„ìˆ˜ í—¤ë” ì¶”ê°€
    header = "FN Thomson Reuters Web of Science\nVR 1.0\n"
    return header + "\n".join(output)

# --- ì•± UI êµ¬ì„± ---

# í—¤ë”
st.markdown(f"""
<div class="app-header">
    {logo_html}
    <h1>WOS Prep for SciMAT</h1>
</div>
""", unsafe_allow_html=True)

if not encoded_logo:
    st.warning(f"ë¡œê³  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `{LOGO_PATH}` íŒŒì¼ì´ ì½”ë“œì™€ ë™ì¼í•œ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

st.markdown("Web of Science ë°ì´í„°ë¥¼ SciMAT ë¶„ì„ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì •ì œí•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤. ì•„ë˜ ì ˆì°¨ì— ë”°ë¼ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  í‚¤ì›Œë“œë¥¼ ì •ê·œí™”í•˜ì„¸ìš”.")

# 1. íŒŒì¼ ì—…ë¡œë“œ
st.markdown("---")
st.header("1ë‹¨ê³„: WOS ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ")
st.info("Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ **'Plain Text'** í˜•ì‹ì˜ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", icon="ğŸ“‚")

uploaded_file = st.file_uploader(
    "íŒŒì¼ì„ ì´ê³³ì— ë“œë˜ê·¸í•˜ê±°ë‚˜ í´ë¦­í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”.",
    type=['txt'],
    label_visibility="collapsed"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'df' not in st.session_state:
    st.session_state.df = None
    st.session_state.df_final = None
    st.session_state.normalization_applied = False
    st.session_state.normalized_count = 0


if uploaded_file is not None:
    try:
        # íŒŒì¼ íŒŒì‹±
        content = uploaded_file.getvalue().decode('utf-8-sig')
        articles_raw = content.strip().split('\nER\n')
        
        records = []
        for article in articles_raw:
            if not article.strip().startswith('FN'): continue # íŒŒì¼ í—¤ë” ê±´ë„ˆë›°ê¸°
            
            record = {}
            # PT, AU, TI ë“± 2ê¸€ì ì½”ë“œë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ë§Œ í•„ë“œë¡œ ì¸ì‹
            lines = article.strip().split('\n')
            field = None
            value_lines = []
            
            for i, line in enumerate(lines):
                # íŒŒì¼ì˜ ì‹œì‘(FN, VR)ì€ ê±´ë„ˆë›´ë‹¤.
                if i < 2 and (line.startswith("FN") or line.startswith("VR")):
                    continue
                
                # ìƒˆë¡œìš´ í•„ë“œê°€ ì‹œì‘ë˜ë©´ ì´ì „ í•„ë“œ ì €ì¥
                if len(line) > 2 and line[2] == ' ' and line[:2].isalpha() and line[:2].isupper():
                    if field and value_lines:
                        record[field] = ' '.join(value_lines)
                    field = line[:2]
                    value_lines = [line[3:].strip()]
                # í•„ë“œê°€ ê³„ì†ë˜ë©´ ë‚´ìš© ì¶”ê°€
                elif field:
                    value_lines.append(line.strip())
            
            # ë§ˆì§€ë§‰ í•„ë“œ ì €ì¥
            if field and value_lines:
                record[field] = ' '.join(value_lines)
                
            if 'TI' in record: # ì œëª©ì´ ìˆëŠ” ë ˆì½”ë“œë§Œ ì¶”ê°€
                records.append(record)
        
        df = pd.DataFrame(records)
        # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ (WOS í‘œì¤€ í•„ë“œ -> ì´í•´í•˜ê¸° ì‰¬ìš´ ì´ë¦„)
        df.rename(columns={
            'AU': 'Authors', 'TI': 'Article Title', 'SO': 'Source Title',
            'DE': 'Author Keywords', 'ID': 'Keywords Plus', 'AB': 'Abstract',
            'C1': 'Addresses', 'PY': 'Publication Year', 'TC': 'WoS Core Collection Times Cited Count'
        }, inplace=True)
        
        st.session_state.df = df
        st.session_state.df_final = df.copy() # ì´ˆê¸° ìµœì¢…ë³¸ì€ ì›ë³¸ê³¼ ë™ì¼
        st.success(f"**{len(df)}ê°œ**ì˜ ë…¼ë¬¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.warning("íŒŒì¼ì´ Web of Scienceì˜ 'Plain Text' í˜•ì‹ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()


if st.session_state.df is not None:
    df = st.session_state.df

    # 2. í‚¤ì›Œë“œ ì •ê·œí™”
    st.markdown("---")
    st.header("2ë‹¨ê³„: í‚¤ì›Œë“œ ì •ê·œí™” (ì„ íƒ ì‚¬í•­)")
    st.markdown("ë¶„ì„ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ë™ì˜ì–´, ë‹¨/ë³µìˆ˜í˜• ë“±ì„ ëŒ€í‘œ í‚¤ì›Œë“œë¡œ í†µì¼í•©ë‹ˆë‹¤. **'Author Keywords'**ì™€ **'Keywords Plus'**ì— ëª¨ë‘ ì ìš©ë©ë‹ˆë‹¤.")
    
    # í‚¤ì›Œë“œ ì²˜ë¦¬ ë° ë¹ˆë„ ë¶„ì„ (df_processedëŠ” UI í‘œì‹œìš© ì„ì‹œ ë°ì´í„°í”„ë ˆì„)
    df_processed = df.copy()
    df_processed['DE_processed'] = df_processed['Author Keywords'].apply(preprocess_keywords)
    df_processed['ID_processed'] = df_processed['Keywords Plus'].apply(preprocess_keywords)
    
    all_keywords = df_processed['DE_processed'].sum() + df_processed['ID_processed'].sum()
    keyword_counts = Counter(all_keywords)
    df_keywords = pd.DataFrame(keyword_counts.items(), columns=['Keyword', 'Frequency']).sort_values(by='Frequency', ascending=False).reset_index(drop=True)

    with st.expander("í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„ ë° ì •ê·œí™” ê·œì¹™ ì„¤ì • ë³´ê¸°", expanded=False):
        st.write(f"ì´ **{len(df_keywords)}ê°œ**ì˜ ê³ ìœ  í‚¤ì›Œë“œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. (í‘œì œì–´ ì¶”ì¶œ ë° ë¶ˆìš©ì–´ ì œê±° í›„)")
        st.dataframe(df_keywords, height=300, use_container_width=True)
        
        st.markdown("<h6>ì •ê·œí™” ê·œì¹™ ì…ë ¥</h6>", unsafe_allow_html=True)
        st.markdown("`ë³€ê²½ë  í‚¤ì›Œë“œ: ëŒ€í‘œ í‚¤ì›Œë“œ` í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”. ì—¬ëŸ¬ ê·œì¹™ì€ **ì‰¼í‘œ(,)**ë¡œ êµ¬ë¶„í•©ë‹ˆë‹¤.")
        
        normalization_rules_str = st.text_area(
            "ì •ê·œí™” ê·œì¹™:", 
            key="normalization_rules",
            height=120,
            placeholder="ì˜ˆ: big data analytics: big data, ai: artificial intelligence, technologies: technology"
        )

    # 3. ì²˜ë¦¬ ë° ë‹¤ìš´ë¡œë“œ
    st.markdown("---")
    st.header("3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ")
    
    if st.button("âœ¨ ì •ê·œí™” ì ìš© ë° ê²°ê³¼ ìƒì„±", use_container_width=True):
        df_final = df.copy() # í•­ìƒ ì›ë³¸ì—ì„œ ì‹œì‘
        
        if normalization_rules_str.strip():
            rules_list = [rule.strip() for rule in normalization_rules_str.split(',') if ':' in rule]
            normalization_map = {}
            for rule in rules_list:
                try:
                    target, representative = rule.split(':')
                    normalization_map[target.strip().lower()] = representative.strip()
                except ValueError:
                    st.warning(f"ì˜ëª»ëœ í˜•ì‹ì˜ ê·œì¹™ '{rule}'ì€(ëŠ”) ë¬´ì‹œë©ë‹ˆë‹¤. 'í‚¤ì›Œë“œ: ëŒ€í‘œí‚¤ì›Œë“œ' í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”.")
            
            def apply_normalization(keywords_str):
                if not isinstance(keywords_str, str): return keywords_str, False
                
                keywords = re.split(r';\s*', keywords_str)
                normalized_keywords = []
                was_normalized = False
                
                for kw in keywords:
                    # ê·œì¹™ ì ìš©ì„ ìœ„í•´ ì†Œë¬¸ì+í‘œì œì–´ ë³€í™˜
                    kw_processed = lemmatizer.lemmatize(kw.lower().strip())
                    
                    if kw_processed in normalization_map:
                        normalized_keywords.append(normalization_map[kw_processed])
                        was_normalized = True
                    else:
                        normalized_keywords.append(kw.strip()) # ì›ë³¸ í‚¤ì›Œë“œ ìœ ì§€
                
                # ì¤‘ë³µ ì œê±° í›„ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ê²°í•©
                final_keywords = '; '.join(sorted(list(set(normalized_keywords))))
                return final_keywords, was_normalized

            # 'Author Keywords'ì™€ 'Keywords Plus'ì— ì •ê·œí™” ì ìš©
            res_de = df_final['Author Keywords'].apply(apply_normalization)
            res_id = df_final['Keywords Plus'].apply(apply_normalization)
            
            df_final['Author Keywords'], normalized_de = zip(*res_de)
            df_final['Keywords Plus'], normalized_id = zip(*res_id)
            
            # í•˜ë‚˜ë¼ë„ ì •ê·œí™”ê°€ ì ìš©ëœ í–‰ì„ ì°¾ê¸° ìœ„í•œ ë§ˆìŠ¤í¬
            normalization_mask = pd.Series(normalized_de) | pd.Series(normalized_id)
            st.session_state.normalized_count = normalization_mask.sum()
            st.success(f"**{st.session_state.normalized_count}ê°œ** ë…¼ë¬¸ì˜ í‚¤ì›Œë“œì— ì •ê·œí™” ê·œì¹™ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.session_state.normalized_count = 0
            st.info("ì…ë ¥ëœ ì •ê·œí™” ê·œì¹™ì´ ì—†ì–´ ì›ë³¸ ë°ì´í„°ë¡œ ìµœì¢… íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        
        st.session_state.df_final = df_final
        st.session_state.normalization_applied = True


    if st.session_state.normalization_applied:
        df_to_show = st.session_state.df_final
        
        st.markdown("<h4>ìµœì¢… ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°</h4>", unsafe_allow_html=True)
        st.dataframe(df_to_show.head(), use_container_width=True)

        st.markdown("<h4>ìš”ì•½ í†µê³„</h4>", unsafe_allow_html=True)
        col1, col2 = st.columns(2)
        with col1:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-value">{len(df_to_show):,}</div>
                <div class="metric-label">ìµœì¢… ë¶„ì„ ëŒ€ìƒ ë…¼ë¬¸</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-value">{st.session_state.normalized_count:,}</div>
                <div class="metric-label">í‚¤ì›Œë“œ ì •ê·œí™” ì ìš© ë…¼ë¬¸</div>
            </div>
            """, unsafe_allow_html=True)

        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        text_data = convert_df_to_scimat_format(df_to_show)
        st.download_button(
            label="ğŸ“¥ SciMAT í˜¸í™˜ í¬ë§· íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.txt)",
            data=text_data,
            file_name="wos_for_scimat_hanyang.txt",
            mime="text/plain"
        )
