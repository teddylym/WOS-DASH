import streamlit as st
import pandas as pd
import altair as alt
import io
import re
from collections import Counter

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Multi-File Merger | SCIMAT Edition",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ ---
st.markdown("""
<style>
    .main-container {
        background: #f8f9fa;
        min-height: 100vh;
    }
    
    .metric-card {
        background: white;
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        margin-bottom: 16px;
        transition: all 0.3s ease;
    }
    
    .metric-card:hover {
        box-shadow: 0 4px 20px rgba(0,56,117,0.15);
        border-color: #003875;
    }
    
    .metric-value {
        font-size: 2.8rem;
        font-weight: 700;
        color: #003875;
        margin: 0;
        line-height: 1;
    }
    
    .metric-label {
        font-size: 1rem;
        color: #6c757d;
        margin: 8px 0 0 0;
        font-weight: 500;
    }
    
    .metric-icon {
        background: linear-gradient(135deg, #003875, #0056b3);
        color: white;
        width: 48px;
        height: 48px;
        border-radius: 12px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.5rem;
        margin-bottom: 16px;
    }
    
    .chart-container {
        background: white;
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        margin: 16px 0;
    }
    
    .chart-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #212529;
        margin-bottom: 16px;
        border-bottom: 2px solid #003875;
        padding-bottom: 8px;
    }
    
    .section-header {
        background: linear-gradient(135deg, #003875, #0056b3);
        color: white;
        padding: 20px 24px;
        border-radius: 12px;
        margin: 24px 0 16px 0;
        box-shadow: 0 4px 16px rgba(0,56,117,0.2);
    }
    
    .section-title {
        font-size: 1.5rem;
        font-weight: 600;
        margin: 0;
    }
    
    .section-subtitle {
        font-size: 1rem;
        opacity: 0.9;
        margin: 4px 0 0 0;
    }
    
    .info-panel {
        background: #e8f0fe;
        border: 1px solid #003875;
        border-radius: 8px;
        padding: 16px;
        margin: 16px 0;
    }
    
    .success-panel {
        background: #d4edda;
        border: 1px solid #28a745;
        border-radius: 8px;
        padding: 16px;
        margin: 16px 0;
    }
    
    .warning-panel {
        background: #fff3cd;
        border: 1px solid #ffc107;
        border-radius: 8px;
        padding: 16px;
        margin: 16px 0;
    }
    
    .feature-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: 20px;
        margin: 24px 0;
    }
    
    .feature-card {
        background: white;
        border-radius: 12px;
        padding: 24px;
        text-align: center;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        transition: all 0.3s ease;
    }
    
    .feature-card:hover {
        transform: translateY(-4px);
        box-shadow: 0 8px 24px rgba(0,56,117,0.15);
        border-color: #003875;
    }
    
    .feature-icon {
        font-size: 3rem;
        margin-bottom: 16px;
        background: linear-gradient(135deg, #003875, #0056b3);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .feature-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #212529;
        margin-bottom: 8px;
    }
    
    .feature-desc {
        font-size: 0.95rem;
        color: #6c757d;
        line-height: 1.5;
    }
    
    .upload-zone {
        background: white;
        border: 2px dashed #003875;
        border-radius: 12px;
        padding: 40px 20px;
        text-align: center;
        margin: 20px 0;
        transition: all 0.3s ease;
    }
    
    .upload-zone:hover {
        background: #f8f9fa;
        border-color: #0056b3;
    }
    
    .progress-indicator {
        background: linear-gradient(90deg, #003875, #0056b3);
        height: 4px;
        border-radius: 2px;
        margin: 16px 0;
        animation: pulse 2s infinite;
    }
    
    .file-status {
        background: #f8f9fa;
        border-radius: 8px;
        padding: 12px;
        margin: 8px 0;
        border-left: 4px solid #003875;
    }
    
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.7; }
    }
</style>
""", unsafe_allow_html=True)

# --- ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ë¡œë”© ë° ë³‘í•© í•¨ìˆ˜ ---
def load_and_merge_wos_files(uploaded_files):
    """ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ì„ ë¡œë”©í•˜ê³  ë³‘í•© - ì¤‘ë³µ ì œê±° ì™„ì „ ìˆ˜ì •"""
    all_dataframes = []
    file_status = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            file_bytes = uploaded_file.getvalue()
            encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'iso-8859-1']
            
            df = None
            encoding_used = None
            
            for encoding in encodings_to_try:
                try:
                    file_content = file_bytes.decode(encoding)
                    
                    # WOS ì›ë³¸ í˜•ì‹ ê²€ì¦ (FNìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•¨)
                    if not file_content.strip().startswith('FN '):
                        continue
                        
                    # WOS í˜•ì‹ íŒŒì‹±
                    df = parse_wos_format(file_content)
                    if df is not None and len(df) > 0:
                        encoding_used = encoding
                        break
                        
                except Exception:
                    continue
            
            if df is not None:
                all_dataframes.append(df)
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'SUCCESS',
                    'papers': len(df),
                    'encoding': encoding_used,
                    'message': f'âœ… {len(df)}í¸ ë…¼ë¬¸ ë¡œë”© ì„±ê³µ'
                })
            else:
                file_status.append({
                    'filename': uploaded_file.name,
                    'status': 'ERROR',
                    'papers': 0,
                    'encoding': 'N/A',
                    'message': 'âŒ WOS Plain Text í˜•ì‹ì´ ì•„ë‹˜'
                })
                
        except Exception as e:
            file_status.append({
                'filename': uploaded_file.name,
                'status': 'ERROR',
                'papers': 0,
                'encoding': 'N/A',
                'message': f'âŒ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}'
            })
    
    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ë³‘í•©
    if all_dataframes:
        merged_df = pd.concat(all_dataframes, ignore_index=True)
        
        # ì¤‘ë³µ ì œê±° ë¡œì§ - ìˆ˜ì •ëœ ë²„ì „
        duplicates_removed = 0
        
        if 'UT' in merged_df.columns:
            # UT í•„ë“œì˜ ìœ íš¨í•œ ê°’ë§Œ í•„í„°ë§
            # ë¹ˆ ê°’, NaN, 'nan' ë¬¸ìì—´, ê³µë°± ë“±ì„ ë¬´íš¨í•œ ê°’ìœ¼ë¡œ ì²˜ë¦¬
            def is_valid_ut(value):
                if pd.isna(value):
                    return False
                str_value = str(value).strip().lower()
                if str_value in ['', 'nan', 'none', 'null']:
                    return False
                return len(str_value) > 0
            
            # ìœ íš¨í•œ UT ê°’ì„ ê°€ì§„ í–‰ë“¤ë§Œ í•„í„°ë§
            valid_ut_mask = merged_df['UT'].apply(is_valid_ut)
            valid_ut_df = merged_df[valid_ut_mask]
            
            if len(valid_ut_df) > 0:
                # ìœ íš¨í•œ UT ê°’ë“¤ ì¤‘ì—ì„œë§Œ ì¤‘ë³µ í™•ì¸
                ut_counts = valid_ut_df['UT'].value_counts()
                actual_duplicates = ut_counts[ut_counts > 1]
                
                if len(actual_duplicates) > 0:
                    # ì‹¤ì œ ì¤‘ë³µì´ ìˆëŠ” ê²½ìš°ë§Œ ì œê±°
                    duplicates_removed = actual_duplicates.sum() - len(actual_duplicates)
                    
                    # ìœ íš¨í•œ UTë¥¼ ê°€ì§„ í–‰ë“¤ì—ì„œ ì¤‘ë³µ ì œê±°
                    deduplicated_valid = valid_ut_df.drop_duplicates(subset=['UT'], keep='first')
                    
                    # ë¬´íš¨í•œ UTë¥¼ ê°€ì§„ í–‰ë“¤ê³¼ ì¤‘ë³µ ì œê±°ëœ ìœ íš¨í•œ í–‰ë“¤ì„ ë‹¤ì‹œ ë³‘í•©
                    invalid_ut_df = merged_df[~valid_ut_mask]
                    merged_df = pd.concat([deduplicated_valid, invalid_ut_df], ignore_index=True)
                # ì¤‘ë³µì´ ì—†ìœ¼ë©´ duplicates_removedëŠ” 0 ìœ ì§€
        
        return merged_df, file_status, duplicates_removed
    else:
        return None, file_status, 0

def parse_wos_format(content):
    """WOS Plain Text í˜•ì‹ì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    lines = content.split('\n')
    records = []
    current_record = {}
    current_field = None
    
    for line in lines:
        line = line.rstrip()
        
        if not line:
            continue
            
        # ë ˆì½”ë“œ ì¢…ë£Œ
        if line == 'ER':
            if current_record:
                records.append(current_record.copy())
                current_record = {}
                current_field = None
            continue
            
        # í—¤ë” ë¼ì¸ ê±´ë„ˆë›°ê¸°
        if line.startswith(('FN ', 'VR ')):
            continue
            
        # ìƒˆ í•„ë“œ ì‹œì‘
        if not line.startswith('   ') and ' ' in line:
            parts = line.split(' ', 1)
            if len(parts) == 2:
                field_tag, field_value = parts
                current_field = field_tag
                current_record[field_tag] = field_value.strip()
        
        # ê¸°ì¡´ í•„ë“œ ì—°ì†
        elif line.startswith('   ') and current_field and current_field in current_record:
            continuation_value = line[3:].strip()
            if continuation_value:
                current_record[current_field] += '; ' + continuation_value
    
    # ë§ˆì§€ë§‰ ë ˆì½”ë“œ ì²˜ë¦¬
    if current_record:
        records.append(current_record)
    
    if not records:
        return None
        
    return pd.DataFrame(records)

# --- ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° íŠ¹í™” ë¶„ë¥˜ í•¨ìˆ˜ ---
def classify_article(row):
    """ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ë¥¼ ìœ„í•œ í¬ê´„ì  ë¶„ë¥˜ - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
    
    # í•µì‹¬ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° í‚¤ì›Œë“œ (ì§ì ‘ì  ê´€ë ¨ì„±)
    core_streaming_keywords = [
        'live streaming', 'livestreaming', 'live stream', 'live broadcast', 'live video',
        'real time streaming', 'real-time streaming', 'streaming platform', 'streaming service',
        'live webcast', 'webcasting', 'live transmission', 'interactive broadcasting',
        'live commerce', 'live shopping', 'live selling', 'livestream commerce',
        'live e-commerce', 'social commerce', 'live marketing', 'streaming monetization',
        'live retail', 'shoppertainment', 'live sales', 'streaming sales',
        'streamer', 'viewer', 'audience engagement', 'streaming audience', 'live audience',
        'streaming behavior', 'viewer behavior', 'streaming experience', 'live interaction',
        'streaming community', 'online community', 'digital community', 'virtual community',
        'parasocial relationship', 'streamer-viewer', 'live chat', 'chat interaction',
        'twitch', 'youtube live', 'facebook live', 'instagram live', 'tiktok live',
        'periscope', 'mixer', 'douyin', 'kuaishou', 'taobao live', 'tmall live',
        'amazon live', 'shopee live', 'live.ly', 'bigo live',
        'live gaming', 'game streaming', 'esports streaming', 'streaming content',
        'live entertainment', 'live performance', 'virtual concert', 'live music'
    ]
    
    # í™•ì¥ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë° ì»¤ë¨¸ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ
    business_commerce_keywords = [
        'e-commerce', 'online shopping', 'digital commerce', 'mobile commerce', 'm-commerce',
        'social commerce', 'influencer marketing', 'content creator', 'digital marketing', 
        'brand engagement', 'consumer behavior', 'purchase intention', 'buying behavior',
        'social influence', 'word of mouth', 'viral marketing', 'user generated content',
        'brand advocacy', 'customer engagement', 'social media marketing', 'digital influence',
        'online influence', 'interactive marketing', 'personalized marketing', 'real-time marketing',
        'digital transformation', 'omnichannel', 'customer experience', 'user experience',
        'engagement marketing', 'social selling', 'digital retail', 'online retail'
    ]
    
    # êµìœ¡ ë° í•™ìŠµ ê´€ë ¨ í‚¤ì›Œë“œ (í™•ì¥)
    education_keywords = [
        'online education', 'e-learning', 'distance learning', 'remote learning',
        'virtual classroom', 'online teaching', 'digital learning', 'mooc',
        'educational technology', 'learning management system', 'blended learning',
        'medical education', 'nursing education', 'surgical training', 'clinical education',
        'telemedicine', 'telehealth', 'digital health', 'health education',
        'interactive learning', 'synchronous learning', 'real-time learning'
    ]
    
    # ê¸°ìˆ ì  ê¸°ë°˜ í‚¤ì›Œë“œ (í™•ì¥)
    technical_keywords = [
        'real time video', 'real-time video', 'video streaming', 'audio streaming',
        'multimedia streaming', 'video compression', 'video encoding', 'video delivery',
        'content delivery', 'cdn', 'edge computing', 'multimedia communication',
        'video communication', 'webrtc', 'peer to peer streaming', 'p2p streaming',
        'distributed streaming', 'mobile streaming', 'mobile video', 'wireless streaming',
        'mobile broadcast', 'smartphone streaming', 'live video transmission',
        'streaming technology', 'adaptive streaming', 'video quality', 'streaming latency',
        'interactive media', 'virtual reality', 'augmented reality', 'vr', 'ar',
        '3d streaming', 'immersive media', 'metaverse'
    ]
    
    # ì‚¬íšŒë¬¸í™”ì  ì˜í–¥ í‚¤ì›Œë“œ (ì‹ ê·œ ì¶”ê°€)
    sociocultural_keywords = [
        'digital culture', 'online culture', 'virtual community', 'digital society',
        'social media', 'social network', 'digital communication', 'online interaction',
        'digital identity', 'virtual identity', 'online presence', 'digital participation',
        'cultural transmission', 'digital religion', 'online religion', 'virtual religion',
        'digital migration', 'online migration', 'digital diaspora', 'virtual diaspora',
        'social cohesion', 'community building', 'social capital', 'digital divide'
    ]
    
    # COVID-19 ë° íŒ¬ë°ë¯¹ ê´€ë ¨ í‚¤ì›Œë“œ (ì‹ ê·œ ì¶”ê°€)
    pandemic_keywords = [
        'covid-19', 'pandemic', 'coronavirus', 'sars-cov-2', 'lockdown', 'quarantine',
        'social distancing', 'remote work', 'work from home', 'digital adaptation',
        'pandemic response', 'crisis communication', 'emergency response'
    ]
    
    # í™•ì¥ëœ ì†Œì…œ ë¯¸ë””ì–´ í‚¤ì›Œë“œ
    social_media_keywords = [
        'social media', 'social network', 'online platform', 'digital platform',
        'user experience', 'user behavior', 'online behavior', 'digital behavior',
        'social interaction', 'online interaction', 'digital interaction',
        'user engagement', 'digital engagement', 'platform economy', 'network effects',
        'viral content', 'content sharing', 'social sharing', 'online community'
    ]
    
    # ì œì™¸ í‚¤ì›Œë“œ (ê¸°ìˆ ì  ë¹„ê´€ë ¨ - ë” ì—„ê²©í•˜ê²Œ)
    exclusion_keywords = [
        'routing protocol', 'network topology', 'packet routing', 'mac protocol',
        'ieee 802.11', 'wimax protocol', 'lte protocol', 'network security protocol',
        'vlsi design', 'circuit design', 'antenna design', 'rf circuit',
        'hardware implementation', 'fpga implementation', 'asic design',
        'signal processing algorithm', 'modulation scheme', 'channel estimation',
        'beamforming algorithm', 'mimo antenna', 'ofdm modulation',
        'frequency allocation', 'spectrum allocation', 'wireless sensor network',
        'satellite communication', 'underwater communication', 'space communication',
        'biomedical signal', 'medical imaging', 'radar system', 'sonar system',
        'geological survey', 'seismic data', 'astronomical data', 'climate modeling'
    ]
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    def extract_text(value):
        if pd.isna(value) or value is None:
            return ""
        return str(value).lower().strip()
    
    title = extract_text(row.get('TI', ''))
    source_title = extract_text(row.get('SO', ''))
    author_keywords = extract_text(row.get('DE', ''))
    keywords_plus = extract_text(row.get('ID', ''))
    abstract = extract_text(row.get('AB', ''))
    
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    
    # ê°œì„ ëœ ë¶„ë¥˜ ë¡œì§
    # 1. ëª…í™•í•œ ì œì™¸ ëŒ€ìƒ ë¨¼ì € í•„í„°ë§
    if any(keyword in full_text for keyword in exclusion_keywords):
        return 'Exclude (ê¸°ìˆ ì  ë¹„ê´€ë ¨)'
    
    # 2. í•µì‹¬ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° í‚¤ì›Œë“œ - ìµœìš°ì„  í¬í•¨
    if any(keyword in full_text for keyword in core_streaming_keywords):
        return 'Include (í•µì‹¬ì—°êµ¬)'
    
    # 3. ë¹„ì¦ˆë‹ˆìŠ¤/ì»¤ë¨¸ìŠ¤ + ë””ì§€í„¸ ì§€í‘œ - í¬ìš©ì  ì ‘ê·¼
    if any(keyword in full_text for keyword in business_commerce_keywords):
        digital_indicators = ['digital', 'online', 'internet', 'web', 'social media', 'platform', 'mobile', 'app', 'virtual', 'interactive']
        if any(indicator in full_text for indicator in digital_indicators):
            return 'Include (ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨)'
    
    # 4. êµìœ¡ + ì˜¨ë¼ì¸/ë””ì§€í„¸ ì§€í‘œ - í¬ìš©ì  ì ‘ê·¼
    if any(keyword in full_text for keyword in education_keywords):
        online_indicators = ['online', 'digital', 'virtual', 'remote', 'distance', 'interactive', 'real-time', 'synchronous']
        if any(indicator in full_text for indicator in online_indicators):
            return 'Include (êµìœ¡ ê´€ë ¨)'
    
    # 5. ê¸°ìˆ ì  ê¸°ë°˜ + ë¼ì´ë¸Œ/ì‹¤ì‹œê°„ ì§€í‘œ
    if any(keyword in full_text for keyword in technical_keywords):
        live_indicators = ['live', 'real time', 'real-time', 'streaming', 'broadcast', 'interactive', 'synchronous', 'instant']
        if any(indicator in full_text for indicator in live_indicators):
            return 'Include (ê¸°ìˆ ì  ê¸°ë°˜)'
    
    # 6. ì‚¬íšŒë¬¸í™”ì  + ë””ì§€í„¸ ì§€í‘œ (ì‹ ê·œ)
    if any(keyword in full_text for keyword in sociocultural_keywords):
        digital_indicators = ['digital', 'online', 'virtual', 'internet', 'web', 'platform', 'social media']
        if any(indicator in full_text for indicator in digital_indicators):
            return 'Include (ì‚¬íšŒë¬¸í™” ê´€ë ¨)'
    
    # 7. íŒ¬ë°ë¯¹ ê´€ë ¨ + ë””ì§€í„¸ ì§€í‘œ (ì‹ ê·œ)
    if any(keyword in full_text for keyword in pandemic_keywords):
        digital_indicators = ['digital', 'online', 'virtual', 'remote', 'streaming', 'platform', 'technology']
        if any(indicator in full_text for indicator in digital_indicators):
            return 'Include (íŒ¬ë°ë¯¹ ë””ì§€í„¸í™”)'
    
    # 8. ì†Œì…œ ë¯¸ë””ì–´ ì¼ë°˜ - ì¡°ê±´ë¶€ í¬í•¨
    if any(keyword in full_text for keyword in social_media_keywords):
        interaction_indicators = ['interaction', 'engagement', 'community', 'sharing', 'content', 'creator', 'influencer']
        if any(indicator in full_text for indicator in interaction_indicators):
            return 'Include (ì†Œì…œë¯¸ë””ì–´ ê´€ë ¨)'
        else:
            return 'Review (ì†Œì…œë¯¸ë””ì–´ ê²€í† )'
    
    # 9. ê¸°íƒ€ - ìµœì†Œ ê²€í†  ëŒ€ìƒ
    return 'Review (ë¶„ë¥˜ ë¶ˆí™•ì‹¤)'

# --- ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ í•¨ìˆ˜ ---
def diagnose_merged_quality(df, file_count, duplicates_removed):
    """ë³‘í•©ëœ WOS ë°ì´í„°ì˜ í’ˆì§ˆ ì§„ë‹¨ - ìˆ˜ì •ëœ ë²„ì „"""
    issues = []
    recommendations = []
    
    # í•„ìˆ˜ í•„ë“œ í™•ì¸
    required_fields = ['TI', 'AU', 'SO', 'PY']
    keyword_fields = ['DE', 'ID']
    
    for field in required_fields:
        if field not in df.columns:
            issues.append(f"âŒ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
        else:
            valid_count = df[field].notna().sum()
            total_count = len(df)
            missing_rate = (total_count - valid_count) / total_count * 100
            
            if missing_rate > 10:
                issues.append(f"âš ï¸ {field} í•„ë“œì˜ {missing_rate:.1f}%ê°€ ëˆ„ë½ë¨")
    
    # í‚¤ì›Œë“œ í•„ë“œ í’ˆì§ˆ í™•ì¸
    has_keywords = False
    for field in keyword_fields:
        if field in df.columns:
            has_keywords = True
            valid_keywords = df[field].notna() & (df[field] != '') & (df[field] != 'nan')
            valid_count = valid_keywords.sum()
            total_count = len(df)
            
            if valid_count < total_count * 0.7:
                issues.append(f"âš ï¸ {field} í•„ë“œì˜ {((total_count-valid_count)/total_count*100):.1f}%ê°€ ë¹„ì–´ìˆìŒ")
    
    if not has_keywords:
        issues.append("âŒ í‚¤ì›Œë“œ í•„ë“œ ì—†ìŒ: DE ë˜ëŠ” ID í•„ë“œ í•„ìš”")
    
    # ë³‘í•© ê´€ë ¨ ì •ë³´ - ì‹¤ì œ ê²°ê³¼ë§Œ ë°˜ì˜ (ì™„ì „ ìˆ˜ì •)
    recommendations.append(f"âœ… {file_count}ê°œ íŒŒì¼ ì„±ê³µì ìœ¼ë¡œ ë³‘í•©ë¨")
    
    # ì¤‘ë³µ ì œê±° ê²°ê³¼ë§Œ ì‹¤ì œ ë°ì´í„°ì— ë”°ë¼ í‘œì‹œ
    if duplicates_removed > 0:
        recommendations.append(f"ğŸ”„ ì¤‘ë³µ ë…¼ë¬¸ {duplicates_removed}í¸ ìë™ ì œê±°ë¨")
    else:
        recommendations.append("âœ… ì¤‘ë³µ ë…¼ë¬¸ ì—†ìŒ - ëª¨ë“  ë…¼ë¬¸ì´ ê³ ìœ  ë°ì´í„°")
    
    recommendations.append("âœ… WOS Plain Text í˜•ì‹ - SCIMAT ìµœì  í˜¸í™˜ì„± í™•ë³´")
    
    return issues, recommendations

# --- WOS Plain Text í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ---
def convert_to_scimat_wos_format(df_to_convert):
    """SCIMAT ì™„ì „ í˜¸í™˜ WOS Plain Text í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']

    for _, row in df_to_convert.iterrows():
        if len(file_content) > 2:
            file_content.append("")
        
        for tag in wos_field_order:
            if tag in row.index and pd.notna(row[tag]) and str(row[tag]).strip() and str(row[tag]).strip().lower() != 'nan':
                value = str(row[tag]).strip()
                
                if tag in multi_line_fields:
                    items = [item.strip() for item in value.split(';') if item.strip()]
                    if items:
                        file_content.append(f"{tag} {items[0]}")
                        for item in items[1:]:
                            file_content.append(f"   {item}")
                else:
                    file_content.append(f"{tag} {value}")
        
        file_content.append("ER")
    
    return "\n".join(file_content).encode('utf-8-sig')

# --- ë©”ì¸ í—¤ë” ---
st.markdown("""
<div style="position: relative; text-align: center; padding: 2rem 0 3rem 0; background: linear-gradient(135deg, #003875, #0056b3); color: white; border-radius: 16px; margin-bottom: 2rem; box-shadow: 0 8px 32px rgba(0,56,117,0.3);">
    <div style="position: absolute; top: 1rem; left: 2rem; color: white;">
        <div style="font-size: 14px; font-weight: 600; margin-bottom: 2px;">HANYANG UNIVERSITY</div>
        <div style="font-size: 12px; opacity: 0.9;">Technology Management Research</div>
        <div style="font-size: 11px; opacity: 0.8; margin-top: 4px;">mot.hanyang.ac.kr</div>
    </div>
    <div style="position: absolute; top: 1rem; right: 2rem; text-align: right; color: rgba(255,255,255,0.9); font-size: 0.85rem;">
        <p style="margin: 0;"><strong>Developed by:</strong> ì„íƒœê²½ (Teddy Lym)</p>
    </div>
    <h1 style="font-size: 3.5rem; font-weight: 700; margin-bottom: 0.5rem; letter-spacing: -0.02em;">
        WOS PREP
    </h1>
    <p style="font-size: 1.3rem; margin: 0; font-weight: 400; opacity: 0.95;">
        SCIMAT Edition
    </p>
    <div style="width: 100px; height: 4px; background-color: rgba(255,255,255,0.3); margin: 2rem auto; border-radius: 2px;"></div>
</div>
""", unsafe_allow_html=True)

# --- í•µì‹¬ ê¸°ëŠ¥ ì†Œê°œ ---
st.markdown("""
<div class="feature-grid">
    <div class="feature-card">
        <div class="feature-icon">ğŸ”—</div>
        <div class="feature-title">ë‹¤ì¤‘ íŒŒì¼ ìë™ ë³‘í•©</div>
        <div class="feature-desc">ì—¬ëŸ¬ WOS íŒŒì¼ì„ í•œ ë²ˆì— ë³‘í•© ì²˜ë¦¬</div>
    </div>
    <div class="feature-card">
        <div class="feature-icon">ğŸš«</div>
        <div class="feature-title">ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì œê±°</div>
        <div class="feature-desc">UT ê¸°ì¤€ ìë™ ì¤‘ë³µ ë…¼ë¬¸ ê°ì§€ ë° ì œê±°</div>
    </div>
    <div class="feature-card">
        <div class="feature-icon">ğŸ¯</div>
        <div class="feature-title">ë°ì´í„° ë¶„ë¥˜</div>
        <div class="feature-desc">ëŒ€ê·œëª¨ ë°ì´í„° í¬ê´„ì  ë¶„ë¥˜ ë° ë¶„ì„</div>
    </div>
</div>
""", unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
st.markdown("""
<div class="section-header">
    <div class="section-title">ğŸ“ ë‹¤ì¤‘ WOS Plain Text íŒŒì¼ ì—…ë¡œë“œ</div>
    <div class="section-subtitle">500ê°œ ë‹¨ìœ„ë¡œ ë‚˜ë‰œ ì—¬ëŸ¬ WOS íŒŒì¼ì„ ëª¨ë‘ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”</div>
</div>
""", unsafe_allow_html=True)

st.markdown("""
<div class="upload-zone">
    <div style="font-size: 3rem; margin-bottom: 16px; color: #003875;">ğŸ“¤</div>
    <div style="padding: 0 20px;">
        <h3 style="color: #212529; margin-bottom: 8px;">WOS Plain Text íŒŒì¼ë“¤ì„ ëª¨ë‘ ì„ íƒí•˜ì„¸ìš”</h3>
        <p style="color: #6c757d; margin: 0;">Ctrl+í´ë¦­ìœ¼ë¡œ ì—¬ëŸ¬ íŒŒì¼ ë™ì‹œ ì„ íƒ ê°€ëŠ¥</p>
    </div>
</div>
""", unsafe_allow_html=True)

uploaded_files = st.file_uploader(
    "WOS Plain Text íŒŒì¼ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
    type=['txt'],
    accept_multiple_files=True,
    label_visibility="collapsed"
)

if uploaded_files:
    st.markdown(f"ğŸ“‹ **ì„ íƒëœ íŒŒì¼ ê°œìˆ˜:** {len(uploaded_files)}ê°œ")
    
    # í”„ë¡œê·¸ë ˆìŠ¤ ì¸ë””ì¼€ì´í„°
    st.markdown('<div class="progress-indicator"></div>', unsafe_allow_html=True)
    
    with st.spinner(f"ğŸ”„ {len(uploaded_files)}ê°œ WOS íŒŒì¼ ë³‘í•© ë° ë¶„ì„ ì¤‘..."):
        # íŒŒì¼ ë³‘í•©
        merged_df, file_status, duplicates_removed = load_and_merge_wos_files(uploaded_files)
        
        if merged_df is None:
            st.error("âš ï¸ ì²˜ë¦¬ ê°€ëŠ¥í•œ WOS Plain Text íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ë“¤ì´ Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ì •í’ˆ Plain Text íŒŒì¼ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            
            # íŒŒì¼ë³„ ìƒíƒœ í‘œì‹œ
            st.markdown("### ğŸ“„ íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ")
            for status in file_status:
                st.markdown(f"""
                <div class="file-status">
                    <strong>{status['filename']}</strong><br>
                    {status['message']}
                </div>
                """, unsafe_allow_html=True)
            st.stop()
        
        # ë…¼ë¬¸ ë¶„ë¥˜ ìˆ˜í–‰
        merged_df['Classification'] = merged_df.apply(classify_article, axis=1)

    # ì„±ê³µì ì¸ íŒŒì¼ ê°œìˆ˜ ê³„ì‚°
    successful_files = len([s for s in file_status if s['status'] == 'SUCCESS'])
    total_papers = len(merged_df)
    
    st.success(f"âœ… ë³‘í•© ì™„ë£Œ! {successful_files}ê°œ íŒŒì¼ì—ì„œ {total_papers:,}í¸ì˜ ë…¼ë¬¸ì„ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    
    # ì¤‘ë³µ ì œê±° ê²°ê³¼ í‘œì‹œ - ì‹¤ì œ ê²°ê³¼ë§Œ
    if duplicates_removed > 0:
        st.info(f"ğŸ”„ ì¤‘ë³µ ë…¼ë¬¸ {duplicates_removed}í¸ì´ ìë™ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤. (ì›ë³¸ ì´ {total_papers + duplicates_removed:,}í¸ â†’ ì •ì œ í›„ {total_papers:,}í¸)")
    else:
        st.info("âœ… ì¤‘ë³µ ë…¼ë¬¸ ì—†ìŒ - ëª¨ë“  ë…¼ë¬¸ì´ ê³ ìœ í•œ ë°ì´í„°ì…ë‹ˆë‹¤.")

    # --- íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ“„ íŒŒì¼ë³„ ì²˜ë¦¬ ìƒíƒœ</div>
        <div class="section-subtitle">ì—…ë¡œë“œëœ ê° íŒŒì¼ì˜ ì²˜ë¦¬ ê²°ê³¼</div>
    </div>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ğŸ“‹ íŒŒì¼ë³„ ìƒì„¸ ìƒíƒœ</div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns([0.6, 0.4])
    
    with col1:        
        for status in file_status:
            color = "#28a745" if status['status'] == 'SUCCESS' else "#dc3545"
            icon = "âœ…" if status['status'] == 'SUCCESS' else "âŒ"
            
            st.markdown(f"""
            <div style="margin: 8px 0; padding: 12px; background: #f8f9fa; border-left: 4px solid {color}; border-radius: 4px;">
                <strong>{icon} {status['filename']}</strong><br>
                <small style="color: #6c757d;">{status['message']}</small>
                {f" | ì¸ì½”ë”©: {status['encoding']}" if status['encoding'] != 'N/A' else ""}
            </div>
            """, unsafe_allow_html=True)
    
    with col2:
        # íŒŒì¼ ì²˜ë¦¬ í†µê³„
        success_count = len([s for s in file_status if s['status'] == 'SUCCESS'])
        error_count = len([s for s in file_status if s['status'] == 'ERROR'])
        
        st.markdown(f"""
        <div class="metric-card" style="background: #f8f9fa;">
            <div class="metric-icon">âœ…</div>
            <div class="metric-value">{success_count}</div>
            <div class="metric-label">ì„±ê³µí•œ íŒŒì¼</div>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown(f"""
        <div class="metric-card" style="background: #f8f9fa;">
            <div class="metric-icon">âŒ</div>
            <div class="metric-value">{error_count}</div>
            <div class="metric-label">ì‹¤íŒ¨í•œ íŒŒì¼</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("</div>", unsafe_allow_html=True)

    # --- ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ê²°ê³¼ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ” ë³‘í•© ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨</div>
        <div class="section-subtitle">ë³‘í•©ëœ WOS ë°ì´í„°ì˜ í’ˆì§ˆê³¼ SCIMAT í˜¸í™˜ì„± ê²€ì¦</div>
    </div>
    """, unsafe_allow_html=True)

    with st.spinner("ğŸ” ë³‘í•© ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì¤‘..."):
        issues, recommendations = diagnose_merged_quality(merged_df, successful_files, duplicates_removed)

    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ğŸ” ë³‘í•© ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ê²°ê³¼</div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown('<h5 style="color: #dc3545; margin-bottom: 12px;">ğŸš¨ ë°œê²¬ëœ ë¬¸ì œì </h5>', unsafe_allow_html=True)
        
        if issues:
            for issue in issues:
                st.markdown(f"- {issue}")
        else:
            st.markdown("âœ… **ë¬¸ì œì  ì—†ìŒ** - ë³‘í•© ë°ì´í„° í’ˆì§ˆ ìš°ìˆ˜")
    
    with col2:
        st.markdown('<h5 style="color: #28a745; margin-bottom: 12px;">ğŸ’¡ ë³‘í•© ê²°ê³¼</h5>', unsafe_allow_html=True)
        
        if recommendations:
            for rec in recommendations:
                st.markdown(f"- {rec}")
        else:
            st.markdown("ğŸ¯ **ìµœì  ìƒíƒœ** - SCIMAT ì™„ë²½ í˜¸í™˜")
    
    st.markdown("</div>", unsafe_allow_html=True)

    # ë³‘í•© ì„±ê³µ ì•Œë¦¼
    st.markdown("""
    <div class="success-panel">
        <h4 style="color: #155724; margin-bottom: 16px;">ğŸ¯ ë‹¤ì¤‘ íŒŒì¼ ë³‘í•© ì„±ê³µ!</h4>
        <p style="color: #155724; margin: 4px 0;">ì—¬ëŸ¬ WOS Plain Text íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ í•˜ë‚˜ë¡œ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        <p style="color: #155724; margin: 4px 0;"><strong>ì¤‘ë³µ ì œê±°:</strong> ë™ì¼í•œ ë…¼ë¬¸ì€ ìë™ìœ¼ë¡œ ì œê±°ë˜ì–´ ì •í™•í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.</p>
        <p style="color: #155724; margin: 4px 0;"><strong>SCIMAT í˜¸í™˜ì„±:</strong> ë³‘í•©ëœ íŒŒì¼ì€ SCIMATì—ì„œ 100% ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.</p>
    </div>
    """, unsafe_allow_html=True)

    # --- ë¶„ì„ ê²°ê³¼ ìš”ì•½ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ“ˆ ë³‘í•© ë°ì´í„° ë¶„ì„ ê²°ê³¼</div>
        <div class="section-subtitle">ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ ë¶„ë¥˜ ê²°ê³¼</div>
    </div>
    """, unsafe_allow_html=True)

    # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
    col1, col2, col3, col4 = st.columns(4)
    
    classification_counts = merged_df['Classification'].value_counts()
    total_papers = len(merged_df)
    include_papers = len(merged_df[merged_df['Classification'].str.contains('Include', na=False)])
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“‹</div>
            <div class="metric-value">{total_papers:,}</div>
            <div class="metric-label">Total Papers</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">âœ…</div>
            <div class="metric-value">{include_papers:,}</div>
            <div class="metric-label">Included Studies</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        processing_rate = (include_papers / total_papers * 100) if total_papers > 0 else 0
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“Š</div>
            <div class="metric-value">{processing_rate:.1f}%</div>
            <div class="metric-label">Inclusion Rate</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ”—</div>
            <div class="metric-value" style="font-size: 1.8rem;">{successful_files}ê°œ</div>
            <div class="metric-label">Merged Files</div>
        </div>
        """, unsafe_allow_html=True)

    # --- ë…¼ë¬¸ ë¶„ë¥˜ í˜„í™© ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">Research Classification Distribution</div>
    """, unsafe_allow_html=True)

    classification_counts_df = merged_df['Classification'].value_counts().reset_index()
    classification_counts_df.columns = ['ë¶„ë¥˜', 'ë…¼ë¬¸ ìˆ˜']

    col1, col2 = st.columns([0.4, 0.6])
    with col1:
        st.dataframe(classification_counts_df, use_container_width=True, hide_index=True)

    with col2:
        # ë„ë„› ì°¨íŠ¸ - í¬ê¸° í™•ëŒ€
        selection = alt.selection_point(fields=['ë¶„ë¥˜'], on='mouseover', nearest=True)

        base = alt.Chart(classification_counts_df).encode(
            theta=alt.Theta(field="ë…¼ë¬¸ ìˆ˜", type="quantitative", stack=True),
            color=alt.Color(field="ë¶„ë¥˜", type="nominal", title="Classification",
                           scale=alt.Scale(range=['#003875', '#0056b3', '#17a2b8', '#28a745', '#ffc107', '#dc3545']),
                           legend=alt.Legend(orient="right", titleColor="#212529", labelColor="#495057")),
            opacity=alt.condition(selection, alt.value(1), alt.value(0.8))
        ).add_params(selection)

        pie = base.mark_arc(outerRadius=150, innerRadius=90)
        text_total = alt.Chart(pd.DataFrame([{'value': f'{total_papers}'}])).mark_text(
            align='center', baseline='middle', fontSize=45, fontWeight='bold', color='#003875'
        ).encode(text='value:N')
        text_label = alt.Chart(pd.DataFrame([{'value': 'Total Papers'}])).mark_text(
            align='center', baseline='middle', fontSize=16, dy=30, color='#495057'
        ).encode(text='value:N')

        chart = (pie + text_total + text_label).properties(
            width=350, height=350
        ).configure_view(strokeWidth=0)
        st.altair_chart(chart, use_container_width=True)

    st.markdown("</div>", unsafe_allow_html=True)

    # --- ë¶„ë¥˜ ìƒì„¸ ê²°ê³¼ ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ë¶„ë¥˜ë³„ ìƒì„¸ ë¶„í¬</div>
    """, unsafe_allow_html=True)
    
    # ë¶„ë¥˜ë³„ ìƒì„¸ í†µê³„ - í°íŠ¸ í¬ê¸° ì¦ê°€
    for classification in merged_df['Classification'].unique():
        count = len(merged_df[merged_df['Classification'] == classification])
        percentage = (count / total_papers * 100)
        
        if classification.startswith('Include'):
            color = "#28a745"
            icon = "âœ…"
        elif classification.startswith('Review'):
            color = "#ffc107"
            icon = "ğŸ“"
        else:
            color = "#dc3545"
            icon = "âŒ"
        
        st.markdown(f"""
        <div style="margin: 12px 0; padding: 16px; background: white; border-left: 4px solid {color}; border-radius: 4px; font-size: 1.1rem;">
            <strong>{icon} {classification}:</strong> {count:,}í¸ ({percentage:.1f}%)
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("</div>", unsafe_allow_html=True)

    # --- ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ ---
    if 'PY' in merged_df.columns:
        st.markdown("""
        <div class="chart-container">
            <div class="chart-title">29ë…„ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ ì§„í™” ë™í–¥ (1996-2024)</div>
        """, unsafe_allow_html=True)
        
        df_trend = merged_df.copy()
        df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
        df_trend.dropna(subset=['PY'], inplace=True)
        df_trend['PY'] = df_trend['PY'].astype(int)
        
        yearly_counts = df_trend['PY'].value_counts().reset_index()
        yearly_counts.columns = ['Year', 'Count']
        yearly_counts = yearly_counts[yearly_counts['Year'] <= 2025].sort_values('Year')

        if len(yearly_counts) > 0:
            line_chart = alt.Chart(yearly_counts).mark_line(
                point={'size': 80, 'filled': True}, strokeWidth=3, color='#003875'
            ).encode(
                x=alt.X('Year:O', title='ë°œí–‰ ì—°ë„'),
                y=alt.Y('Count:Q', title='ë…¼ë¬¸ ìˆ˜'),
                tooltip=['Year', 'Count']
            ).properties(height=300)
            
            st.altair_chart(line_chart, use_container_width=True)
        
        st.markdown("</div>", unsafe_allow_html=True)

    # --- í‚¤ì›Œë“œ ìƒ˜í”Œ í™•ì¸ ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">ë³‘í•© ë°ì´í„° í‚¤ì›Œë“œ í’ˆì§ˆ í™•ì¸</div>
    """, unsafe_allow_html=True)
    
    sample_data = []
    sample_rows = merged_df[merged_df['Classification'].str.contains('Include', na=False)].head(3)
    
    for idx, row in sample_rows.iterrows():
        title = str(row.get('TI', 'N/A'))[:80] + "..." if len(str(row.get('TI', 'N/A'))) > 80 else str(row.get('TI', 'N/A'))
        de_keywords = str(row.get('DE', 'N/A')) if pd.notna(row.get('DE')) else 'N/A'
        id_keywords = str(row.get('ID', 'N/A')) if pd.notna(row.get('ID')) else 'N/A'
        
        # í‚¤ì›Œë“œ ê°œìˆ˜ ê³„ì‚°
        de_count = len([k.strip() for k in de_keywords.split(';') if k.strip()]) if de_keywords != 'N/A' else 0
        id_count = len([k.strip() for k in id_keywords.split(';') if k.strip()]) if id_keywords != 'N/A' else 0
        
        sample_data.append({
            'ë…¼ë¬¸ ì œëª©': title,
            'DE í‚¤ì›Œë“œ': de_keywords[:100] + "..." if len(de_keywords) > 100 else de_keywords,
            'ID í‚¤ì›Œë“œ': id_keywords[:100] + "..." if len(id_keywords) > 100 else id_keywords,
            'DE ê°œìˆ˜': de_count,
            'ID ê°œìˆ˜': id_count
        })
    
    if sample_data:
        sample_df = pd.DataFrame(sample_data)
        st.dataframe(sample_df, use_container_width=True, hide_index=True)
        
        # í‚¤ì›Œë“œ í’ˆì§ˆ í‰ê°€
        avg_de = sum([d['DE ê°œìˆ˜'] for d in sample_data]) / len(sample_data) if sample_data else 0
        avg_id = sum([d['ID ê°œìˆ˜'] for d in sample_data]) / len(sample_data) if sample_data else 0
        
        if avg_de >= 3 and avg_id >= 3:
            st.success("âœ… í‚¤ì›Œë“œ í’ˆì§ˆ ìš°ìˆ˜ - SCIMATì—ì„œ ì›í™œí•œ ê·¸ë£¹í•‘ ì˜ˆìƒ")
        elif avg_de >= 2 or avg_id >= 2:
            st.warning("âš ï¸ í‚¤ì›Œë“œ í’ˆì§ˆ ë³´í†µ - SCIMATì—ì„œ ì¼ë¶€ ì œí•œ ê°€ëŠ¥")
        else:
            st.error("âŒ í‚¤ì›Œë“œ í’ˆì§ˆ ë¶€ì¡± - ì›ë³¸ WOS ë‹¤ìš´ë¡œë“œ ì„¤ì • í™•ì¸ í•„ìš”")

    st.markdown("</div>", unsafe_allow_html=True)

    # ìµœì¢… ë°ì´í„°ì…‹ ì¤€ë¹„ (ì œì™¸ëœ ë…¼ë¬¸ë§Œ ë¹¼ê³ )
    df_final = merged_df[~merged_df['Classification'].str.contains('Exclude', na=False)].copy()
    
    # Classification ì»¬ëŸ¼ë§Œ ì œê±° (ì›ë³¸ WOS í˜•ì‹ ìœ ì§€)
    df_final_output = df_final.drop(columns=['Classification'], errors='ignore')

    # ìµœì¢… í†µê³„
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“‹</div>
            <div class="metric-value">{len(df_final_output):,}</div>
            <div class="metric-label">ìµœì¢… ë¶„ì„ ëŒ€ìƒ<br><small style="color: #6c757d;">(Exclude ì œì™¸)</small></div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        include_count = len(merged_df[merged_df['Classification'].str.contains('Include', na=False)])
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">âœ…</div>
            <div class="metric-value">{include_count:,}</div>
            <div class="metric-label">í•µì‹¬ í¬í•¨ ì—°êµ¬</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        review_count = len(merged_df[merged_df['Classification'].str.contains('Review', na=False)])
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“</div>
            <div class="metric-value">{review_count:,}</div>
            <div class="metric-label">ê²€í†  ëŒ€ìƒ</div>
        </div>
        """, unsafe_allow_html=True)

    # --- ë¶„ë¥˜ë³„ ë…¼ë¬¸ ìƒì„¸ ëª©ë¡ (Reviewì™€ Exclude ëª¨ë‘ í† ê¸€) ---
    # Review ë¶„ë¥˜ ë…¼ë¬¸ë“¤ í† ê¸€
    review_papers = merged_df[merged_df['Classification'].str.contains('Review', na=False)]
    
    if len(review_papers) > 0:
        with st.expander(f"ğŸ“ Review (ê²€í†  í•„ìš”) - ë…¼ë¬¸ ëª©ë¡ ({len(review_papers)}í¸)", expanded=False):
            st.markdown("""
            <div style="background: #fff3cd; padding: 12px; border-radius: 8px; margin-bottom: 16px;">
                <strong>ğŸ“‹ ê²€í†  ì•ˆë‚´:</strong> ì•„ë˜ ë…¼ë¬¸ë“¤ì€ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ì™€ì˜ ê´€ë ¨ì„±ì„ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•œ ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤.
                ì œëª©ê³¼ ì¶œíŒ ì •ë³´ë¥¼ í™•ì¸í•˜ì—¬ ì—°êµ¬ ë²”ìœ„ì— í¬í•¨í• ì§€ ê²°ì •í•˜ì„¸ìš”.
            </div>
            """, unsafe_allow_html=True)
            
            # Review ë…¼ë¬¸ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            review_excel_data = []
            for idx, (_, paper) in enumerate(review_papers.iterrows(), 1):
                review_excel_data.append({
                    'ë²ˆí˜¸': idx,
                    'ë…¼ë¬¸ ì œëª©': str(paper.get('TI', 'N/A')),
                    'ì¶œíŒì—°ë„': str(paper.get('PY', 'N/A')),
                    'ì €ë„ëª…': str(paper.get('SO', 'N/A')),
                    'ì €ì': str(paper.get('AU', 'N/A')),
                    'ë¶„ë¥˜': str(paper.get('Classification', 'N/A')),
                    'ì €ì í‚¤ì›Œë“œ': str(paper.get('DE', 'N/A')),
                    'WOS í‚¤ì›Œë“œ': str(paper.get('ID', 'N/A')),
                    'ì´ˆë¡': str(paper.get('AB', 'N/A'))
                })
            
            review_excel_df = pd.DataFrame(review_excel_data)
            
            # ì—‘ì…€ íŒŒì¼ ìƒì„±
            excel_buffer = io.BytesIO()
            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                review_excel_df.to_excel(writer, sheet_name='Review_Papers', index=False)
            excel_data = excel_buffer.getvalue()
            
            st.download_button(
                label="ğŸ“Š ê²€í†  ë…¼ë¬¸ ëª©ë¡ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
                data=excel_data,
                file_name=f"review_papers_list_{len(review_papers)}í¸.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                type="secondary",
                use_container_width=True
            )
            
            st.markdown("<br>", unsafe_allow_html=True)
            
            for idx, (_, paper) in enumerate(review_papers.iterrows(), 1):
                title = str(paper.get('TI', 'N/A'))
                year = str(paper.get('PY', 'N/A'))
                source = str(paper.get('SO', 'N/A'))
                classification = str(paper.get('Classification', 'N/A'))
                
                # ë¶„ë¥˜ë³„ ìƒ‰ìƒ ì„¤ì •
                if 'ë¶ˆí™•ì‹¤' in classification:
                    badge_color = "#6c757d"
                    badge_text = "ë¶„ë¥˜ ë¶ˆí™•ì‹¤"
                elif 'ì†Œì…œë¯¸ë””ì–´' in classification:
                    badge_color = "#17a2b8"
                    badge_text = "ì†Œì…œë¯¸ë””ì–´"
                elif 'ë¹„ì¦ˆë‹ˆìŠ¤' in classification:
                    badge_color = "#28a745"
                    badge_text = "ë¹„ì¦ˆë‹ˆìŠ¤"
                elif 'ê¸°ìˆ ì ' in classification:
                    badge_color = "#fd7e14"
                    badge_text = "ê¸°ìˆ ì "
                elif 'êµìœ¡' in classification:
                    badge_color = "#6f42c1"
                    badge_text = "êµìœ¡"
                else:
                    badge_color = "#ffc107"
                    badge_text = "ê¸°íƒ€"
                
                st.markdown(f"""
                <div style="margin: 8px 0; padding: 12px; background: white; border-left: 3px solid #ffc107; border-radius: 4px; box-shadow: 0 1px 3px rgba(0,0,0,0.1);">
                    <div style="display: flex; align-items: center; margin-bottom: 6px;">
                        <span style="background: {badge_color}; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8rem; margin-right: 8px;">{badge_text}</span>
                        <span style="color: #6c757d; font-size: 0.9rem;">#{idx}</span>
                    </div>
                    <div style="font-weight: 600; color: #212529; margin-bottom: 4px; line-height: 1.4;">
                        {title}
                    </div>
                    <div style="font-size: 0.9rem; color: #6c757d;">
                        <strong>ì—°ë„:</strong> {year} | <strong>ì €ë„:</strong> {source}
                    </div>
                </div>
                """, unsafe_allow_html=True)

    # Exclude ë¶„ë¥˜ ë…¼ë¬¸ë“¤ í† ê¸€ (ì‹ ê·œ ì¶”ê°€)
    exclude_papers = merged_df[merged_df['Classification'].str.contains('Exclude', na=False)]
    
    if len(exclude_papers) > 0:
        with st.expander(f"âŒ Exclude (ì œì™¸ ëŒ€ìƒ) - ë…¼ë¬¸ ëª©ë¡ ({len(exclude_papers)}í¸)", expanded=False):
            st.markdown("""
            <div style="background: #f8d7da; padding: 12px; border-radius: 8px; margin-bottom: 16px;">
                <strong>ğŸš« ì œì™¸ ì•ˆë‚´:</strong> ì•„ë˜ ë…¼ë¬¸ë“¤ì€ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë° ì—°êµ¬ì™€ ê´€ë ¨ì„±ì´ ë‚®ì•„ ìµœì¢… ë¶„ì„ì—ì„œ ì œì™¸ë˜ëŠ” ë…¼ë¬¸ë“¤ì…ë‹ˆë‹¤.
                ê¸°ìˆ ì ìœ¼ë¡œ ë¹„ê´€ë ¨ëœ ì—°êµ¬ë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
            </div>
            """, unsafe_allow_html=True)
            
            for idx, (_, paper) in enumerate(exclude_papers.iterrows(), 1):
                title = str(paper.get('TI', 'N/A'))
                year = str(paper.get('PY', 'N/A'))
                source = str(paper.get('SO', 'N/A'))
                
                st.markdown(f"""
                <div style="margin: 8px 0; padding: 12px; background: white; border-left: 3px solid #dc3545; border-radius: 4px; box-shadow: 0 1px 3px rgba(0,0,0,0.1);">
                    <div style="display: flex; align-items: center; margin-bottom: 6px;">
                        <span style="background: #dc3545; color: white; padding: 2px 8px; border-radius: 12px; font-size: 0.8rem; margin-right: 8px;">ê¸°ìˆ ì  ë¹„ê´€ë ¨</span>
                        <span style="color: #6c757d; font-size: 0.9rem;">#{idx}</span>
                    </div>
                    <div style="font-weight: 600; color: #212529; margin-bottom: 4px; line-height: 1.4;">
                        {title}
                    </div>
                    <div style="font-size: 0.9rem; color: #6c757d;">
                        <strong>ì—°ë„:</strong> {year} | <strong>ì €ë„:</strong> {source}
                    </div>
                </div>
                """, unsafe_allow_html=True)

    # ë³‘í•© ì„±ê³¼ ê°•ì¡° - ì‹¤ì œ ë°ì´í„° ê¸°ë°˜
    success_info = []
    success_info.append(f"<strong>íŒŒì¼ í†µí•©:</strong> {successful_files}ê°œì˜ WOS íŒŒì¼ì„ í•˜ë‚˜ë¡œ ë³‘í•©")
    
    if duplicates_removed > 0:
        success_info.append(f"<strong>ì¤‘ë³µ ì œê±°:</strong> {duplicates_removed}í¸ì˜ ì¤‘ë³µ ë…¼ë¬¸ ìë™ ê°ì§€ ë° ì œê±°")
    
    success_info.append(f"<strong>ìµœì¢… ê·œëª¨:</strong> {total_papers:,}í¸ì˜ ë…¼ë¬¸ìœ¼ë¡œ ëŒ€ê·œëª¨ ì—°êµ¬ ë¶„ì„ ê°€ëŠ¥")
    success_info.append("<strong>SCIMAT í˜¸í™˜:</strong> ì™„ë²½í•œ WOS Plain Text í˜•ì‹ìœ¼ë¡œ 100% í˜¸í™˜ì„± ë³´ì¥")
    
    success_content = "".join([f"<p style='color: #003875; margin: 4px 0;'>{info}</p>" for info in success_info])
    
    st.markdown(f"""
    <div class="info-panel">
        <h4 style="color: #003875; margin-bottom: 12px;">ğŸ† ë³‘í•© ì„±ê³¼</h4>
        {success_content}
    </div>
    """, unsafe_allow_html=True)

    # --- ìµœì¢… íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
    # SCIMAT í˜¸í™˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ - ê¸°ë³¸ íŒŒë€ìƒ‰ ë²„íŠ¼
    text_data = convert_to_scimat_wos_format(df_final_output)
    
    st.download_button(
        label="Download",
        data=text_data,
        file_name="live_streaming_merged_scimat_ready.txt",
        mime="text/plain",
        type="primary",
        use_container_width=True,
        key="download_final_file"
    )

# --- í•˜ë‹¨ ì—¬ë°± ë° ì¶”ê°€ ì •ë³´ ---
st.markdown("<br>", unsafe_allow_html=True)

# ë„ì›€ë§ ì„¹ì…˜
with st.expander("â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)"):
    st.markdown("""
    **Q: ì—¬ëŸ¬ WOS íŒŒì¼ì„ ì–´ë–»ê²Œ í•œ ë²ˆì— ì²˜ë¦¬í•˜ë‚˜ìš”?**
    A: WOSì—ì„œ ì—¬ëŸ¬ ë²ˆ Plain Text ë‹¤ìš´ë¡œë“œí•œ í›„, ëª¨ë“  .txt íŒŒì¼ì„ í•œ ë²ˆì— ì—…ë¡œë“œí•˜ë©´ ìë™ìœ¼ë¡œ ë³‘í•©ë©ë‹ˆë‹¤.
    
    **Q: ì¤‘ë³µëœ ë…¼ë¬¸ì´ ìˆì„ê¹Œë´ ê±±ì •ë©ë‹ˆë‹¤.**
    A: UT(Unique Article Identifier) ê¸°ì¤€ìœ¼ë¡œ ìë™ ì¤‘ë³µ ì œê±°ë˜ë©°, UTê°€ ì—†ìœ¼ë©´ ì œëª©+ì €ì ì¡°í•©ìœ¼ë¡œ ì¤‘ë³µì„ ê°ì§€í•©ë‹ˆë‹¤.
    
    **Q: WOSì—ì„œ ì–´ë–¤ ì„¤ì •ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•˜ë‚˜ìš”?**
    A: Export â†’ Record Content: "Full Record and Cited References", File Format: "Plain Text"ë¡œ ì„¤ì •í•˜ì„¸ìš”. ì¸ìš© ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ ì°¸ê³ ë¬¸í—Œ ì •ë³´ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
    
    **Q: SCIMATì—ì„œ í‚¤ì›Œë“œ ì •ë¦¬ë¥¼ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**
    A: Group set â†’ Word â†’ Find similar words by distances (Maximum distance: 1)ë¡œ ìœ ì‚¬ í‚¤ì›Œë“œë¥¼ ìë™ í†µí•©í•˜ê³ , Word Group manual setì—ì„œ ìˆ˜ë™ìœ¼ë¡œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ì„ ê·¸ë£¹í™”í•˜ì„¸ìš”.
    
    **Q: SCIMAT ë¶„ì„ ì„¤ì •ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**
    A: Unit of Analysis: "Author's words + Source's words", Network Type: "Co-occurrence", Normalization: "Equivalence Index", Clustering: "Simple Centers Algorithm" (Maximum network size: 50)ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    
    **Q: ë³‘í•©ëœ íŒŒì¼ì´ SCIMATì—ì„œ ì œëŒ€ë¡œ ë¡œë”©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**
    A: ì›ë³¸ WOS íŒŒì¼ë“¤ì´ 'FN Clarivate Analytics Web of Science'ë¡œ ì‹œì‘í•˜ëŠ” ì •í’ˆ Plain Text íŒŒì¼ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.
    
    **Q: SCIMATì—ì„œ PeriodëŠ” ì–´ë–»ê²Œ ì„¤ì •í•˜ë‚˜ìš”?**
    A: ì—°êµ¬ ë¶„ì•¼ì˜ ì§„í™” ë‹¨ê³„ë¥¼ ë°˜ì˜í•˜ì—¬ ì˜ë¯¸ ìˆê²Œ êµ¬ë¶„í•˜ë˜, ê° Periodë‹¹ ìµœì†Œ 50í¸ ì´ìƒì˜ ë…¼ë¬¸ì„ í¬í•¨í•˜ë„ë¡ ì„¤ì •í•˜ì„¸ìš”.
    
    **Q: ëª‡ ê°œì˜ íŒŒì¼ê¹Œì§€ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆë‚˜ìš”?**
    A: ê¸°ìˆ ì ìœ¼ë¡œëŠ” ì œí•œì´ ì—†ì§€ë§Œ, ì•ˆì •ì„±ì„ ìœ„í•´ 10ê°œ ì´í•˜ì˜ íŒŒì¼ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ë§¤ìš° í° ë°ì´í„°ì…‹ì˜ ê²½ìš° ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬í•˜ì„¸ìš”.
    """)

# SCIMAT ë¶„ì„ ê°€ì´ë“œ ì¶”ê°€
with st.expander("ğŸ“Š SCIMAT ì™„ë²½ ë¶„ì„ ê°€ì´ë“œ"):
    st.markdown("""
    ### ğŸ¯ SCIMAT ë¶„ì„ í•µì‹¬ ë‹¨ê³„
    
    **1ë‹¨ê³„: í”„ë¡œì íŠ¸ ìƒì„±**
    - SCIMAT ì‹¤í–‰ â†’ File â†’ New Project â†’ ê²½ë¡œ ì„¤ì •
    
    **2ë‹¨ê³„: ë°ì´í„° ë¡œë”©**
    - File â†’ Add Files â†’ "ISI WoS" ì„ íƒ â†’ ë‹¤ìš´ë¡œë“œí•œ .txt íŒŒì¼ ì„ íƒ
    
    **3ë‹¨ê³„: í‚¤ì›Œë“œ ì •ë¦¬ (ì¤‘ìš”!)**
    - **ìë™ í†µí•©**: Group set â†’ Word â†’ Find similar words by distances (ê±°ë¦¬: 1)
    - **ìˆ˜ë™ ê·¸ë£¹í™”**: Word Group manual setì—ì„œ ê´€ë ¨ í‚¤ì›Œë“œë“¤ ë¬¶ê¸°
    - **ëª©ì **: ë°ì´í„° í’ˆì§ˆ í–¥ìƒ, ì˜ë¯¸ ìˆëŠ” í´ëŸ¬ìŠ¤í„° í˜•ì„±
    
    **4ë‹¨ê³„: ì‹œê°„ êµ¬ê°„ ì„¤ì •**
    - Knowledge base â†’ Periods â†’ Periods manager
    - ì—°êµ¬ ë¶„ì•¼ ì§„í™” ë‹¨ê³„ë¥¼ ë°˜ì˜í•œ ì˜ë¯¸ ìˆëŠ” êµ¬ê°„ ì„¤ì •
    - ê° Periodë‹¹ ìµœì†Œ 50í¸ ì´ìƒ ë…¼ë¬¸ í¬í•¨ ê¶Œì¥
    
    **5ë‹¨ê³„: ë¶„ì„ ì„¤ì •**
    - **Unit of Analysis**: "Word Group" + "Author's words + Source's words"
    - **Data Reduction**: Minimum frequency: 2 (ë…¸ì´ì¦ˆ ì œê±°)
    - **Network Type**: "Co-occurrence" (í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ë¶„ì„)
    - **Normalization**: "Equivalence Index" (í‘œì¤€ ì •ê·œí™” ë°©ë²•)
    - **Clustering**: "Simple Centers Algorithm" (í•´ì„ ìš©ì´, ì•ˆì •ì )
    - **Maximum network size**: 50 (ì‹œê°ì  ë¶„ì„ ê°€ëŠ¥í•œ ì ì • í¬ê¸°)
    - **Document Mapper**: "Core Mapper" (í•µì‹¬ ë…¼ë¬¸ ì‹ë³„)
    - **Performance Measures**: G-index, Sum Citations ëª¨ë‘ ì„ íƒ
    - **Evolution Map**: "Jaccard Index" (ì‹œê¸°ê°„ ì£¼ì œ ì—°ì†ì„± ì¸¡ì •)
    
    **6ë‹¨ê³„: ê²°ê³¼ í•´ì„**
    - **ì „ëµì  ë‹¤ì´ì–´ê·¸ë¨ 4ì‚¬ë¶„ë©´**:
      - ìš°ìƒë‹¨: Motor Themes (í•µì‹¬ ì£¼ì œ) - ì¤‘ì‹¬ì„±â†‘, ë°€ë„â†‘
      - ì¢Œìƒë‹¨: Specialized Themes (ì „ë¬¸í™”ëœ ì£¼ì œ) - ì¤‘ì‹¬ì„±â†“, ë°€ë„â†‘  
      - ì¢Œí•˜ë‹¨: Emerging/Declining Themes (ì‹ í¥/ì‡ í‡´ ì£¼ì œ) - ì¤‘ì‹¬ì„±â†“, ë°€ë„â†“
      - ìš°í•˜ë‹¨: Basic Themes (ê¸°ì´ˆ ì£¼ì œ) - ì¤‘ì‹¬ì„±â†‘, ë°€ë„â†“
    
    ### ğŸ’¡ ì„±ê³µì ì¸ ë¶„ì„ì„ ìœ„í•œ íŒ
    - **í‚¤ì›Œë“œ ì •ë¦¬ì— ì¶©ë¶„í•œ ì‹œê°„ íˆ¬ì** (ë¶„ì„ í’ˆì§ˆì˜ í•µì‹¬!)
    - **Period êµ¬ë¶„**: ë„ˆë¬´ ì„¸ë¶„í™”í•˜ì§€ ë§ê³  ì˜ë¯¸ìˆëŠ” êµ¬ê°„ìœ¼ë¡œ
    - **ê° Periodë‹¹ ìµœì†Œ 50í¸** ì´ìƒìœ¼ë¡œ í†µê³„ì  ì˜ë¯¸ í™•ë³´
    - **ì„¤ì •ê°’ ì¡°ì •**: ë¶„ì•¼ íŠ¹ì„±ì— ë”°ë¼ Maximum network size ì¡°ì • ê°€ëŠ¥ (30-100)
    
    ### ğŸ”§ ë¬¸ì œ í•´ê²°
    - **ì„í¬íŠ¸ ì•ˆë¨**: WOS íŒŒì¼ì´ Plain Textì¸ì§€ í™•ì¸
    - **ë¶„ì„ ì¤‘ë‹¨**: Java ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì¬ì‹œì‘
    - **í•œê¸€ ê¹¨ì§**: íŒŒì¼ ì¸ì½”ë”© UTF-8ë¡œ ë³€ê²½
    """)

st.markdown("<br><br>", unsafe_allow_html=True)
