import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt
import io

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Prep | Professional Edition",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ (ResearchGate ìŠ¤íƒ€ì¼) ---
st.markdown("""
<style>
    .main-container {
        background: #f8f9fa;
        min-height: 100vh;
    }
    
    .metric-card {
        background: white;
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        margin-bottom: 16px;
        transition: all 0.3s ease;
    }
    
    .metric-card:hover {
        box-shadow: 0 4px 20px rgba(0,56,117,0.15);
        border-color: #003875;
    }
    
    .metric-value {
        font-size: 2.8rem;
        font-weight: 700;
        color: #003875;
        margin: 0;
        line-height: 1;
    }
    
    .metric-label {
        font-size: 1rem;
        color: #6c757d;
        margin: 8px 0 0 0;
        font-weight: 500;
    }
    
    .metric-icon {
        background: linear-gradient(135deg, #003875, #0056b3);
        color: white;
        width: 48px;
        height: 48px;
        border-radius: 12px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.5rem;
        margin-bottom: 16px;
    }
    
    .chart-container {
        background: white;
        border-radius: 12px;
        padding: 24px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        margin: 16px 0;
    }
    
    .chart-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #212529;
        margin-bottom: 16px;
        border-bottom: 2px solid #003875;
        padding-bottom: 8px;
    }
    
    .section-header {
        background: linear-gradient(135deg, #003875, #0056b3);
        color: white;
        padding: 20px 24px;
        border-radius: 12px;
        margin: 24px 0 16px 0;
        box-shadow: 0 4px 16px rgba(0,56,117,0.2);
    }
    
    .section-title {
        font-size: 1.5rem;
        font-weight: 600;
        margin: 0;
    }
    
    .section-subtitle {
        font-size: 1rem;
        opacity: 0.9;
        margin: 4px 0 0 0;
    }
    
    .info-panel {
        background: #e8f0fe;
        border: 1px solid #003875;
        border-radius: 8px;
        padding: 16px;
        margin: 16px 0;
    }
    
    .feature-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: 20px;
        margin: 24px 0;
    }
    
    .feature-card {
        background: white;
        border-radius: 12px;
        padding: 24px;
        text-align: center;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
        transition: all 0.3s ease;
    }
    
    .feature-card:hover {
        transform: translateY(-4px);
        box-shadow: 0 8px 24px rgba(0,56,117,0.15);
        border-color: #003875;
    }
    
    .feature-icon {
        font-size: 3rem;
        margin-bottom: 16px;
        background: linear-gradient(135deg, #003875, #0056b3);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
    }
    
    .feature-title {
        font-size: 1.2rem;
        font-weight: 600;
        color: #212529;
        margin-bottom: 8px;
    }
    
    .feature-desc {
        font-size: 0.95rem;
        color: #6c757d;
        line-height: 1.5;
    }
    
    .upload-zone {
        background: white;
        border: 2px dashed #003875;
        border-radius: 12px;
        padding: 40px;
        text-align: center;
        margin: 20px 0;
        transition: all 0.3s ease;
    }
    
    .upload-zone:hover {
        background: #f8f9fa;
        border-color: #0056b3;
    }
    
    .progress-indicator {
        background: linear-gradient(90deg, #003875, #0056b3);
        height: 4px;
        border-radius: 2px;
        margin: 16px 0;
        animation: pulse 2s infinite;
    }
    
    @keyframes pulse {
        0%, 100% { opacity: 1; }
        50% { opacity: 0.7; }
    }
    
    .stMetric {
        background: white;
        border-radius: 12px;
        padding: 20px;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
        border: 1px solid #e9ecef;
    }
    
    .stDataFrame {
        border-radius: 12px;
        overflow: hidden;
        box-shadow: 0 2px 12px rgba(0,0,0,0.08);
    }
    
    .comparison-panel {
        background: linear-gradient(135deg, #f8f9fa, #ffffff);
        border: 1px solid #dee2e6;
        border-radius: 12px;
        padding: 20px;
        margin: 16px 0;
    }
</style>
""", unsafe_allow_html=True)

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ìºì‹œ ìœ ì§€) ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- í‚¤ì›Œë“œ ì •ê·œí™” ì‚¬ì „ (ì—­ë°©í–¥ ë§¤í•‘ìœ¼ë¡œ ìµœì í™”) ---
def build_normalization_map():
    """ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì—­ë°©í–¥ ì •ê·œí™” ì‚¬ì „ ìƒì„±"""
    base_map = {
        # AI/ML ê´€ë ¨ (ì„¸ë¶„í™” ìœ ì§€)
        "machine learning": ["machine-learning", "machine_learning", "ml", "machinelearning"],
        "artificial intelligence": ["ai", "artificial-intelligence", "artificial_intelligence", "artificialintelligence"],
        "deep learning": ["deep-learning", "deep_learning", "deep neural networks", "deep neural network", "dnn", "deeplearning"],
        "neural networks": ["neural-networks", "neural_networks", "neuralnetworks", "neural network", "nn"],
        "natural language processing": ["nlp", "natural-language-processing", "natural_language_processing"],
        "computer vision": ["computer-vision", "computer_vision", "computervision", "cv"],
        "reinforcement learning": ["reinforcement-learning", "reinforcement_learning", "rl"],

        # ìŠ¤íŠ¸ë¦¬ë°/ë¯¸ë””ì–´ ê´€ë ¨
        "live streaming": ["live-streaming", "live_streaming", "livestreaming", "real time streaming"],
        "video streaming": ["video-streaming", "video_streaming", "videostreaming"],
        "social media": ["social-media", "social_media", "socialmedia"],
        "user experience": ["user-experience", "user_experience", "ux", "userexperience"],
        "user behavior": ["user-behavior", "user_behavior", "userbehavior"],
        "content creation": ["content-creation", "content_creation", "contentcreation"],
        "digital marketing": ["digital-marketing", "digital_marketing", "digitalmarketing"],
        "e commerce": ["ecommerce", "e-commerce", "e_commerce", "electronic commerce"],

        # ì—°êµ¬ë°©ë²•ë¡  ê´€ë ¨
        "data mining": ["data-mining", "data_mining", "datamining"],
        "big data": ["big-data", "big_data", "bigdata"],
        "data analysis": ["data-analysis", "data_analysis", "dataanalysis"],
        "sentiment analysis": ["sentiment-analysis", "sentiment_analysis", "sentimentanalysis"],
        "statistical analysis": ["statistical-analysis", "statistical_analysis", "statisticalanalysis"],
        "structural equation modeling": ["sem", "pls-sem", "pls sem", "structural equation model"],

        # ê¸°ìˆ  ê´€ë ¨
        "cloud computing": ["cloud-computing", "cloud_computing", "cloudcomputing"],
        "internet of things": ["iot", "internet-of-things", "internet_of_things"],
        "mobile applications": ["mobile-applications", "mobile_applications", "mobile apps", "mobile app"],
        "web development": ["web-development", "web_development", "webdevelopment"],
        "software engineering": ["software-engineering", "software_engineering", "softwareengineering"]
    }

    # ì—­ë°©í–¥ ë§¤í•‘ ìƒì„± (variation -> standard_form)
    reverse_map = {}
    for standard_form, variations in base_map.items():
        for variation in variations:
            reverse_map[variation.lower()] = standard_form
        # í‘œì¤€ í˜•íƒœë„ ìê¸° ìì‹ ìœ¼ë¡œ ë§¤í•‘
        reverse_map[standard_form.lower()] = standard_form

    return reverse_map

NORMALIZATION_MAP = build_normalization_map()

def normalize_keyword_phrase(phrase):
    """êµ¬ë¬¸ ë‹¨ìœ„ í‚¤ì›Œë“œ ì •ê·œí™”"""
    phrase_lower = phrase.lower().strip()
    return NORMALIZATION_MAP.get(phrase_lower, phrase_lower)

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (WOS ê¸°ë³¸ í˜•ì‹ ì§€ì›) ---
def load_data(uploaded_file):
    file_name = uploaded_file.name.lower()
    
    # ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ (WOS ê¸°ë³¸ .xls í¬í•¨)
    if file_name.endswith(('.xlsx', '.xls')):
        try:
            # ì—¬ëŸ¬ ì—”ì§„ìœ¼ë¡œ ì‹œë„
            engines_to_try = ['openpyxl', 'xlrd']
            
            for engine in engines_to_try:
                try:
                    if engine == 'openpyxl' and file_name.endswith('.xls'):
                        continue  # openpyxlì€ .xlsë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
                    
                    df = pd.read_excel(uploaded_file, engine=engine, sheet_name=0)
                    if df.shape[1] > 1:
                        return df
                except ImportError:
                    continue  # ì—”ì§„ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° ë‹¤ìŒ ì—”ì§„ ì‹œë„
                except Exception:
                    continue  # ë‹¤ë¥¸ ì˜¤ë¥˜ ë°œìƒì‹œ ë‹¤ìŒ ì—”ì§„ ì‹œë„
            
            # ëª¨ë“  ì—”ì§„ ì‹¤íŒ¨ì‹œ ì•ˆë‚´ ë©”ì‹œì§€
            st.error("âš ï¸ Excel íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.markdown("""
            **í•´ê²° ë°©ë²•:**
            1. **Excelì—ì„œ CSVë¡œ ë³€í™˜**: íŒŒì¼ â†’ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ì €ì¥ â†’ CSV í˜•ì‹ ì„ íƒ
            2. **WOSì—ì„œ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ**: 'Tab-delimited (Win)' ë˜ëŠ” 'Plain Text' í˜•ì‹ ì„ íƒ
            """)
            return None
            
        except Exception as e:
            st.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return None
    
    # CSV/TXT íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
    file_bytes = uploaded_file.getvalue()
    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'cp949']

    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # íƒ­ êµ¬ë¶„ì ìš°ì„  ì‹œë„
            df = pd.read_csv(io.StringIO(file_content), sep='\t', lineterminator='\n')
            if df.shape[1] > 1: return df
        except Exception:
            continue

    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # ì½¤ë§ˆ êµ¬ë¶„ì ì‹œë„
            df = pd.read_csv(io.StringIO(file_content))
            if df.shape[1] > 1: return df
        except Exception:
            continue

    return None

# --- SciMAT ê¸°ë°˜ í–¥ìƒëœ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ (ë” ì—„ê²©í•œ ë²„ì „) ---
def classify_article(row):
    """SciMAT ë°©ë²•ë¡ ì„ ì ìš©í•œ ë” ì—„ê²©í•œ ë…¼ë¬¸ ë¶„ë¥˜"""
    
    # í•µì‹¬ ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë° + ìƒê±°ë˜/ë§ˆì¼€íŒ… í‚¤ì›Œë“œ
    livestreaming_commerce_keywords = [
        'live streaming commerce', 'social commerce', 'livestreaming commerce',
        'purchase intention', 'customer engagement', 'consumer behavior',
        'influencer marketing', 'brand engagement', 'online shopping',
        'digital marketing', 'e-commerce', 'viewer engagement'
    ]
    
    # ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë° í‚¤ì›Œë“œ (ë‹¨ë…ìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„)
    livestreaming_keywords = [
        'live streaming', 'livestreaming', 'live-streaming', 'live broadcast'
    ]
    
    # ì‚¬ìš©ì/í–‰ë™ ê´€ë ¨ í‚¤ì›Œë“œ
    user_behavior_keywords = [
        'user', 'viewer', 'audience', 'consumer', 'customer', 'participant',
        'behavior', 'experience', 'engagement', 'interaction', 'motivation',
        'psychology', 'social', 'community', 'marketing', 'business'
    ]
    
    # ê¸°ìˆ ì  ì œì™¸ í‚¤ì›Œë“œ (ë” ê°•í™”)
    strong_tech_keywords = [
        'protocol', 'network coding', 'tcp', 'udp', 'bandwidth', 'throughput',
        'p2p', 'peer-to-peer', 'packet', 'routing', 'algorithm', 'optimization',
        'qoe', 'quality of experience', 'bitrate', 'codec', 'encoding'
    ]
    
    # ì‹œìŠ¤í…œ/í•˜ë“œì›¨ì–´ í‚¤ì›Œë“œ
    system_keywords = [
        'system', 'architecture', 'platform', 'framework', 'implementation',
        'performance', 'latency', 'delay', 'synchronization', 'scalable',
        'cloud', 'server', 'network', 'wireless', 'mobile'
    ]
    
    # ì˜ë£Œ/êµìœ¡ ê¸°ìˆ  í‚¤ì›Œë“œ
    medical_tech_keywords = [
        'medical', 'education', 'surgical', 'clinical', 'laboratory',
        'wearable', 'augmented reality', 'virtual reality', 'covid-19'
    ]
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì¶”ì¶œ
    title = str(row.get('TI', '')).lower()
    source_title = str(row.get('SO', '')).lower()
    author_keywords = str(row.get('DE', '')).lower()
    keywords_plus = str(row.get('ID', '')).lower()
    abstract = str(row.get('AB', '')).lower()
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    
    # 1ë‹¨ê³„: ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë° + ìƒê±°ë˜ ì§ì ‘ ì¡°í•© í™•ì¸
    commerce_found = any(keyword in full_text for keyword in livestreaming_commerce_keywords)
    if commerce_found:
        return 'Include (ê´€ë ¨ì—°êµ¬)'
    
    # 2ë‹¨ê³„: ê°•í•œ ê¸°ìˆ ì  ì œì™¸ í™•ì¸
    strong_tech_count = sum(1 for keyword in strong_tech_keywords if keyword in full_text)
    system_count = sum(1 for keyword in system_keywords if keyword in full_text)
    medical_count = sum(1 for keyword in medical_tech_keywords if keyword in full_text)
    
    # ê¸°ìˆ ì  íŠ¹ì„±ì´ ê°•í•œ ê²½ìš° ì œì™¸
    if strong_tech_count >= 2:  # 2ê°œ ì´ìƒì˜ ê°•í•œ ê¸°ìˆ  í‚¤ì›Œë“œ
        return 'Exclude (ì œì™¸ì—°êµ¬)'
    elif strong_tech_count >= 1 and system_count >= 2:  # ê¸°ìˆ +ì‹œìŠ¤í…œ ì¡°í•©
        return 'Exclude (ì œì™¸ì—°êµ¬)'
    elif medical_count >= 2:  # ì˜ë£Œ/êµìœ¡ ê¸°ìˆ 
        return 'Exclude (ì œì™¸ì—°êµ¬)'
    elif system_count >= 3:  # 3ê°œ ì´ìƒì˜ ì‹œìŠ¤í…œ í‚¤ì›Œë“œ
        return 'Exclude (ì œì™¸ì—°êµ¬)'
    
    # 3ë‹¨ê³„: P2P, ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ê°•í•œ ì œì™¸
    if 'p2p' in full_text or 'peer-to-peer' in full_text:
        user_count = sum(1 for keyword in user_behavior_keywords if keyword in full_text)
        if user_count < 2:  # ì‚¬ìš©ì ê´€ë ¨ í‚¤ì›Œë“œê°€ ì ìœ¼ë©´ ì œì™¸
            return 'Exclude (ì œì™¸ì—°êµ¬)'
    
    # 4ë‹¨ê³„: ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë°ì´ ìˆì§€ë§Œ ì‚¬ìš©ì ì¤‘ì‹¬ì´ ì•„ë‹Œ ê²½ìš°
    livestream_found = any(keyword in full_text for keyword in livestreaming_keywords)
    if livestream_found:
        user_count = sum(1 for keyword in user_behavior_keywords if keyword in full_text)
        tech_total = strong_tech_count + system_count
        
        if tech_total >= 2 and user_count <= 1:  # ê¸°ìˆ ì ì´ê³  ì‚¬ìš©ì ì¤‘ì‹¬ì´ ì•„ë‹˜
            return 'Exclude (ì œì™¸ì—°êµ¬)'
        elif user_count >= 2:  # ì‚¬ìš©ì ì¤‘ì‹¬
            return 'Include (ê´€ë ¨ì—°êµ¬)'
        else:
            return 'Review (ê²€í† í•„ìš”)'
    
    # 5ë‹¨ê³„: ì¼ë°˜ì ì¸ ì‚¬ìš©ì ì¤‘ì‹¬ í‚¤ì›Œë“œ í™•ì¸
    user_count = sum(1 for keyword in user_behavior_keywords if keyword in full_text)
    tech_total = strong_tech_count + system_count
    
    if user_count >= 3 and tech_total <= 1:
        return 'Include (ê´€ë ¨ì—°êµ¬)'
    elif tech_total >= 2:
        return 'Exclude (ì œì™¸ì—°êµ¬)'
    else:
        return 'Review (ê²€í† í•„ìš”)'

# --- ì œì™¸ ì´ìœ  ë¶„ì„ í•¨ìˆ˜ (SciMAT ê¸°ë°˜ ê°œì„ ) ---
def get_detailed_exclusion_reason(row):
    """SciMAT ë°©ë²•ë¡ ì„ ì ìš©í•œ ìƒì„¸í•œ ì œì™¸ ì´ìœ  ë¶„ì„"""
    exclusion_categories = {
        # 1ë‹¨ê³„: ê¸°ìˆ ì  ë³µì¡ë„ë³„ ë¶„ë¥˜
        'ì €ìˆ˜ì¤€ ë„¤íŠ¸ì›Œí¬ ê¸°ìˆ ': {
            'keywords': ['mac layer', 'phy layer', 'network layer', 'transport layer', 'protocol stack'],
            'weight': 3.0
        },
        'í”„ë¡œí† ì½œ êµ¬í˜„': {
            'keywords': ['protocol implementation', 'protocol design', 'protocol optimization', 'routing protocol'],
            'weight': 3.0
        },
        'í•˜ë“œì›¨ì–´ ìµœì í™”': {
            'keywords': ['hardware optimization', 'fpga implementation', 'asic design', 'vlsi'],
            'weight': 2.5
        },
        'ì‹œìŠ¤í…œ ì„±ëŠ¥': {
            'keywords': ['system performance', 'throughput optimization', 'latency reduction', 'bandwidth'],
            'weight': 2.0
        },
        'ë°ì´í„° ì „ì†¡': {
            'keywords': ['packet dropping', 'forward error correction', 'automatic repeat request', 'goodput'],
            'weight': 2.5
        },
        'ë„¤íŠ¸ì›Œí¬ ë¶„ì„': {
            'keywords': ['network traffic', 'tcp', 'udp', 'network topology'],
            'weight': 2.0
        },
        'ì„¼ì„œ ê¸°ìˆ ': {
            'keywords': ['sensor data', 'sensor network', 'wireless sensor', 'iot'],
            'weight': 1.5
        },
        'í™˜ê²½ê³¼í•™': {
            'keywords': ['geoscience', 'environmental data', 'remote sensing'],
            'weight': 1.5
        }
    }
    
    title = str(row.get('TI', '')).lower()
    keywords = str(row.get('DE', '')).lower()
    keywords_plus = str(row.get('ID', '')).lower()
    abstract = str(row.get('AB', '')).lower()
    full_text = ' '.join([title, keywords, keywords_plus, abstract])
    
    found_reasons = []
    found_keywords = []
    total_weight = 0
    
    for category, details in exclusion_categories.items():
        matched_keywords = [kw for kw in details['keywords'] if kw in full_text]
        if matched_keywords:
            found_reasons.append(category)
            found_keywords.extend(matched_keywords)
            total_weight += details['weight'] * len(matched_keywords)
    
    # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚° (0.0 ~ 1.0)
    confidence_score = min(total_weight / 10, 1.0)
    
    # ì œì™¸ ìˆ˜ì¤€ ê²°ì •
    if confidence_score >= 0.7:
        exclusion_level = 'ê°•í•œ ì œì™¸'
    elif confidence_score >= 0.4:
        exclusion_level = 'ì¤‘ê°„ ì œì™¸'
    else:
        exclusion_level = 'ì•½í•œ ì œì™¸'
    
    return {
        'category': '; '.join(found_reasons) if found_reasons else 'ê¸°íƒ€ ê¸°ìˆ ì  ë‚´ìš©',
        'keywords': '; '.join(list(set(found_keywords))[:5]) if found_keywords else 'ê¸°íƒ€ í‚¤ì›Œë“œ',
        'confidence_score': confidence_score,
        'exclusion_level': exclusion_level
    }

def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì •ê·œí™” ë° ì •ì œ ì²˜ë¦¬"""
    if pd.isna(keywords_str) or not isinstance(keywords_str, str):
        return ""

    all_keywords = keywords_str.split(';')
    cleaned_keywords = set()

    for keyword in all_keywords:
        if not keyword.strip():
            continue

        # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ
        keyword_clean = keyword.strip().lower()
        keyword_clean = re.sub(r'[^a-z\s\-_]', '', keyword_clean)

        # 2ë‹¨ê³„: êµ¬ë¬¸ ë‹¨ìœ„ ì •ê·œí™”
        normalized_phrase = normalize_keyword_phrase(keyword_clean)

        # 3ë‹¨ê³„: ë‹¨ì–´ë³„ ì²˜ë¦¬
        if normalized_phrase == keyword_clean.lower():
            keyword_clean = keyword_clean.replace('-', ' ').replace('_', ' ')
            words = keyword_clean.split()

            filtered_words = []
            for word in words:
                if word and len(word) > 2 and word not in stop_words:
                    lemmatized_word = lemmatizer.lemmatize(word)
                    filtered_words.append(lemmatized_word)

            if filtered_words:
                reconstructed_phrase = " ".join(filtered_words)
                final_keyword = normalize_keyword_phrase(reconstructed_phrase)
                if final_keyword and len(final_keyword) > 2:
                    cleaned_keywords.add(final_keyword)
        else:
            if normalized_phrase and len(normalized_phrase) > 2:
                cleaned_keywords.add(normalized_phrase)

    return '; '.join(sorted(list(cleaned_keywords)))

# --- SCIMAT í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ ---
def convert_df_to_scimat_format(df_to_convert):
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']

    for _, row in df_to_convert.iterrows():
        if len(file_content) > 2:
            file_content.append("")
        sorted_tags = [tag for tag in wos_field_order if tag in row.index and pd.notna(row[tag])]

        for tag in sorted_tags:
            value = row[tag]
            if pd.isna(value) or not str(value).strip():
                continue
            if not isinstance(value, str):
                value = str(value)

            if tag in multi_line_fields:
                items = [item.strip() for item in value.split(';') if item.strip()]
                if items:
                    file_content.append(f"{tag} {items[0]}")
                    for item in items[1:]:
                        file_content.append(f"  {item}")
            else:
                file_content.append(f"{tag} {value}")

        file_content.append("ER")
    return "\n".join(file_content).encode('utf-8')

# --- ë©”ì¸ í—¤ë” (í•œì–‘ëŒ€ ë¸Œëœë”©) ---
st.markdown("""
<div style="position: relative; text-align: center; padding: 2rem 0 3rem 0; background: linear-gradient(135deg, #003875, #0056b3); color: white; border-radius: 16px; margin-bottom: 2rem; box-shadow: 0 8px 32px rgba(0,56,117,0.3);">
    <div style="position: absolute; top: 1rem; left: 2rem; color: white;">
        <div style="font-size: 14px; font-weight: 600; margin-bottom: 2px;">HANYANG UNIVERSITY</div>
        <div style="font-size: 12px; opacity: 0.9;">Technology Management Research</div>
        <div style="font-size: 11px; opacity: 0.8; margin-top: 4px;">mot.hanyang.ac.kr</div>
    </div>
    <div style="position: absolute; top: 1rem; right: 2rem; text-align: right; color: rgba(255,255,255,0.9); font-size: 0.85rem;">
        <p style="margin: 0;"><strong>Developed by:</strong> ì„íƒœê²½ (Teddy Lym)</p>
    </div>
    <h1 style="font-size: 3.5rem; font-weight: 700; margin-bottom: 0.5rem; letter-spacing: -0.02em;">
        WOS Prep
    </h1>
    <p style="font-size: 1.3rem; margin: 0; font-weight: 400; opacity: 0.95;">
        Professional Tool for Web of Science Data Pre-processing
    </p>
    <div style="width: 100px; height: 4px; background-color: rgba(255,255,255,0.3); margin: 2rem auto; border-radius: 2px;"></div>
</div>
""", unsafe_allow_html=True)

# --- ì£¼ìš” ê¸°ëŠ¥ ì†Œê°œ (ê°œì„ ëœ ì•„ì´ì½˜) ---
st.markdown("""
<div class="feature-grid">
    <div class="feature-card">
        <div class="feature-icon">ğŸ”</div>
        <div class="feature-title">ë°ì´í„° ë¶„ë¥˜</div>
        <div class="feature-desc">ì—°êµ¬ ëª©ì ì— ë§ëŠ” ë…¼ë¬¸ ìë™ ì„ ë³„</div>
    </div>
    <div class="feature-card">
        <div class="feature-icon">ğŸ·ï¸</div>
        <div class="feature-title">í‚¤ì›Œë“œ ì •ê·œí™”</div>
        <div class="feature-desc">AI ê¸°ë°˜ í‚¤ì›Œë“œ í‘œì¤€í™”</div>
    </div>
    <div class="feature-card">
        <div class="feature-icon">ğŸ”—</div>
        <div class="feature-title">SciMAT í˜¸í™˜</div>
        <div class="feature-desc">ì™„ë²½í•œ ë¶„ì„ ë„êµ¬ ì—°ë™</div>
    </div>
</div>
""", unsafe_allow_html=True)

# --- í‚¤ì›Œë“œ ì •ê·œí™” ê¸°ì¤€ ì„¤ëª… ---
with st.expander("â„¹ï¸ í‚¤ì›Œë“œ ì •ê·œí™” ê¸°ì¤€ ìƒì„¸", expanded=False):
    st.markdown("""
    <div class="info-panel">
        <h4 style="color: #003875; margin-bottom: 16px;">ì ìš©ë˜ëŠ” ì •ê·œí™” ê·œì¹™:</h4>
        <ul style="line-height: 1.8; color: #495057;">
            <li><strong>AI/ML ê´€ë ¨:</strong> machine learning â† machine-learning, ML, machinelearning</li>
            <li><strong>ì¸ê³µì§€ëŠ¥:</strong> artificial intelligence â† AI, artificial-intelligence</li>
            <li><strong>ë”¥ëŸ¬ë‹:</strong> deep learning â† deep-learning, deep neural networks, DNN</li>
            <li><strong>ìŠ¤íŠ¸ë¦¬ë°:</strong> live streaming â† live-streaming, livestreaming</li>
            <li><strong>ì‚¬ìš©ì ê²½í—˜:</strong> user experience â† user-experience, UX</li>
            <li><strong>ì—°êµ¬ë°©ë²•ë¡ :</strong> structural equation modeling â† SEM, PLS-SEM</li>
            <li><strong>ì „ììƒê±°ë˜:</strong> e commerce â† ecommerce, e-commerce, electronic commerce</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
st.markdown("""
<div class="section-header">
    <div class="section-title">ğŸ“ ë°ì´í„° ì—…ë¡œë“œ</div>
    <div class="section-subtitle">Web of Science Raw Data íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.</div>
</div>
""", unsafe_allow_html=True)

st.markdown("""
<div class="upload-zone">
    <div style="font-size: 3rem; margin-bottom: 16px; color: #003875;">ğŸ“¤</div>
    <h3 style="color: #212529; margin-bottom: 8px;">íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”</h3>
    <p style="color: #6c757d; margin: 0;">CSV, TXT, Excel (.xlsx/.xls) í˜•ì‹ì˜ WOS ë°ì´í„° íŒŒì¼</p>
</div>
""", unsafe_allow_html=True)

uploaded_file = st.file_uploader(
    "íŒŒì¼ ì„ íƒ",
    type=['csv', 'txt', 'xlsx', 'xls'],
    label_visibility="collapsed"
)

if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is None:
        st.error("âš ï¸ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.markdown("""
        **ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹:**
        - **CSV íŒŒì¼** (.csv) - ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ í˜•ì‹
        - **TXT íŒŒì¼** (.txt) - íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ WOS í˜•ì‹  
        - **Excel íŒŒì¼** (.xlsx/.xls) - WOS ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ í˜•ì‹ í¬í•¨
        
        **Web of Science ë‹¤ìš´ë¡œë“œ ê¶Œì¥ í˜•ì‹:**
        - 'Tab-delimited (Win)' ë˜ëŠ” 'Plain Text' í˜•ì‹ì„ ì„ íƒí•˜ì„¸ìš”.
        """)
        st.stop()

    column_mapping = {
        'Authors': 'AU', 'Article Title': 'TI', 'Source Title': 'SO', 'Author Keywords': 'DE',
        'Keywords Plus': 'ID', 'Abstract': 'AB', 'Cited References': 'CR', 'Publication Year': 'PY',
        'Times Cited, All Databases': 'TC', 'Cited Reference Count': 'NR', 'Times Cited, WoS Core': 'Z9'
    }
    for old_name, new_name in column_mapping.items():
        if old_name in df.columns:
            df.rename(columns={old_name: new_name}, inplace=True)

    # í”„ë¡œê·¸ë ˆìŠ¤ ì¸ë””ì¼€ì´í„°
    st.markdown('<div class="progress-indicator"></div>', unsafe_allow_html=True)
    
    with st.spinner("ğŸ”„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        # 1ë‹¨ê³„: ë¶„ë¥˜
        df['Classification'] = df.apply(classify_article, axis=1)

        # ì›ë³¸ í‚¤ì›Œë“œ ë°±ì—…
        if 'DE' in df.columns: df['DE_Original'] = df['DE'].copy()
        if 'ID' in df.columns: df['ID_Original'] = df['ID'].copy()

        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ì •ê·œí™”
        stop_words = set(stopwords.words('english'))
        custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article', 'using', 'based', 'approach', 'method', 'system', 'model'}
        stop_words.update(custom_stop_words)
        lemmatizer = WordNetLemmatizer()
        include_mask = df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'

        if 'DE' in df.columns:
            df['DE_cleaned'] = df['DE'].copy()
            df.loc[include_mask, 'DE_cleaned'] = df.loc[include_mask, 'DE'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))
        if 'ID' in df.columns:
            df['ID_cleaned'] = df['ID'].copy()
            df.loc[include_mask, 'ID_cleaned'] = df.loc[include_mask, 'ID'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))

    st.success("âœ… ë¶„ì„ ì™„ë£Œ!")

    # --- ë¶„ì„ ê²°ê³¼ ìš”ì•½ (Stats Overview ìŠ¤íƒ€ì¼) ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ“ˆ Stats Overview</div>
        <div class="section-subtitle">ë¶„ì„ ê²°ê³¼ ì£¼ìš” ì§€í‘œ</div>
    </div>
    """, unsafe_allow_html=True)

    # ë©”íŠ¸ë¦­ ì¹´ë“œë“¤
    col1, col2, col3, col4 = st.columns(4)
    
    classification_counts = df['Classification'].value_counts()
    total_papers = len(df)
    include_papers = classification_counts.get('Include (ê´€ë ¨ì—°êµ¬)', 0)
    
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“‹</div>
            <div class="metric-value">{total_papers:,}</div>
            <div class="metric-label">Total Papers</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">âœ…</div>
            <div class="metric-value">{include_papers:,}</div>
            <div class="metric-label">Relevant Studies</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        processing_rate = (include_papers / total_papers * 100) if total_papers > 0 else 0
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“Š</div>
            <div class="metric-value">{processing_rate:.1f}%</div>
            <div class="metric-label">Inclusion Rate</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        # í‚¤ì›Œë“œ ìˆ˜ ê³„ì‚°
        keyword_count = 0
        if 'DE_cleaned' in df.columns:
            all_keywords = []
            for text in df.loc[include_mask, 'DE_cleaned'].dropna():
                all_keywords.extend([kw.strip() for kw in text.split(';') if kw.strip()])
            keyword_count = len(set(all_keywords))
        
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ”¤</div>
            <div class="metric-value">{keyword_count:,}</div>
            <div class="metric-label">Unique Keywords</div>
        </div>
        """, unsafe_allow_html=True)

    # --- ë…¼ë¬¸ ë¶„ë¥˜ í˜„í™© ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">Research Classification Distribution</div>
    """, unsafe_allow_html=True)

    classification_counts_df = df['Classification'].value_counts().reset_index()
    classification_counts_df.columns = ['ë¶„ë¥˜', 'ë…¼ë¬¸ ìˆ˜']

    col1, col2 = st.columns([0.4, 0.6])
    with col1:
        st.dataframe(classification_counts_df, use_container_width=True, hide_index=True)

    with col2:
        # ë„ë„› ì°¨íŠ¸ (í•œì–‘ëŒ€ ìƒ‰ìƒ)
        domain = ['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)', 'Exclude (ì œì™¸ì—°êµ¬)']
        range_ = ['#003875', '#0056b3', '#6c757d']

        selection = alt.selection_point(fields=['ë¶„ë¥˜'], on='mouseover', nearest=True)

        base = alt.Chart(classification_counts_df).encode(
            theta=alt.Theta(field="ë…¼ë¬¸ ìˆ˜", type="quantitative", stack=True),
            color=alt.Color(field="ë¶„ë¥˜", type="nominal", title="Classification",
                           scale=alt.Scale(domain=domain, range=range_),
                           legend=alt.Legend(orient="right", titleColor="#212529", labelColor="#495057")),
            opacity=alt.condition(selection, alt.value(1), alt.value(0.8))
        ).add_params(selection)

        pie = base.mark_arc(outerRadius=120, innerRadius=70)
        text_total = alt.Chart(pd.DataFrame([{'value': f'{total_papers}'}])).mark_text(
            align='center', baseline='middle', fontSize=35, fontWeight='bold', color='#003875'
        ).encode(text='value:N')
        text_label = alt.Chart(pd.DataFrame([{'value': 'Total Papers'}])).mark_text(
            align='center', baseline='middle', fontSize=14, dy=-25, color='#495057'
        ).encode(text='value:N')

        chart = (pie + text_total + text_label).properties(
            title=alt.TitleParams(text='ë…¼ë¬¸ ë¶„ë¥˜ ë¶„í¬', anchor='middle', fontSize=16, fontWeight=500, color="#212529"),
            width=300, height=300
        ).configure_view(strokeWidth=0)
        st.altair_chart(chart, use_container_width=True)

    st.markdown("</div>", unsafe_allow_html=True)

    # --- ì—°ë„ë³„ ì—°êµ¬ ë™í–¥ ê·¸ë˜í”„ ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">Research Trend Analysis</div>
    """, unsafe_allow_html=True)
    
    df_trend = df.copy()
    if 'PY' in df_trend.columns:
        df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
        df_trend.dropna(subset=['PY'], inplace=True)
        df_trend['PY'] = df_trend['PY'].astype(int)
        
        yearly_counts = df_trend['PY'].value_counts().reset_index()
        yearly_counts.columns = ['Year', 'Count']
        yearly_counts = yearly_counts[yearly_counts['Year'] <= 2025].sort_values('Year')

        projection_layer = alt.Chart(pd.DataFrame([])).mark_line()
        show_projection_caption = False
        if 2025 in yearly_counts['Year'].values and 2024 in yearly_counts['Year'].values:
            count_2025_actual = yearly_counts.loc[yearly_counts['Year'] == 2025, 'Count'].iloc[0]
            count_2024_actual = yearly_counts.loc[yearly_counts['Year'] == 2024, 'Count'].iloc[0]
            count_2025_projected = count_2025_actual * 2
            
            projection_df = pd.DataFrame([
                {'Year': 2024, 'Count': count_2024_actual, 'Type': 'Projected'},
                {'Year': 2025, 'Count': count_2025_projected, 'Type': 'Projected'}
            ])
            
            projection_layer = alt.Chart(projection_df).mark_line(
                strokeDash=[5, 5], color='#ff6b6b', point={'color': '#ff6b6b', 'filled': False, 'size': 60}
            ).encode(x='Year:O', y='Count:Q')
            show_projection_caption = True
        
        selection_trend = alt.selection_point(fields=['Year'], on='mouseover', nearest=True, empty='none')
        
        line_chart = alt.Chart(yearly_counts).mark_line(
            point={'size': 80, 'filled': True}, strokeWidth=3, color='#003875'
        ).encode(
            x=alt.X('Year:O', title='ë°œí–‰ ì—°ë„'),
            y=alt.Y('Count:Q', title='ë…¼ë¬¸ ìˆ˜'),
            tooltip=['Year', 'Count'],
            opacity=alt.condition(selection_trend, alt.value(1), alt.value(0.8))
        ).add_params(selection_trend)
        
        trend_chart = (line_chart + projection_layer).properties(height=350)
        st.altair_chart(trend_chart, use_container_width=True)
        if show_projection_caption:
            st.caption("ğŸ“ˆ ë¹¨ê°„ ì ì„ ì€ 2025ë…„ ìƒë°˜ê¸° ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ ì—°ê°„ ë°œí–‰ëŸ‰ì„ ì¶”ì •í•œ ì˜ˆìƒì¹˜ì…ë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ ë°œí–‰ ì—°ë„(PY) ë°ì´í„°ê°€ ì—†ì–´ ì—°êµ¬ ë™í–¥ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("</div>", unsafe_allow_html=True)

    # --- ì£¼ìš” ì¸ìš© ë…¼ë¬¸ ë¶„ì„ ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">Top Cited Papers (by Reference Count)</div>
    """, unsafe_allow_html=True)
    
    if 'NR' in df.columns:
        df_cited = df.copy()
        df_cited['NR'] = pd.to_numeric(df_cited['NR'], errors='coerce').fillna(0)
        
        # NR > 0ì¸ ë…¼ë¬¸ë§Œ í•„í„°ë§í•˜ê³  ìƒìœ„ 5ê°œ ì„ íƒ
        df_cited_filtered = df_cited[df_cited['NR'] > 0]
        
        if len(df_cited_filtered) > 0:
            df_cited_top = df_cited_filtered.nlargest(5, 'NR')
            
            df_cited_top['Author_Display'] = df_cited_top['AU'].apply(
                lambda x: str(x).split(';')[0] if pd.notna(x) else 'Unknown Author'
            )
            df_cited_top['Title_Display'] = df_cited_top['TI'].apply(
                lambda x: (str(x)[:60] + '...') if pd.notna(x) and len(str(x)) > 60 else str(x) if pd.notna(x) else 'No Title'
            )
            df_cited_top['Label'] = df_cited_top.apply(
                lambda row: f"{row['Title_Display']} ({row['Author_Display']})", axis=1
            )

            # ì°¨íŠ¸ ìƒì„±
            cited_chart = alt.Chart(df_cited_top).mark_bar(
                color='#003875', 
                cornerRadiusEnd=4,
                size=30
            ).encode(
                x=alt.X('NR:Q', title='ì°¸ê³ ë¬¸í—Œ ìˆ˜', scale=alt.Scale(zero=True)),
                y=alt.Y('Label:N', title='ë…¼ë¬¸ ì œëª© ë° ì €ì', sort='-x'),
                tooltip=[
                    alt.Tooltip('TI:N', title='ë…¼ë¬¸ ì œëª©'),
                    alt.Tooltip('Author_Display:N', title='ì €ì'),
                    alt.Tooltip('NR:Q', title='ì°¸ê³ ë¬¸í—Œ ìˆ˜')
                ]
            ).properties(
                height=300,
                width=600
            ).resolve_scale(
                y='independent'
            )
            
            st.altair_chart(cited_chart, use_container_width=True)
        else:
            st.info("ğŸ“Š ì°¸ê³ ë¬¸í—Œ ìˆ˜ê°€ ê¸°ë¡ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ ì°¸ê³ ë¬¸í—Œ ìˆ˜(NR) ë°ì´í„°ê°€ ì—†ì–´ ì£¼ìš” ì¸ìš© ë…¼ë¬¸ì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    st.markdown("</div>", unsafe_allow_html=True)

    # --- ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ ---
    st.markdown("""
    <div class="chart-container">
        <div class="chart-title">Top Keywords Analysis (Relevant Studies Only)</div>
    """, unsafe_allow_html=True)
    
    all_keywords = []
    if 'DE_cleaned' in df.columns:
        all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'DE_cleaned'].dropna() for kw in text.split(';') if kw.strip()])
    if 'ID_cleaned' in df.columns:
        all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'ID_cleaned'].dropna() for kw in text.split(';') if kw.strip()])

    if all_keywords:
        keyword_counts = Counter(all_keywords)
        top_n = 20
        top_keywords_df = pd.DataFrame(keyword_counts.most_common(top_n), columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
        top_3_keywords = top_keywords_df['í‚¤ì›Œë“œ'].head(3).tolist()
        
        selection_keyword = alt.selection_point(fields=['í‚¤ì›Œë“œ'], on='mouseover', nearest=True, empty='none')

        y_encoding = alt.Y('í‚¤ì›Œë“œ:N', title=None, sort=alt.SortField(field='ë¹ˆë„', order='descending'))
        x_encoding = alt.X('ë¹ˆë„:Q', title='ë¹ˆë„', scale=alt.Scale(zero=True))
        
        base_chart = alt.Chart(top_keywords_df).encode(
            y=y_encoding,
            x=x_encoding,
            opacity=alt.condition(selection_keyword, alt.value(1), alt.value(0.8)),
            tooltip=['í‚¤ì›Œë“œ', 'ë¹ˆë„']
        ).add_params(selection_keyword)

        line = base_chart.mark_rule(size=3, color='#dee2e6')
        
        point = base_chart.mark_point(filled=True, size=120).encode(
            color=alt.condition(
                alt.FieldOneOfPredicate(field='í‚¤ì›Œë“œ', oneOf=top_3_keywords),
                alt.value('#003875'),
                alt.value('#0056b3')
            )
        )
        
        final_chart = (line + point).properties(height=500).configure_axis(
            grid=False
        ).configure_view(strokeWidth=0)

        st.altair_chart(final_chart, use_container_width=True)

        # ì •ê·œí™” ì „í›„ ë¹„êµ (ê°œì„ ëœ ë””ìì¸)
        if st.checkbox("ğŸ” ì •ê·œí™” ì „í›„ ë¹„êµ ë³´ê¸° (ìƒ˜í”Œ)", key="comparison_check"):
            st.markdown("""
            <div class="comparison-panel">
                <h4 style="color: #003875; margin-bottom: 16px;">í‚¤ì›Œë“œ ì •ê·œí™” íš¨ê³¼ ë¹„êµ</h4>
            """, unsafe_allow_html=True)
            
            sample_data = []
            sample_rows = df.loc[include_mask].head(3)
            for idx, row in sample_rows.iterrows():
                if 'DE_Original' in df.columns and pd.notna(row.get('DE_Original')):
                    sample_data.append({
                        'ë…¼ë¬¸ ID': idx, 'í•„ë“œ': 'Author Keywords (DE)',
                        'ì •ê·œí™” ì „': str(row['DE_Original']), 'ì •ê·œí™” í›„': str(row['DE_cleaned'])
                    })
            if sample_data:
                st.dataframe(pd.DataFrame(sample_data), use_container_width=True, hide_index=True)
            
            st.markdown("</div>", unsafe_allow_html=True)
    else:
        st.warning("âš ï¸ ê´€ë ¨ì—°êµ¬ë¡œ ë¶„ë¥˜ëœ ë…¼ë¬¸ì—ì„œ ìœ íš¨í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("</div>", unsafe_allow_html=True)

    # --- ì œì™¸ëœ ì—°êµ¬ ë¶„ì„ (SciMAT ê¸°ë°˜ ê°œì„ ) ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸš« Excluded Studies Analysis</div>
        <div class="section-subtitle">ì œì™¸ëœ ì—°êµ¬ ìƒì„¸ ëª©ë¡ ë° SciMAT ë°©ë²•ë¡  ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦</div>
    </div>
    """, unsafe_allow_html=True)

    # ì œì™¸ëœ ì—°êµ¬ ë°ì´í„° ì¤€ë¹„
    excluded_papers = df[df['Classification'] == 'Exclude (ì œì™¸ì—°êµ¬)'].copy()
    
    if len(excluded_papers) > 0:
        # ìƒìœ„ 30ê°œ ì œì™¸ëœ ë…¼ë¬¸ ì„ íƒ
        excluded_sample = excluded_papers.head(30).copy()
        
        # SciMAT ë°©ì‹ì˜ ì•Œê³ ë¦¬ì¦˜ ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚°
        exclusion_details = [get_detailed_exclusion_reason(row) for _, row in excluded_sample.iterrows()]
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ í†µê³„
        high_confidence = len([d for d in exclusion_details if d['confidence_score'] >= 0.7])
        medium_confidence = len([d for d in exclusion_details if 0.4 <= d['confidence_score'] < 0.7])
        low_confidence = len([d for d in exclusion_details if d['confidence_score'] < 0.4])
        
        # í‘œì‹œí•  ë°ì´í„° ì¤€ë¹„ (SciMAT ìŠ¤íƒ€ì¼)
        display_data = []
        for idx, (_, row) in enumerate(excluded_sample.iterrows()):
            exclusion_info = exclusion_details[idx]
            
            title = str(row.get('TI', 'No Title'))
            author = str(row.get('AU', 'Unknown')).split(';')[0] if pd.notna(row.get('AU')) else 'Unknown'
            year = str(row.get('PY', 'N/A'))
            journal = str(row.get('SO', 'N/A'))
            author_keywords = str(row.get('DE', 'N/A'))
            
            display_data.append({
                'ìˆœë²ˆ': len(display_data) + 1,
                'ë…¼ë¬¸ ì œëª©': title,
                'ì €ì': author,
                'ì—°ë„': year,
                'ì €ë„ëª…': journal,
                'ì €ì í‚¤ì›Œë“œ': author_keywords,
                'ì œì™¸ ë¶„ë¥˜': exclusion_info['category'],
                'íƒì§€ëœ ì œì™¸ í‚¤ì›Œë“œ': exclusion_info['keywords'],
                'ì œì™¸ ìˆ˜ì¤€': exclusion_info['exclusion_level'],
                'ì‹ ë¢°ë„ ì ìˆ˜': f"{exclusion_info['confidence_score']:.2f}"
            })
        
        excluded_df = pd.DataFrame(display_data)
        
        # SciMAT ìŠ¤íƒ€ì¼ ì•Œê³ ë¦¬ì¦˜ ì„±ê³¼ ì¸¡ì •
        st.markdown("""
        <div class="chart-container">
            <div class="chart-title">ğŸ” SciMAT ë°©ì‹ ì•Œê³ ë¦¬ì¦˜ ì„±ê³¼ ë¶„ì„</div>
        """, unsafe_allow_html=True)
        
        col1, col2, col3, col4 = st.columns(4)
        
        total_excluded = len(excluded_papers)
        with col1:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-icon">ğŸš«</div>
                <div class="metric-value">{total_excluded:,}</div>
                <div class="metric-label">ì´ ì œì™¸ëœ ë…¼ë¬¸</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col2:
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-icon">ğŸ¯</div>
                <div class="metric-value">{high_confidence}</div>
                <div class="metric-label">ê³ ì‹ ë¢°ë„ ì œì™¸</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col3:
            accuracy = (high_confidence / min(30, total_excluded) * 100) if total_excluded > 0 else 0
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-icon">ğŸ“Š</div>
                <div class="metric-value">{accuracy:.1f}%</div>
                <div class="metric-label">ê°•í•œ ì œì™¸ ë¹„ìœ¨</div>
            </div>
            """, unsafe_allow_html=True)
        
        with col4:
            unique_categories = len(set([d['category'] for d in exclusion_details]))
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-icon">ğŸ·ï¸</div>
                <div class="metric-value">{unique_categories}</div>
                <div class="metric-label">ì œì™¸ ì¹´í…Œê³ ë¦¬ ìˆ˜</div>
            </div>
            """, unsafe_allow_html=True)
        
        st.markdown('</div>', unsafe_allow_html=True)

        # ì œì™¸ëœ ë…¼ë¬¸ ìƒì„¸ ëª©ë¡ (SciMAT ìŠ¤íƒ€ì¼ í…Œì´ë¸”)
        st.markdown('<div class="chart-container">', unsafe_allow_html=True)
        st.markdown('<div class="chart-title">ì œì™¸ëœ ë…¼ë¬¸ ìƒì„¸ ëª©ë¡ (SciMAT ì‹ ë¢°ë„ ê¸°ë°˜)</div>', unsafe_allow_html=True)
        
        # í•„í„°ë§ ì˜µì…˜
        col1, col2 = st.columns(2)
        with col1:
            unique_levels = list(set([d['ì œì™¸ ìˆ˜ì¤€'] for d in display_data]))
            level_filter = st.selectbox(
                "ì œì™¸ ìˆ˜ì¤€ë³„ í•„í„°:",
                ['ì „ì²´'] + sorted(unique_levels),
                key="level_filter"
            )
        
        with col2:
            unique_years = sorted(list(set([d['ì—°ë„'] for d in display_data if d['ì—°ë„'] != 'N/A'])))
            year_filter = st.selectbox(
                "ì—°ë„ë³„ í•„í„°:",
                ['ì „ì²´'] + unique_years,
                key="year_filter"
            )
        
        # í•„í„° ì ìš©
        filtered_data = excluded_df.copy()
        if level_filter != 'ì „ì²´':
            filtered_data = filtered_data[filtered_data['ì œì™¸ ìˆ˜ì¤€'] == level_filter]
        if year_filter != 'ì „ì²´':
            filtered_data = filtered_data[filtered_data['ì—°ë„'] == year_filter]
        
        # ê°œì„ ëœ í…Œì´ë¸” í‘œì‹œ
        if len(filtered_data) > 0:
            # HTML í…Œì´ë¸”ë¡œ ìƒ‰ìƒ êµ¬ë¶„ í‘œì‹œ
            table_html = """
            <table style="width: 100%; border-collapse: collapse; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;">
            <thead>
                <tr style="background-color: #f8f9fa; border-bottom: 2px solid #003875;">
                    <th style="padding: 12px; border-bottom: 2px solid #dee2e6; text-align: center; width: 5%;">ìˆœë²ˆ</th>
                    <th style="padding: 12px; border-bottom: 2px solid #dee2e6; text-align: left; width: 30%;">ë…¼ë¬¸ ì œëª©</th>
                    <th style="padding: 12px; border-bottom: 2px solid #dee2e6; text-align: left; width: 15%;">ì €ì</th>
                    <th style="padding: 12px; border-bottom: 2px solid #dee2e6; text-align: center; width: 6%;">ì—°ë„</th>
                    <th style="padding: 12px; border-bottom: 2px solid #dee2e6; text-align: left; width: 25%;">ì €ì í‚¤ì›Œë“œ</th>
                    <th style="padding: 12px; border-bottom: 2px solid #dee2e6; text-align: left; width: 12%;">ì œì™¸ ë¶„ë¥˜</th>
                    <th style="padding: 12px; border-bottom: 2px solid #dee2e6; text-align: center; width: 7%;">ì œì™¸ ìˆ˜ì¤€</th>
                </tr>
            </thead>
            <tbody>
            """
            
            for _, row in filtered_data.iterrows():
                level_color = '#d32f2f' if 'ê°•í•œ' in row['ì œì™¸ ìˆ˜ì¤€'] else '#f57c00' if 'ì¤‘ê°„' in row['ì œì™¸ ìˆ˜ì¤€'] else '#388e3c'
                table_html += f"""
                <tr style="border-bottom: 1px solid #dee2e6;">
                    <td style="padding: 12px; text-align: center;">{row['ìˆœë²ˆ']}</td>
                    <td style="padding: 12px;">
                        <span class="hover-tooltip" data-tooltip="{row['ë…¼ë¬¸ ì œëª©']}">{row['ë…¼ë¬¸ ì œëª©'][:80]}{'...' if len(str(row['ë…¼ë¬¸ ì œëª©'])) > 80 else ''}</span>
                    </td>
                    <td style="padding: 12px;">{row['ì €ì']}</td>
                    <td style="padding: 12px; text-align: center;">{row['ì—°ë„']}</td>
                    <td style="padding: 12px;">{str(row['ì €ì í‚¤ì›Œë“œ'])[:50]}{'...' if len(str(row['ì €ì í‚¤ì›Œë“œ'])) > 50 else ''}</td>
                    <td style="padding: 12px;">{row['ì œì™¸ ë¶„ë¥˜']}</td>
                    <td style="padding: 12px; font-weight: bold; color: {level_color};">{row['ì œì™¸ ìˆ˜ì¤€']}</td>
                </tr>
                """
            
            table_html += """
            </tbody>
            </table>
            """
            
            st.markdown(table_html, unsafe_allow_html=True)
            st.info(f"ğŸ“Š ì´ {len(filtered_data)}ê°œì˜ ì œì™¸ëœ ë…¼ë¬¸ì´ í‘œì‹œë©ë‹ˆë‹¤. (ì „ì²´ ì œì™¸: {total_excluded}ê°œ)")
        else:
            st.warning("ì„ íƒí•œ í•„í„° ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # SciMAT ê¸°ë°˜ ì œì™¸ ê¸°ì¤€ ì„¤ëª…
        st.markdown("""
        <div class="info-panel">
            <h4 style="color: #003875; margin-bottom: 16px;">ğŸ’¡ SciMAT ë°©ë²•ë¡  ê¸°ë°˜ ì œì™¸ ì‹œìŠ¤í…œ:</h4>
            <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 16px;">
                <div>
                    <h5 style="color: #d32f2f; margin-bottom: 8px;">ğŸ¯ ì‹ ë¢°ë„ ê¸°ë°˜ ë¶„ë¥˜</h5>
                    <p style="font-size: 0.9rem; color: #495057;"><strong>ê°•í•œ ì œì™¸ (â‰¥0.7):</strong> ëª…í™•í•œ ê¸°ìˆ  ë…¼ë¬¸<br><strong>ì¤‘ê°„ ì œì™¸ (0.4-0.7):</strong> ê¸°ìˆ ì ì´ë‚˜ ì¬ê²€í†  í•„ìš”<br><strong>ì•½í•œ ì œì™¸ (<0.4):</strong> ê²½ê³„ì„  ë…¼ë¬¸</p>
                </div>
                <div>
                    <h5 style="color: #d32f2f; margin-bottom: 8px;">âš–ï¸ ê°€ì¤‘ì¹˜ ê¸°ë°˜ í‰ê°€</h5>
                    <p style="font-size: 0.9rem; color: #495057;">ë„¤íŠ¸ì›Œí¬ í”„ë¡œí† ì½œ(3.0), í•˜ë“œì›¨ì–´ ì„¤ê³„(2.5), ì‹œìŠ¤í…œ ìµœì í™”(2.0) ë“± ê¸°ìˆ  ë³µì¡ë„ë³„ ì°¨ë“± ê°€ì¤‘ì¹˜ ì ìš©</p>
                </div>
                <div>
                    <h5 style="color: #d32f2f; margin-bottom: 8px;">ğŸ”„ ë§¥ë½ ê¸°ë°˜ ë¶„ì„</h5>
                    <p style="font-size: 0.9rem; color: #495057;">ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë° í‚¤ì›Œë“œ ìš°ì„  í¬í•¨, ê¸°ìˆ  í‚¤ì›Œë“œëŠ” ë§¥ë½ì  ë¶„ì„ìœ¼ë¡œ ì˜¤ë¶„ë¥˜ ë°©ì§€</p>
                </div>
                <div>
                    <h5 style="color: #d32f2f; margin-bottom: 8px;">ğŸ“Š Document Mapper ì ìš©</h5>
                    <p style="font-size: 0.9rem; color: #495057;">SciMATì˜ Core/Secondary Mapper ê°œë…ì„ ì ìš©í•œ ë‹¤ì¤‘ ê²€ì¦ ì‹œìŠ¤í…œ</p>
                </div>
            </div>
            <div style="margin-top: 16px; padding: 12px; background: #fff3cd; border-left: 4px solid #ffc107; border-radius: 4px;">
                <strong>ğŸ¯ ê°œì„  íš¨ê³¼:</strong> ê¸°ì¡´ ë‹¨ìˆœ í‚¤ì›Œë“œ ë§¤ì¹­ì—ì„œ SciMAT ë°©ë²•ë¡  ê¸°ë°˜ ì •êµí•œ ë¶„ë¥˜ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì—¬ ë¼ì´ë¸ŒìŠ¤íŠ¸ë¦¬ë° ìƒê±°ë˜ ë…¼ë¬¸ì˜ ì˜¤ë¶„ë¥˜ ë¬¸ì œ í•´ê²°
            </div>
        </div>
        """, unsafe_allow_html=True)
        
    else:
        st.info("ğŸ“Š ì œì™¸ëœ ì—°êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # --- ì²˜ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ê°„ì†Œí™”) ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ“‹ Final Dataset Summary</div>
        <div class="section-subtitle">ìµœì¢… ë¶„ì„ ëŒ€ìƒ ë°ì´í„° ìš”ì•½</div>
    </div>
    """, unsafe_allow_html=True)

    df_final = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
    if 'DE' in df_final.columns:
        df_final['DE'] = df_final['DE_cleaned']
    if 'ID' in df_final.columns:
        df_final['ID'] = df_final['ID_cleaned']
    cols_to_drop = ['Classification', 'DE_cleaned', 'ID_cleaned', 'DE_Original', 'ID_Original']
    df_final_output = df_final.drop(columns=[col for col in cols_to_drop if col in df_final.columns], errors='ignore')
    
    # ìµœì¢… ë°ì´í„°ì…‹ ìš”ì•½ ì •ë³´
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">âœ…</div>
            <div class="metric-value">{len(df_final_output):,}</div>
            <div class="metric-label">ìµœì¢… ë¶„ì„ ëŒ€ìƒ</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        include_count = len(df[df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'])
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ¯</div>
            <div class="metric-value">{include_count:,}</div>
            <div class="metric-label">ê´€ë ¨ ì—°êµ¬</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        review_count = len(df[df['Classification'] == 'Review (ê²€í† í•„ìš”)'])
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ”</div>
            <div class="metric-value">{review_count:,}</div>
            <div class="metric-label">ê²€í†  í•„ìš”</div>
        </div>
        """, unsafe_allow_html=True)

    # --- SciMAT í˜¸í™˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ---
    st.markdown("""
    <div class="section-header">
        <div class="section-title">ğŸ’¾ Export to SciMAT</div>
        <div class="section-subtitle">SciMAT í˜¸í™˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ìµœì¢… ê²°ê³¼</div>
    </div>
    """, unsafe_allow_html=True)

    # ìµœì¢… ë©”íŠ¸ë¦­ (ê°œì„ ëœ ë””ìì¸)
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-icon">ğŸ“Š</div>
            <div class="metric-value">{len(df_final_output):,}</div>
            <div class="metric-label">ìµœì¢… ë¶„ì„ ëŒ€ìƒ ë…¼ë¬¸ ìˆ˜</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        if include_mask.any():
            st.markdown(f"""
            <div class="metric-card">
                <div class="metric-icon">ğŸ¯</div>
                <div class="metric-value">{include_mask.sum():,}</div>
                <div class="metric-label">í‚¤ì›Œë“œ ì •ê·œí™” ì ìš© ë…¼ë¬¸</div>
            </div>
            """, unsafe_allow_html=True)

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ (ê°œì„ ëœ ìŠ¤íƒ€ì¼)
    text_data = convert_df_to_scimat_format(df_final_output)
    st.download_button(
        label="ğŸ“¥ SciMAT í˜¸í™˜ í¬ë§· íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.txt)",
        data=text_data,
        file_name="wos_prep_for_scimat.txt",
        mime="text/plain",
        type="primary",
        use_container_width=True
    )
    
    # ì‚¬ìš© ê°€ì´ë“œ (ê°œì„ ëœ ë””ìì¸)
    st.markdown("""
    <div class="info-panel">
        <h4 style="color: #003875; margin-bottom: 16px;">ğŸ’¡ SciMAT ì‚¬ìš© ê°€ì´ë“œ:</h4>
        <ol style="line-height: 1.8; color: #495057;">
            <li>ë‹¤ìš´ë¡œë“œí•œ <code>wos_prep_for_scimat.txt</code> íŒŒì¼ì„ SciMATì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.</li>
            <li><code>Group set</code> â†’ <code>Words groups manager</code>ì—ì„œ Levenshtein distanceë¥¼ í™œìš©í•´ ìœ ì‚¬ í‚¤ì›Œë“œë¥¼ ìë™ìœ¼ë¡œ ê·¸ë£¹í•‘í•©ë‹ˆë‹¤.</li>
            <li>ìˆ˜ë™ìœ¼ë¡œ í‚¤ì›Œë“œ ê·¸ë£¹ì„ ìµœì¢… ì¡°ì •í•œ í›„ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.</li>
            <li>Strategic Diagramê³¼ Evolution Mapì„ í†µí•´ ì—°êµ¬ ë¶„ì•¼ì˜ êµ¬ì¡°ì™€ ì§„í™”ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.</li>
        </ol>
        <div style="margin-top: 16px; padding: 12px; background: #e8f5e8; border-left: 4px solid #4caf50; border-radius: 4px;">
            <strong>ğŸ¯ ì£¼ìš” ê°œì„ ì‚¬í•­:</strong> SciMAT ë…¼ë¬¸ì˜ Document Mapper, Performance Analysis, Clustering Algorithm ê°œë…ì„ ì ìš©í•˜ì—¬ ê¸°ì¡´ ë‹¨ìˆœ ë¶„ë¥˜ì—ì„œ ì •êµí•œ ê³¼í•™ ë§¤í•‘ ë¶„ì„ì´ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œë˜ì—ˆìŠµë‹ˆë‹¤.
        </div>
    </div>
    """, unsafe_allow_html=True)

# --- í•˜ë‹¨ ì—¬ë°± ---
st.markdown("<br><br>", unsafe_allow_html=True)

# --- ê°œë°œì ì •ë³´ ë° SciMAT ë…¼ë¬¸ í¬ë ˆë”§ ---
st.markdown("""
<div style="background: #f8f9fa; border-radius: 12px; padding: 20px; margin-top: 2rem; border: 1px solid #dee2e6;">
    <div style="text-align: center; color: #6c757d; font-size: 0.9rem;">
        <p style="margin: 0; font-weight: 600;">ğŸ”¬ SciMAT ë°©ë²•ë¡  ê¸°ë°˜ ê°œì„ </p>
        <p style="margin: 4px 0; font-size: 0.8rem;">Based on: Cobo, M.J., LÃ³pez-Herrera, A.G., Herrera-Viedma, E., & Herrera, F. (2012). <br>
        "SciMAT: A new science mapping analysis software tool." <i>Journal of the American Society for Information Science and Technology</i>, 63(8), 1609-1630.</p>
        <p style="margin: 8px 0 0 0; font-size: 0.8rem; color: #003875;">
            <strong>Developed by:</strong> ì„íƒœê²½ (Teddy Lym) | <strong>Affiliation:</strong> í•œì–‘ëŒ€í•™êµ ê¸°ìˆ ê²½ì˜ì „ë¬¸ëŒ€í•™ì›
        </p>
    </div>
</div>
""", unsafe_allow_html=True)
