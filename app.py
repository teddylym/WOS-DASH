import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt
import io

# --- 1. í˜ì´ì§€ ì„¤ì • ë° ê¸°ë³¸ ìŠ¤íƒ€ì¼ ---
st.set_page_config(
    page_title="WOS Prep | Professional Dashboard",
    layout="wide",  # ë„“ì€ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ë³€ê²½
    initial_sidebar_state="collapsed"
)

# --- ì‚¬ìš©ì ì •ì˜ CSS ---
st.markdown("""
<style>
    /* ê¸°ë³¸ í°íŠ¸ ë° ìƒ‰ìƒ ì„¤ì • */
    body {
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        background-color: #F0F2F5;
    }
    /* ê¸°ë³¸ ìƒ‰ìƒ ë³€ìˆ˜ */
    :root {
        --primary-color: #0096FF;
        --text-color: #333;
        --light-text-color: #6c757d;
        --bg-color: #FFFFFF;
        --border-color: #EAEAEA;
    }
    /* Streamlit ê¸°ë³¸ ìš”ì†Œ ìˆ¨ê¸°ê¸° */
    .stDeployButton { display: none; }
    /* ë©”ì¸ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .main .block-container {
        padding: 2rem;
        max-width: 1400px;
    }
    /* ì¹´ë“œ ìŠ¤íƒ€ì¼ */
    .metric-card {
        background-color: var(--bg-color);
        border: 1px solid var(--border-color);
        border-radius: 10px;
        padding: 1.5rem;
        text-align: center;
        box-shadow: 0 4px 6px rgba(0,0,0,0.04);
        transition: all 0.3s ease-in-out;
    }
    .metric-card:hover {
        transform: translateY(-5px);
        box-shadow: 0 12px 16px rgba(0,0,0,0.08);
    }
    .metric-card h3 {
        color: var(--light-text-color);
        font-size: 1rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
    }
    .metric-card p {
        color: var(--text-color);
        font-size: 2.25rem;
        font-weight: 700;
        margin: 0;
    }
    /* ì°¨íŠ¸ ì»¨í…Œì´ë„ˆ ìŠ¤íƒ€ì¼ */
    .chart-container {
        background-color: var(--bg-color);
        border: 1px solid var(--border-color);
        border-radius: 10px;
        padding: 2rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.04);
    }
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    h1, h2, h3, h4, h5, h6, .st-emotion-cache-10trblm, .st-emotion-cache-1kyxreq {
        color: var(--text-color) !important;
    }
    .st-emotion-cache-1kyxreq {
        font-weight: 600;
        font-size: 1.5rem;
        border-bottom: 2px solid var(--primary-color);
        padding-bottom: 0.5rem;
        margin-bottom: 1.5rem;
    }
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton>button {
        background-color: var(--primary-color);
        color: white;
        border-radius: 8px;
        border: none;
        padding: 0.75rem 1.5rem;
        font-weight: 600;
    }
    /* íŒŒì¼ ì—…ë¡œë” ìŠ¤íƒ€ì¼ */
    .stFileUploader {
        background-color: var(--bg-color);
        border: 2px dashed var(--primary-color);
        border-radius: 10px;
        padding: 2rem;
    }
</style>
""", unsafe_allow_html=True)

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ìºì‹œ ìœ ì§€) ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- 2. ë°±ì—”ë“œ ê¸°ëŠ¥ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „) ---
def build_normalization_map():
    """ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì—­ë°©í–¥ ì •ê·œí™” ì‚¬ì „ ìƒì„±"""
    base_map = {
        # AI/ML ê´€ë ¨ (ì„¸ë¶„í™” ìœ ì§€)
        "machine learning": ["machine-learning", "machine_learning", "ml", "machinelearning"],
        "artificial intelligence": ["ai", "artificial-intelligence", "artificial_intelligence", "artificialintelligence"],
        "deep learning": ["deep-learning", "deep_learning", "deep neural networks", "deep neural network", "dnn", "deeplearning"],
        "neural networks": ["neural-networks", "neural_networks", "neuralnetworks", "neural network", "nn"],
        "natural language processing": ["nlp", "natural-language-processing", "natural_language_processing"],
        "computer vision": ["computer-vision", "computer_vision", "computervision", "cv"],
        "reinforcement learning": ["reinforcement-learning", "reinforcement_learning", "rl"],
        
        # ìŠ¤íŠ¸ë¦¬ë°/ë¯¸ë””ì–´ ê´€ë ¨  
        "live streaming": ["live-streaming", "live_streaming", "livestreaming", "real time streaming"],
        "video streaming": ["video-streaming", "video_streaming", "videostreaming"],
        "social media": ["social-media", "social_media", "socialmedia"],
        "user experience": ["user-experience", "user_experience", "ux", "userexperience"],
        "user behavior": ["user-behavior", "user_behavior", "userbehavior"],
        "content creation": ["content-creation", "content_creation", "contentcreation"],
        "digital marketing": ["digital-marketing", "digital_marketing", "digitalmarketing"],
        "e commerce": ["ecommerce", "e-commerce", "e_commerce", "electronic commerce"],
        
        # ì—°êµ¬ë°©ë²•ë¡  ê´€ë ¨
        "data mining": ["data-mining", "data_mining", "datamining"],
        "big data": ["big-data", "big_data", "bigdata"],
        "data analysis": ["data-analysis", "data_analysis", "dataanalysis"],
        "sentiment analysis": ["sentiment-analysis", "sentiment_analysis", "sentimentanalysis"],
        "statistical analysis": ["statistical-analysis", "statistical_analysis", "statisticalanalysis"],
        "structural equation modeling": ["sem", "pls-sem", "pls sem", "structural equation model"],
        
        # ê¸°ìˆ  ê´€ë ¨
        "cloud computing": ["cloud-computing", "cloud_computing", "cloudcomputing"],
        "internet of things": ["iot", "internet-of-things", "internet_of_things"],
        "mobile applications": ["mobile-applications", "mobile_applications", "mobile apps", "mobile app"],
        "web development": ["web-development", "web_development", "webdevelopment"],
        "software engineering": ["software-engineering", "software_engineering", "softwareengineering"]
    }
    
    # ì—­ë°©í–¥ ë§¤í•‘ ìƒì„± (variation -> standard_form)
    reverse_map = {}
    for standard_form, variations in base_map.items():
        for variation in variations:
            reverse_map[variation.lower()] = standard_form
        # í‘œì¤€ í˜•íƒœë„ ìê¸° ìì‹ ìœ¼ë¡œ ë§¤í•‘
        reverse_map[standard_form.lower()] = standard_form
    
    return reverse_map

NORMALIZATION_MAP = build_normalization_map()

def normalize_keyword_phrase(phrase):
    """êµ¬ë¬¸ ë‹¨ìœ„ í‚¤ì›Œë“œ ì •ê·œí™”"""
    phrase_lower = phrase.lower().strip()
    return NORMALIZATION_MAP.get(phrase_lower, phrase_lower)

def load_data(uploaded_file):
    file_bytes = uploaded_file.getvalue()
    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'cp949']
    
    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # íƒ­ êµ¬ë¶„ì ìš°ì„  ì‹œë„
            df = pd.read_csv(io.StringIO(file_content), sep='\t', lineterminator='\n')
            if df.shape[1] > 1: return df
        except Exception:
            continue
            
    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # ì½¤ë§ˆ êµ¬ë¶„ì ì‹œë„
            df = pd.read_csv(io.StringIO(file_content))
            if df.shape[1] > 1: return df
        except Exception:
            continue
            
    return None

def classify_article(row):
    inclusion_keywords = ['user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'behavior', 'experience', 'engagement', 'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 'society', 'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization', 'education', 'learning', 'influencer']
    exclusion_keywords = ['protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer', 'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput', 'sensor data', 'geoscience', 'environmental data', 'wlan', 'ofdm', 'error correction', 'tcp', 'udp', 'network traffic']
    title = str(row.get('TI', '')).lower()
    source_title = str(row.get('SO', '')).lower()
    author_keywords = str(row.get('DE', '')).lower()
    keywords_plus = str(row.get('ID', '')).lower()
    abstract = str(row.get('AB', '')).lower()
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    if any(keyword in full_text for keyword in exclusion_keywords): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in full_text for keyword in inclusion_keywords): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì •ê·œí™” ë° ì •ì œ ì²˜ë¦¬"""
    if pd.isna(keywords_str) or not isinstance(keywords_str, str): 
        return ""
    
    all_keywords = keywords_str.split(';')
    cleaned_keywords = set()
    
    for keyword in all_keywords:
        if not keyword.strip():
            continue
            
        # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ (í•˜ì´í”ˆê³¼ ì–¸ë”ìŠ¤ì½”ì–´ ê³µë°±ìœ¼ë¡œ ë³€í™˜)
        keyword_clean = keyword.strip().lower()
        keyword_clean = re.sub(r'[^a-z\s\-_]', '', keyword_clean)
        
        # 2ë‹¨ê³„: êµ¬ë¬¸ ë‹¨ìœ„ ì •ê·œí™” ë¨¼ì € ì‹œë„ (í•˜ì´í”ˆ í¬í•¨ ìƒíƒœë¡œ)
        normalized_phrase = normalize_keyword_phrase(keyword_clean)
        
        # 3ë‹¨ê³„: ì •ê·œí™”ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë‹¨ì–´ë³„ ì²˜ë¦¬
        if normalized_phrase == keyword_clean.lower():
            # í•˜ì´í”ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¨ì–´ë³„ ì²˜ë¦¬
            keyword_clean = keyword_clean.replace('-', ' ').replace('_', ' ')
            words = keyword_clean.split()
            
            # ë¶ˆìš©ì–´ ì œê±° ë° í‘œì œì–´ ì¶”ì¶œ
            filtered_words = []
            for word in words:
                if word and len(word) > 2 and word not in stop_words:
                    lemmatized_word = lemmatizer.lemmatize(word)
                    filtered_words.append(lemmatized_word)
            
            if filtered_words:
                reconstructed_phrase = " ".join(filtered_words)
                # ì¬êµ¬ì„±ëœ êµ¬ë¬¸ì— ëŒ€í•´ ë‹¤ì‹œ ì •ê·œí™” ì‹œë„
                final_keyword = normalize_keyword_phrase(reconstructed_phrase)
                if final_keyword and len(final_keyword) > 2:
                    cleaned_keywords.add(final_keyword)
        else:
            # ì´ë¯¸ ì •ê·œí™”ëœ ê²½ìš° ë°”ë¡œ ì¶”ê°€
            if normalized_phrase and len(normalized_phrase) > 2:
                cleaned_keywords.add(normalized_phrase)
    
    return '; '.join(sorted(list(cleaned_keywords)))

def convert_df_to_scimat_format(df_to_convert):
    """ì™„ì „í•œ SciMAT í˜•ì‹ ë³€í™˜ í•¨ìˆ˜"""
    # ì›ë³¸ WoS íŒŒì¼ê³¼ ì™„ì „íˆ ë™ì¼í•œ í•„ë“œ ìˆœì„œ ë° í—¤ë”
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    
    # ì›ë³¸ê³¼ ì™„ì „íˆ ë™ì¼í•œ í—¤ë”
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']
    
    for _, row in df_to_convert.iterrows():
        # ì²« ë²ˆì§¸ ë ˆì½”ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë¹ˆ ì¤„ ì¶”ê°€ (ì›ë³¸ê³¼ ë™ì¼)
        if len(file_content) > 2:
            file_content.append("")
            
        sorted_tags = [tag for tag in wos_field_order if tag in row.index and pd.notna(row[tag])]
        
        for tag in sorted_tags:
            value = row[tag]
            if pd.isna(value):
                continue
            if not isinstance(value, str): 
                value = str(value)
            if not value.strip(): 
                continue

            if tag in multi_line_fields:
                items = [item.strip() for item in value.split(';') if item.strip()]
                if items:
                    file_content.append(f"{tag} {items[0]}")
                    for item in items[1:]:
                        file_content.append(f"   {item}")
            else:
                file_content.append(f"{tag} {value}")

        file_content.append("ER")
    
    # ì›ë³¸ê³¼ ë™ì¼: UTF-8 (BOM ì—†ìŒ)ìœ¼ë¡œ ì¸ì½”ë”©
    return "\n".join(file_content).encode('utf-8')

# --- 3. UI ë Œë”ë§ ---
st.title("WOS Prep Dashboard")
st.markdown("<p style='margin-top:-1rem; color: var(--light-text-color)'>Web of Science ë°ì´í„° ìë™ ì „ì²˜ë¦¬ ë° ì‹œê°í™” ë„êµ¬</p>", unsafe_allow_html=True)

# --- íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ---
with st.container():
    st.markdown("### ğŸ“ ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader(
        "Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ Tab-delimited ë˜ëŠ” Plain Text í˜•ì‹ì˜ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
        type=['csv', 'txt'],
        label_visibility="collapsed"
    )

if uploaded_file is None:
    st.info("ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    st.stop()

# --- ë°ì´í„° ì²˜ë¦¬ ---
df = load_data(uploaded_file)
if df is None:
    st.error("âŒ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# ì»¬ëŸ¼ëª… ë§¤í•‘
column_mapping = {
    'Authors': 'AU', 'Article Title': 'TI', 'Source Title': 'SO', 'Author Keywords': 'DE',
    'Keywords Plus': 'ID', 'Abstract': 'AB', 'Cited References': 'CR', 'Publication Year': 'PY',
    'Times Cited, All Databases': 'TC', 'Cited Reference Count': 'NR', 'Times Cited, WoS Core': 'Z9'
}
df.rename(columns=column_mapping, inplace=True)

# ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ ì‹¤í–‰
with st.spinner("ğŸ”„ ë°ì´í„° ë¶„ì„ ì¤‘..."):
    df['Classification'] = df.apply(classify_article, axis=1)
    stop_words = set(stopwords.words('english'))
    custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article', 'using', 'based', 'approach', 'method', 'system', 'model'}
    stop_words.update(custom_stop_words)
    lemmatizer = WordNetLemmatizer()
    include_mask = df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'
    
    if 'DE' in df.columns:
        df['DE_cleaned'] = df['DE'].copy()
        df.loc[include_mask, 'DE_cleaned'] = df.loc[include_mask, 'DE'].apply(
            lambda x: clean_keyword_string(x, stop_words, lemmatizer)
        )
    if 'ID' in df.columns:
        df['ID_cleaned'] = df['ID'].copy()
        df.loc[include_mask, 'ID_cleaned'] = df.loc[include_mask, 'ID'].apply(
            lambda x: clean_keyword_string(x, stop_words, lemmatizer)
        )

# --- ìƒë‹¨ í†µê³„ ì¹´ë“œ ---
st.markdown("### ğŸ“Š Stats Overview")
total_papers = len(df)
final_papers = len(df[df['Classification'] != 'Exclude (ì œì™¸ì—°êµ¬)'])
included_papers = df['Classification'].value_counts().get('Include (ê´€ë ¨ì—°êµ¬)', 0)
reviewed_papers = df['Classification'].value_counts().get('Review (ê²€í† í•„ìš”)', 0)

col1, col2, col3, col4 = st.columns(4)
with col1:
    st.markdown(f'<div class="metric-card"><h3>ì´ ë…¼ë¬¸ ìˆ˜</h3><p>{total_papers:,}</p></div>', unsafe_allow_html=True)
with col2:
    st.markdown(f'<div class="metric-card"><h3>ìµœì¢… ë¶„ì„ ëŒ€ìƒ</h3><p>{final_papers:,}</p></div>', unsafe_allow_html=True)
with col3:
    st.markdown(f'<div class="metric-card"><h3>ê´€ë ¨ ì—°êµ¬</h3><p>{included_papers:,}</p></div>', unsafe_allow_html=True)
with col4:
    st.markdown(f'<div class="metric-card"><h3>ê²€í†  í•„ìš”</h3><p>{reviewed_papers:,}</p></div>', unsafe_allow_html=True)

st.markdown("<br>", unsafe_allow_html=True)

# --- ì¤‘ì•™ ë¶„ì„ ì„¹ì…˜ ---
col1, col2 = st.columns(2)
with col1:
    with st.container(border=True):
        st.markdown("<h5 style='text-align:center;'>ë…¼ë¬¸ ë¶„ë¥˜ ë¶„í¬</h5>", unsafe_allow_html=True)
        classification_counts = df['Classification'].value_counts().reset_index()
        classification_counts.columns = ['ë¶„ë¥˜', 'ë…¼ë¬¸ ìˆ˜']
        
        # ë„ë„›ê·¸ë˜í”„ì— ì¤‘ì•™ í…ìŠ¤íŠ¸ ì¶”ê°€
        base_chart = alt.Chart(classification_counts).mark_arc(
            innerRadius=70, 
            outerRadius=110,
            stroke='white',
            strokeWidth=2
        ).encode(
            theta=alt.Theta(field="ë…¼ë¬¸ ìˆ˜", type="quantitative"),
            color=alt.Color(field="ë¶„ë¥˜", type="nominal", scale=alt.Scale(
                domain=['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)', 'Exclude (ì œì™¸ì—°êµ¬)'],
                range=['#0096FF', '#7F7F7F', '#D3D3D3']
            ), legend=alt.Legend(title="ë¶„ë¥˜", orient="bottom")),
            tooltip=['ë¶„ë¥˜', 'ë…¼ë¬¸ ìˆ˜']
        ).properties(height=300)
        
        # ì¤‘ì•™ í…ìŠ¤íŠ¸ ì¶”ê°€
        center_text = alt.Chart(pd.DataFrame({'text': [f'Total\n{total_papers:,}\nPapers']})).mark_text(
            align='center',
            baseline='middle',
            fontSize=12,
            fontWeight='bold',
            color='#333'
        ).encode(
            x=alt.value(0),
            y=alt.value(0),
            text='text:N'
        )
        
        combined_chart = base_chart + center_text
        st.altair_chart(combined_chart, use_container_width=True)

with col2:
    with st.container(border=True):
        st.markdown("<h5 style='text-align:center;'>ì—°ë„ë³„ ì—°êµ¬ ë™í–¥</h5>", unsafe_allow_html=True)
        if 'PY' in df.columns:
            df_trend = df.copy()
            df_trend['PY'] = pd.to_numeric(df_trend['PY'], errors='coerce')
            yearly_counts = df_trend.dropna(subset=['PY'])['PY'].astype(int).value_counts().sort_index().reset_index()
            yearly_counts.columns = ['Year', 'Count']
            
            line_chart = alt.Chart(yearly_counts).mark_line(point=True, color='#0096FF').encode(
                x=alt.X('Year:O', title='ë°œí–‰ ì—°ë„'),
                y=alt.Y('Count:Q', title='ë…¼ë¬¸ ìˆ˜'),
                tooltip=['Year', 'Count']
            ).properties(height=300)
            st.altair_chart(line_chart, use_container_width=True)
        else:
            st.warning("ë°œí–‰ ì—°ë„(PY) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("<br>", unsafe_allow_html=True)

# --- ì£¼ìš” ì¸ìš© ë° í‚¤ì›Œë“œ ë¶„ì„ ---
with st.container(border=True):
    st.markdown("<h5 style='text-align:center;'>ì£¼ìš” ì¸ìš© ë° í‚¤ì›Œë“œ ë¶„ì„</h5>", unsafe_allow_html=True)
    tab1, tab2 = st.tabs(["ì£¼ìš” ì¸ìš© ë…¼ë¬¸", "ì£¼ìš” í‚¤ì›Œë“œ"])
    
    with tab1:
        if 'TC' in df.columns:  # NR ëŒ€ì‹  TC ì‚¬ìš©
            df_cited = df.copy()
            df_cited['TC'] = pd.to_numeric(df_cited['TC'], errors='coerce').fillna(0)
            df_cited = df_cited.sort_values(by='TC', ascending=False).head(5)
            
            # ì €ìëª…ê³¼ ì œëª© ì²˜ë¦¬
            df_cited['Author_Short'] = df_cited['AU'].fillna('Unknown').str.split(';').str[0]
            df_cited['Title_Short'] = df_cited['TI'].fillna('Untitled').str[:40] + '...'
            df_cited['Label'] = df_cited['Author_Short'] + " - " + df_cited['Title_Short']
            
            cited_chart = alt.Chart(df_cited).mark_bar(color='#0096FF').encode(
                x=alt.X('TC:Q', title='ì¸ìš© íšŸìˆ˜'),
                y=alt.Y('Label:N', title='ë…¼ë¬¸', sort='-x'),
                tooltip=['TI', 'AU', 'TC']
            ).properties(height=300)
            st.altair_chart(cited_chart, use_container_width=True)
        else:
            st.warning("ì¸ìš© íšŸìˆ˜(TC) ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tab2:
        all_keywords = []
        if 'DE_cleaned' in df.columns:
            for text in df.loc[include_mask, 'DE_cleaned'].dropna():
                if text and isinstance(text, str):
                    all_keywords.extend([kw.strip() for kw in text.split(';') if kw.strip()])
        if 'ID_cleaned' in df.columns:
            for text in df.loc[include_mask, 'ID_cleaned'].dropna():
                if text and isinstance(text, str):
                    all_keywords.extend([kw.strip() for kw in text.split(';') if kw.strip()])
        
        if all_keywords:
            keyword_counts = Counter(all_keywords)
            top_keywords = keyword_counts.most_common(20)
            top_keywords_df = pd.DataFrame(top_keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
            top_3_keywords = top_keywords_df['í‚¤ì›Œë“œ'].head(3).tolist()
            
            # ìˆ˜ì •ëœ Altair ì°¨íŠ¸ (ì˜¬ë°”ë¥¸ ì •ë ¬ ë¬¸ë²• ì‚¬ìš©)
            line = alt.Chart(top_keywords_df).mark_rule(size=2, color='#0096FF').encode(
                x='ë¹ˆë„:Q',
                y=alt.Y('í‚¤ì›Œë“œ:N', sort='-x')  # ìˆ˜ì •ëœ ë¶€ë¶„
            )
            
            lollipop_chart = alt.Chart(top_keywords_df).mark_point(filled=True, size=100).encode(
                x=alt.X('ë¹ˆë„:Q', title='ë¹ˆë„'),
                y=alt.Y('í‚¤ì›Œë“œ:N', sort='-x', title=None),  # ìˆ˜ì •ëœ ë¶€ë¶„
                color=alt.condition(
                    alt.FieldOneOfPredicate(field='í‚¤ì›Œë“œ', oneOf=top_3_keywords),
                    alt.value('#FF6B6B'), alt.value('#0096FF')
                ),
                tooltip=['í‚¤ì›Œë“œ', 'ë¹ˆë„']
            )
            
            st.altair_chart((line + lollipop_chart).properties(height=500), use_container_width=True)
        else:
            st.warning("ê´€ë ¨ì—°êµ¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ë‹¤ìš´ë¡œë“œ ---
st.markdown("<br>", unsafe_allow_html=True)
with st.expander("ğŸ“‹ ì²˜ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ë‹¤ìš´ë¡œë“œ"):
    df_final = df[df['Classification'] != 'Exclude (ì œì™¸ì—°êµ¬)'].copy()
    
    # ì •ì œëœ í‚¤ì›Œë“œë¡œ êµì²´
    for col in ['DE', 'ID']:
        if f'{col}_cleaned' in df_final.columns:
            df_final[col] = df_final[f'{col}_cleaned']
    
    cols_to_drop = [c for c in ['Classification', 'DE_cleaned', 'ID_cleaned', 'DE_Original', 'ID_Original'] if c in df_final.columns]
    df_final_output = df_final.drop(columns=cols_to_drop)
    
    st.dataframe(df_final_output.head(10))
    
    csv = df_final_output.to_csv(index=False).encode('utf-8')
    scimat_text = convert_df_to_scimat_format(df_final_output)

    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ", 
            data=csv,
            file_name="wos_preprocessed_data.csv", 
            mime="text/csv", 
            use_container_width=True
        )
    with col2:
        st.download_button(
            label="ğŸ“¥ SciMAT í˜¸í™˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", 
            data=scimat_text,
            file_name="wos_for_scimat.txt", 
            mime="text/plain", 
            use_container_width=True
        )
