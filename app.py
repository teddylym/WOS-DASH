import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import io
import altair as alt

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(page_title="WOS Analysis Dashboard", layout="centered")

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
@st.cache_data
def load_data(uploaded_file):
    encodings_to_try = ['utf-8', 'latin1', 'cp949', 'utf-8-sig']
    df = None
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df_try = pd.read_csv(uploaded_file, sep='\t', encoding=encoding)
            if df_try.shape[1] > 2: return df_try
        except Exception: continue
    for encoding in encodings_to_try:
        try:
            uploaded_file.seek(0)
            df_try = pd.read_csv(uploaded_file, encoding=encoding)
            if df_try.shape[1] > 2: return df_try
        except Exception: continue
    return None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def classify_article(row):
    inclusion_keywords = ['user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'behavior', 'experience', 'engagement', 'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 'society', 'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization', 'education', 'learning', 'influencer']
    exclusion_keywords = ['protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer', 'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput', 'sensor data', 'geoscience', 'environmental data', 'wlan', 'ofdm', 'error correction', 'tcp', 'udp', 'network traffic']
    title = str(row.get('Article Title', row.get('TI', ''))).lower()
    source_title = str(row.get('Source Title', row.get('SO', ''))).lower()
    author_keywords = str(row.get('Author Keywords', row.get('DE', ''))).lower()
    keywords_plus = str(row.get('Keywords Plus', row.get('ID', ''))).lower()
    abstract = str(row.get('Abstract', row.get('AB', ''))).lower()
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    if any(keyword in full_text for keyword in exclusion_keywords): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in full_text for keyword in inclusion_keywords): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

# --- ìˆ˜ì •ëœ í‚¤ì›Œë“œ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ë‹¨ì¼ ë¬¸ìì—´ ì²˜ë¦¬ìš©) ---
def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    if not isinstance(keywords_str, str):
        return ""
    all_keywords = keywords_str.lower().split(';')
    cleaned_keywords = set()
    for keyword in all_keywords:
        keyword = keyword.strip().replace('-', ' ')
        keyword = re.sub(r'[^a-z\s]', '', keyword)
        final_words = [lemmatizer.lemmatize(w) for w in keyword.split() if w not in stop_words and len(w) > 2]
        if final_words:
            cleaned_keywords.add(" ".join(final_words))
    return '; '.join(sorted(list(cleaned_keywords)))

# --- Streamlit UI ë° ì‹¤í–‰ ë¡œì§ ---
st.title("WOS ë°ì´í„° ë¶„ì„ ë° ì •ì œ ë„êµ¬")
st.caption("WOS Data Classifier & Preprocessor")

uploaded_file = st.file_uploader("WoS Raw Data íŒŒì¼(CSV/TXT)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['csv', 'txt'])

if uploaded_file is not None:
    df_raw = load_data(uploaded_file)
    
    if df_raw is None:
        st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # WoS ì›ë³¸ í•„ë“œëª…ê³¼ í‘œì¤€ 2ìë¦¬ íƒœê·¸ ë§¤í•‘
    column_mapping = {
        'Authors': 'AU', 'AU': 'AU',
        'Article Title': 'TI', 'TI': 'TI',
        'Source Title': 'SO', 'SO': 'SO',
        'Author Keywords': 'DE', 'DE': 'DE',
        'Keywords Plus': 'ID', 'ID': 'ID',
        'Abstract': 'AB', 'AB': 'AB',
        'Cited References': 'CR', 'CR': 'CR',
        'Publication Year': 'PY', 'PY': 'PY',
        'Times Cited, All Databases': 'TC', 'TC': 'TC'
    }
    
    df = df_raw.copy()
    # í˜„ì¬ ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ” ì—´ë§Œ ê³¨ë¼ì„œ ì´ë¦„ í‘œì¤€í™”
    rename_dict = {col: standard_col for col, standard_col in column_mapping.items() if col in df.columns}
    df.rename(columns=rename_dict, inplace=True)

    if st.button("ë¶„ì„ ë° ë³€í™˜ ì‹œì‘ / Start Analysis & Conversion"):
        with st.spinner("ë¶„ì„ ì¤‘... / Analyzing..."):
            
            # --- 1. ë…¼ë¬¸ ë¶„ë¥˜ ---
            df['Classification'] = df.apply(classify_article, axis=1)
            
            # --- 2. 'ê´€ë ¨ì—°êµ¬' ê·¸ë£¹ì˜ DE, ID í•„ë“œ ì§ì ‘ ì „ì²˜ë¦¬ ---
            stop_words = set(stopwords.words('english'))
            custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article'}
            stop_words.update(custom_stop_words)
            lemmatizer = WordNetLemmatizer()

            include_mask = df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'
            
            if 'DE' in df.columns:
                df.loc[include_mask, 'DE'] = df.loc[include_mask, 'DE'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))
            if 'ID' in df.columns:
                df.loc[include_mask, 'ID'] = df.loc[include_mask, 'ID'].apply(lambda x: clean_keyword_string(x, stop_words, lemmatizer))

            st.success("âœ… ë¶„ì„ ë° ë³€í™˜ ì™„ë£Œ! / Process Complete!")
            
            # --- 3. ìµœì¢… ì¶œë ¥ íŒŒì¼ ìƒì„± ---
            # í¬í•¨/ê²€í† í•„ìš” ì—°êµ¬ë§Œ ì„ íƒ
            df_final = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
            
            # ì´ë¯¸ì§€ì— ëª…ì‹œëœ 9ê°œ í•µì‹¬ í•„ë“œë§Œ ì„ íƒ
            final_columns = ['AU', 'TI', 'SO', 'DE', 'ID', 'AB', 'CR', 'PY', 'TC']
            # ì›ë³¸ì— ì—†ëŠ” ì—´ì´ ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬, ìˆëŠ” ì—´ë§Œ ì„ íƒ
            existing_final_columns = [col for col in final_columns if col in df_final.columns]
            df_final_output = df_final[existing_final_columns]
            
            st.subheader("ê²°ê³¼ ìš”ì•½ / Summary")
            st.metric("ìµœì¢… ë¶„ì„ ëŒ€ìƒ ë…¼ë¬¸ ìˆ˜ (Include + Review)", len(df_final))
            
            st.subheader("ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 10ê°œ)")
            st.dataframe(df_final_output.head(10))

            # --- ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ìƒì„± ---
            @st.cache_data
            def convert_df_to_text(df_to_convert):
                # SciMAT í˜¸í™˜ì„±ì„ ìœ„í•´ íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                return df_to_convert.to_csv(sep='\t', index=False, encoding='utf-8-sig').encode('utf-8-sig')

            text_data = convert_df_to_text(df_final_output)
            
            st.download_button(
               label="ğŸ“¥ ìµœì¢… íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.txt for SciMAT)",
               data=text_data,
               file_name="wos_processed_for_scimat.txt",
               mime="text/plain",
            )
