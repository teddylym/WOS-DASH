import streamlit as st
import pandas as pd
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
from collections import Counter
import altair as alt
import io

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="WOS Prep - Professional Data Preprocessing", 
    layout="centered",
    initial_sidebar_state="collapsed"
)

# --- NLTK ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (ìºì‹œ ìœ ì§€) ---
@st.cache_resource
def download_nltk_resources():
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('stopwords', quiet=True)
download_nltk_resources()

# --- í‚¤ì›Œë“œ ì •ê·œí™” ì‚¬ì „ (ì—­ë°©í–¥ ë§¤í•‘ìœ¼ë¡œ ìµœì í™”) ---
def build_normalization_map():
    """ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ì—­ë°©í–¥ ì •ê·œí™” ì‚¬ì „ ìƒì„±"""
    base_map = {
        # AI/ML ê´€ë ¨ (ì„¸ë¶„í™” ìœ ì§€)
        "machine learning": ["machine-learning", "machine_learning", "ml", "machinelearning"],
        "artificial intelligence": ["ai", "artificial-intelligence", "artificial_intelligence", "artificialintelligence"],
        "deep learning": ["deep-learning", "deep_learning", "deep neural networks", "deep neural network", "dnn", "deeplearning"],
        "neural networks": ["neural-networks", "neural_networks", "neuralnetworks", "neural network", "nn"],
        "natural language processing": ["nlp", "natural-language-processing", "natural_language_processing"],
        "computer vision": ["computer-vision", "computer_vision", "computervision", "cv"],
        "reinforcement learning": ["reinforcement-learning", "reinforcement_learning", "rl"],
        
        # ìŠ¤íŠ¸ë¦¬ë°/ë¯¸ë””ì–´ ê´€ë ¨  
        "live streaming": ["live-streaming", "live_streaming", "livestreaming", "real time streaming"],
        "video streaming": ["video-streaming", "video_streaming", "videostreaming"],
        "social media": ["social-media", "social_media", "socialmedia"],
        "user experience": ["user-experience", "user_experience", "ux", "userexperience"],
        "user behavior": ["user-behavior", "user_behavior", "userbehavior"],
        "content creation": ["content-creation", "content_creation", "contentcreation"],
        "digital marketing": ["digital-marketing", "digital_marketing", "digitalmarketing"],
        "e commerce": ["ecommerce", "e-commerce", "e_commerce", "electronic commerce"],
        
        # ì—°êµ¬ë°©ë²•ë¡  ê´€ë ¨
        "data mining": ["data-mining", "data_mining", "datamining"],
        "big data": ["big-data", "big_data", "bigdata"],
        "data analysis": ["data-analysis", "data_analysis", "dataanalysis"],
        "sentiment analysis": ["sentiment-analysis", "sentiment_analysis", "sentimentanalysis"],
        "statistical analysis": ["statistical-analysis", "statistical_analysis", "statisticalanalysis"],
        "structural equation modeling": ["sem", "pls-sem", "pls sem", "structural equation model"],
        
        # ê¸°ìˆ  ê´€ë ¨
        "cloud computing": ["cloud-computing", "cloud_computing", "cloudcomputing"],
        "internet of things": ["iot", "internet-of-things", "internet_of_things"],
        "mobile applications": ["mobile-applications", "mobile_applications", "mobile apps", "mobile app"],
        "web development": ["web-development", "web_development", "webdevelopment"],
        "software engineering": ["software-engineering", "software_engineering", "softwareengineering"]
    }
    
    # ì—­ë°©í–¥ ë§¤í•‘ ìƒì„± (variation -> standard_form)
    reverse_map = {}
    for standard_form, variations in base_map.items():
        for variation in variations:
            reverse_map[variation.lower()] = standard_form
        # í‘œì¤€ í˜•íƒœë„ ìê¸° ìì‹ ìœ¼ë¡œ ë§¤í•‘
        reverse_map[standard_form.lower()] = standard_form
    
    return reverse_map

NORMALIZATION_MAP = build_normalization_map()

def normalize_keyword_phrase(phrase):
    """êµ¬ë¬¸ ë‹¨ìœ„ í‚¤ì›Œë“œ ì •ê·œí™”"""
    phrase_lower = phrase.lower().strip()
    return NORMALIZATION_MAP.get(phrase_lower, phrase_lower)

# --- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ---
def load_data(uploaded_file):
    file_bytes = uploaded_file.getvalue()
    encodings_to_try = ['utf-8-sig', 'utf-8', 'latin1', 'cp949']
    
    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # íƒ­ êµ¬ë¶„ì ìš°ì„  ì‹œë„
            df = pd.read_csv(io.StringIO(file_content), sep='\t', lineterminator='\n')
            if df.shape[1] > 1: return df
        except Exception:
            continue
            
    for encoding in encodings_to_try:
        try:
            file_content = file_bytes.decode(encoding)
            # ì½¤ë§ˆ êµ¬ë¶„ì ì‹œë„
            df = pd.read_csv(io.StringIO(file_content))
            if df.shape[1] > 1: return df
        except Exception:
            continue
            
    return None

# --- í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜ ---
def classify_article(row):
    inclusion_keywords = ['user', 'viewer', 'audience', 'streamer', 'consumer', 'participant', 'behavior', 'experience', 'engagement', 'interaction', 'motivation', 'psychology', 'social', 'community', 'cultural', 'society', 'commerce', 'marketing', 'business', 'brand', 'purchase', 'monetization', 'education', 'learning', 'influencer']
    exclusion_keywords = ['protocol', 'network coding', 'wimax', 'ieee 802.16', 'mac layer', 'packet dropping', 'bandwidth', 'fec', 'arq', 'goodput', 'sensor data', 'geoscience', 'environmental data', 'wlan', 'ofdm', 'error correction', 'tcp', 'udp', 'network traffic']
    title = str(row.get('TI', '')).lower()
    source_title = str(row.get('SO', '')).lower()
    author_keywords = str(row.get('DE', '')).lower()
    keywords_plus = str(row.get('ID', '')).lower()
    abstract = str(row.get('AB', '')).lower()
    full_text = ' '.join([title, source_title, author_keywords, keywords_plus, abstract])
    if any(keyword in full_text for keyword in exclusion_keywords): return 'Exclude (ì œì™¸ì—°êµ¬)'
    if any(keyword in full_text for keyword in inclusion_keywords): return 'Include (ê´€ë ¨ì—°êµ¬)'
    return 'Review (ê²€í† í•„ìš”)'

def clean_keyword_string(keywords_str, stop_words, lemmatizer):
    """ê°œì„ ëœ í‚¤ì›Œë“œ ì •ê·œí™” ë° ì •ì œ ì²˜ë¦¬"""
    if pd.isna(keywords_str) or not isinstance(keywords_str, str): 
        return ""
    
    all_keywords = keywords_str.split(';')
    cleaned_keywords = set()
    
    for keyword in all_keywords:
        if not keyword.strip():
            continue
            
        # 1ë‹¨ê³„: ê¸°ë³¸ ì •ì œ (í•˜ì´í”ˆê³¼ ì–¸ë”ìŠ¤ì½”ì–´ ê³µë°±ìœ¼ë¡œ ë³€í™˜)
        keyword_clean = keyword.strip().lower()
        keyword_clean = re.sub(r'[^a-z\s\-_]', '', keyword_clean)
        
        # 2ë‹¨ê³„: êµ¬ë¬¸ ë‹¨ìœ„ ì •ê·œí™” ë¨¼ì € ì‹œë„ (í•˜ì´í”ˆ í¬í•¨ ìƒíƒœë¡œ)
        normalized_phrase = normalize_keyword_phrase(keyword_clean)
        
        # 3ë‹¨ê³„: ì •ê·œí™”ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë‹¨ì–´ë³„ ì²˜ë¦¬
        if normalized_phrase == keyword_clean.lower():
            # í•˜ì´í”ˆì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¨ì–´ë³„ ì²˜ë¦¬
            keyword_clean = keyword_clean.replace('-', ' ').replace('_', ' ')
            words = keyword_clean.split()
            
            # ë¶ˆìš©ì–´ ì œê±° ë° í‘œì œì–´ ì¶”ì¶œ
            filtered_words = []
            for word in words:
                if word and len(word) > 2 and word not in stop_words:
                    lemmatized_word = lemmatizer.lemmatize(word)
                    filtered_words.append(lemmatized_word)
            
            if filtered_words:
                reconstructed_phrase = " ".join(filtered_words)
                # ì¬êµ¬ì„±ëœ êµ¬ë¬¸ì— ëŒ€í•´ ë‹¤ì‹œ ì •ê·œí™” ì‹œë„
                final_keyword = normalize_keyword_phrase(reconstructed_phrase)
                if final_keyword and len(final_keyword) > 2:
                    cleaned_keywords.add(final_keyword)
        else:
            # ì´ë¯¸ ì •ê·œí™”ëœ ê²½ìš° ë°”ë¡œ ì¶”ê°€
            if normalized_phrase and len(normalized_phrase) > 2:
                cleaned_keywords.add(normalized_phrase)
    
    return '; '.join(sorted(list(cleaned_keywords)))

# --- SCIMAT í˜•ì‹ ë³€í™˜ í•¨ìˆ˜ (ì™„ì „ WoS í‘œì¤€ ì¤€ìˆ˜) ---
def convert_df_to_scimat_format(df_to_convert):
    # ì›ë³¸ WoS íŒŒì¼ê³¼ ì™„ì „íˆ ë™ì¼í•œ í•„ë“œ ìˆœì„œ ë° í—¤ë”
    wos_field_order = [
        'PT', 'AU', 'AF', 'TI', 'SO', 'LA', 'DT', 'DE', 'ID', 'AB', 'C1', 'C3', 'RP',
        'EM', 'RI', 'OI', 'FU', 'FX', 'CR', 'NR', 'TC', 'Z9', 'U1', 'U2', 'PU', 'PI', 'PA',
        'SN', 'EI', 'J9', 'JI', 'PD', 'PY', 'VL', 'IS', 'BP', 'EP', 'DI', 'EA', 'PG',
        'WC', 'WE', 'SC', 'GA', 'UT', 'PM', 'OA', 'DA'
    ]
    
    # ì›ë³¸ê³¼ ì™„ì „íˆ ë™ì¼í•œ í—¤ë”
    file_content = ["FN Clarivate Analytics Web of Science", "VR 1.0"]
    multi_line_fields = ['AU', 'AF', 'DE', 'ID', 'C1', 'C3', 'CR']
    
    for _, row in df_to_convert.iterrows():
        # ì²« ë²ˆì§¸ ë ˆì½”ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë¹ˆ ì¤„ ì¶”ê°€ (ì›ë³¸ê³¼ ë™ì¼)
        if len(file_content) > 2:
            file_content.append("")
            
        sorted_tags = [tag for tag in wos_field_order if tag in row.index and pd.notna(row[tag])]
        
        for tag in sorted_tags:
            value = row[tag]
            if pd.isna(value):
                continue
            if not isinstance(value, str): 
                value = str(value)
            if not value.strip(): 
                continue

            if tag in multi_line_fields:
                items = [item.strip() for item in value.split(';') if item.strip()]
                if items:
                    file_content.append(f"{tag} {items[0]}")
                    for item in items[1:]:
                        file_content.append(f"   {item}")
            else:
                file_content.append(f"{tag} {value}")

        file_content.append("ER")
    
    # ì›ë³¸ê³¼ ë™ì¼: UTF-8 (BOM ì—†ìŒ)ìœ¼ë¡œ ì¸ì½”ë”©
    return "\n".join(file_content).encode('utf-8')

# --- Streamlit UI ë° ì‹¤í–‰ ë¡œì§ ---
# í—¤ë” ì„¹ì…˜ - ì „ë¬¸ì  ë””ìì¸
st.markdown("""
<div style="text-align: center; padding: 2rem 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
            border-radius: 10px; margin-bottom: 2rem; color: white;">
    <h1 style="font-size: 3rem; font-weight: 300; margin: 0; letter-spacing: -1px;">
        ğŸ“Š WOS Prep
    </h1>
    <p style="font-size: 1.2rem; margin: 0.5rem 0 0 0; opacity: 0.9; font-weight: 300;">
        Professional Data Preprocessing for Science Mapping Analysis
    </p>
</div>
""", unsafe_allow_html=True)

# ê¸°ëŠ¥ ê°œìš” ì„¹ì…˜
col1, col2, col3 = st.columns(3)

with col1:
    st.markdown("""
    <div style="text-align: center; padding: 1.5rem; border: 1px solid #e0e0e0; border-radius: 8px; height: 120px;">
        <div style="font-size: 2rem; margin-bottom: 0.5rem;">âš¡</div>
        <h4 style="margin: 0; font-weight: 500;">Smart Classification</h4>
        <p style="margin: 0.5rem 0 0 0; color: #666; font-size: 0.9rem;">AI-powered research filtering</p>
    </div>
    """, unsafe_allow_html=True)

with col2:
    st.markdown("""
    <div style="text-align: center; padding: 1.5rem; border: 1px solid #e0e0e0; border-radius: 8px; height: 120px;">
        <div style="font-size: 2rem; margin-bottom: 0.5rem;">ğŸ”¬</div>
        <h4 style="margin: 0; font-weight: 500;">SciMAT Integration</h4>
        <p style="margin: 0.5rem 0 0 0; color: #666; font-size: 0.9rem;">Seamless compatibility</p>
    </div>
    """, unsafe_allow_html=True)

with col3:
    st.markdown("""
    <div style="text-align: center; padding: 1.5rem; border: 1px solid #e0e0e0; border-radius: 8px; height: 120px;">
        <div style="font-size: 2rem; margin-bottom: 0.5rem;">ğŸ¯</div>
        <h4 style="margin: 0; font-weight: 500;">Keyword Normalization</h4>
        <p style="margin: 0.5rem 0 0 0; color: #666; font-size: 0.9rem;">Advanced standardization</p>
    </div>
    """, unsafe_allow_html=True)

st.markdown("<br>", unsafe_allow_html=True)

# ì •ê·œí™” ì˜ˆì‹œ í‘œì‹œ - ì „ë¬¸ì  ìŠ¤íƒ€ì¼
with st.expander("âš™ï¸ Advanced Keyword Normalization Rules"):
    st.markdown("""
    **Machine Learning & AI Standardization:**
    - `machine learning` â† machine-learning, ML, machinelearning
    - `artificial intelligence` â† AI, artificial-intelligence
    - `deep learning` â† deep-learning, deep neural networks, DNN
    
    **Streaming & Media Normalization:**
    - `live streaming` â† live-streaming, livestreaming
    - `user experience` â† user-experience, UX
    
    **Research Methodology Standardization:**
    - `structural equation modeling` â† SEM, PLS-SEM
    - `e commerce` â† ecommerce, e-commerce, electronic commerce
    """)

# íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ - ì „ë¬¸ì  ë””ìì¸
st.markdown("""
<div style="border: 2px dashed #ccc; border-radius: 10px; padding: 2rem; text-align: center; margin: 2rem 0;">
    <div style="font-size: 3rem; margin-bottom: 1rem;">ğŸ“</div>
    <h3 style="margin: 0; color: #333;">Upload Your WOS Data</h3>
    <p style="color: #666; margin: 0.5rem 0;">Support for CSV and TXT formats from Web of Science</p>
</div>
""", unsafe_allow_html=True)

uploaded_file = st.file_uploader(
    "Choose your Web of Science file", 
    type=['csv', 'txt'],
    help="Please upload Tab-delimited or Plain Text format files downloaded from Web of Science"
)

if uploaded_file is not None:
    df = load_data(uploaded_file)
    if df is None:
        st.error("íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Web of Scienceì—ì„œ ë‹¤ìš´ë¡œë“œí•œ 'Tab-delimited' ë˜ëŠ” 'Plain Text' í˜•ì‹ì˜ íŒŒì¼ì´ ë§ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # ì›ë³¸ ì»¬ëŸ¼ëª… ë³´ì¡´ (ëŒ€ì†Œë¬¸ì ë³€í™˜í•˜ì§€ ì•ŠìŒ)
    column_mapping = {
        'Authors': 'AU', 'Article Title': 'TI', 'Source Title': 'SO', 'Author Keywords': 'DE',
        'Keywords Plus': 'ID', 'Abstract': 'AB', 'Cited References': 'CR', 'Publication Year': 'PY',
        'Times Cited, All Databases': 'TC', 'Times Cited, WoS Core': 'Z9'
    }
    
    # ì»¬ëŸ¼ëª…ì´ ì´ë¯¸ WoS íƒœê·¸ í˜•ì‹ì¸ ê²½ìš°ëŠ” ë³€í™˜í•˜ì§€ ì•ŠìŒ
    for old_name, new_name in column_mapping.items():
        if old_name in df.columns:
            df.rename(columns={old_name: new_name}, inplace=True)

    with st.spinner("ğŸ” Processing your data with advanced algorithms..."):
        # 1ë‹¨ê³„: ë¶„ë¥˜ (ì›ë³¸ í‚¤ì›Œë“œ ê¸°ì¤€)
        df['Classification'] = df.apply(classify_article, axis=1)
        
        # ì›ë³¸ í‚¤ì›Œë“œ ë°±ì—… (ë¹„êµìš©)
        if 'DE' in df.columns:
            df['DE_Original'] = df['DE'].copy()
        if 'ID' in df.columns:
            df['ID_Original'] = df['ID'].copy()
            
        # 2ë‹¨ê³„: Include ë…¼ë¬¸ë§Œ í‚¤ì›Œë“œ ì •ê·œí™”
        stop_words = set(stopwords.words('english'))
        custom_stop_words = {'study', 'research', 'analysis', 'results', 'paper', 'article', 'using', 'based', 'approach', 'method', 'system', 'model'}
        stop_words.update(custom_stop_words)
        lemmatizer = WordNetLemmatizer()
        
        include_mask = df['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)'
        
        if 'DE' in df.columns:
            df['DE_cleaned'] = df['DE'].copy()
            df.loc[include_mask, 'DE_cleaned'] = df.loc[include_mask, 'DE'].apply(
                lambda x: clean_keyword_string(x, stop_words, lemmatizer)
            )
        if 'ID' in df.columns:
            df['ID_cleaned'] = df['ID'].copy()
            df.loc[include_mask, 'ID_cleaned'] = df.loc[include_mask, 'ID'].apply(
                lambda x: clean_keyword_string(x, stop_words, lemmatizer)
            )
        
        st.success("âœ… Processing completed successfully!")
        
        # ê²°ê³¼ ìš”ì•½ - ì „ë¬¸ì  ë©”íŠ¸ë¦­ ë””ìŠ¤í”Œë ˆì´
        st.markdown("### ğŸ“ˆ Analysis Results")
        
        # ë©”íŠ¸ë¦­ ì¹´ë“œ ìŠ¤íƒ€ì¼
        classification_counts = df['Classification'].value_counts().reset_index()
        classification_counts.columns = ['Classification', 'Count']
        
        metric_col1, metric_col2, metric_col3 = st.columns(3)
        
        include_count = classification_counts[classification_counts['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)']['Count'].iloc[0] if len(classification_counts[classification_counts['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)']) > 0 else 0
        review_count = classification_counts[classification_counts['Classification'] == 'Review (ê²€í† í•„ìš”)']['Count'].iloc[0] if len(classification_counts[classification_counts['Classification'] == 'Review (ê²€í† í•„ìš”)']) > 0 else 0
        exclude_count = classification_counts[classification_counts['Classification'] == 'Exclude (ì œì™¸ì—°êµ¬)']['Count'].iloc[0] if len(classification_counts[classification_counts['Classification'] == 'Exclude (ì œì™¸ì—°êµ¬)']) > 0 else 0
        
        with metric_col1:
            st.metric("Relevant Studies", include_count, delta=f"{include_count/(len(df))*100:.1f}%")
        with metric_col2:
            st.metric("Review Required", review_count, delta=f"{review_count/(len(df))*100:.1f}%")
        with metric_col3:
            st.metric("Excluded", exclude_count, delta=f"{exclude_count/(len(df))*100:.1f}%")
        
        # ì°¨íŠ¸ í‘œì‹œ
        chart = alt.Chart(classification_counts).mark_arc(innerRadius=50).encode(
            theta=alt.Theta(field="Count", type="quantitative"), 
            color=alt.Color(field="Classification", type="nominal", title="Classification"),
            tooltip=['Classification', 'Count']
        ).properties(title='Research Classification Distribution', width=400, height=300)
        st.altair_chart(chart, use_container_width=True)
        
        st.markdown("---")
        st.markdown("### ğŸ” Keyword Analysis for Relevant Studies")
        all_keywords = []
        if 'DE_cleaned' in df.columns: 
            all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'DE_cleaned'].dropna() for kw in text.split(';') if kw.strip()])
        if 'ID_cleaned' in df.columns: 
            all_keywords.extend([kw.strip() for text in df.loc[include_mask, 'ID_cleaned'].dropna() for kw in text.split(';') if kw.strip()])
        
        if all_keywords:
            keyword_counts = Counter(all_keywords)
            top_n = 20
            top_keywords = keyword_counts.most_common(top_n)
            df_keywords = pd.DataFrame(top_keywords, columns=['Keyword', 'Frequency'])
            
            keyword_chart = alt.Chart(df_keywords).mark_bar(color='#667eea').encode(
                x=alt.X('Frequency:Q', title='Frequency'), 
                y=alt.Y('Keyword:N', title='Keywords', sort='-x'),
                tooltip=['Keyword', 'Frequency']
            ).properties(title=f'Top {top_n} Normalized Keywords', width=600, height=400)
            st.altair_chart(keyword_chart, use_container_width=True)
            
            # ì •ê·œí™” ì „í›„ ë¹„êµ ìƒ˜í”Œ í‘œì‹œ
            if st.checkbox("ğŸ”¬ View Normalization Examples"):
                st.markdown("**Keyword Normalization Comparison (Top 3 Relevant Studies)**")
                sample_data = []
                sample_rows = df.loc[include_mask].head(3)
                
                for idx, row in sample_rows.iterrows():
                    if 'DE_Original' in df.columns and pd.notna(row.get('DE_Original')):
                        sample_data.append({
                            'Paper ID': idx,
                            'Field': 'Author Keywords (DE)',
                            'Before': str(row['DE_Original'])[:80] + "..." if len(str(row['DE_Original'])) > 80 else str(row['DE_Original']),
                            'After': str(row['DE_cleaned'])[:80] + "..." if len(str(row['DE_cleaned'])) > 80 else str(row['DE_cleaned'])
                        })
                    if 'ID_Original' in df.columns and pd.notna(row.get('ID_Original')):
                        sample_data.append({
                            'Paper ID': idx,
                            'Field': 'Keywords Plus (ID)',
                            'Before': str(row['ID_Original'])[:80] + "..." if len(str(row['ID_Original'])) > 80 else str(row['ID_Original']),
                            'After': str(row['ID_cleaned'])[:80] + "..." if len(str(row['ID_cleaned'])) > 80 else str(row['ID_cleaned'])
                        })
                
                if sample_data:
                    comparison_df = pd.DataFrame(sample_data)
                    st.dataframe(comparison_df, use_container_width=True)
                else:
                    st.info("No comparison data available.")
        else:
            st.warning("âš ï¸ No valid keywords found in relevant studies.")

        st.markdown("---")
        st.markdown("### ğŸ“¥ Download Processed Data")
        
        # ë‹¤ìš´ë¡œë“œ ì˜µì…˜ì„ ì¹´ë“œ ìŠ¤íƒ€ì¼ë¡œ ê°œì„ 
        st.markdown("""
        <div style="background: #f8f9fa; padding: 1rem; border-radius: 8px; margin: 1rem 0;">
            <h4 style="margin: 0 0 0.5rem 0; color: #333;">Choose Your Export Format</h4>
            <p style="margin: 0; color: #666; font-size: 0.9rem;">Select the appropriate format based on your analysis workflow</p>
        </div>
        """, unsafe_allow_html=True)
        
        # ì„¸ ê°€ì§€ ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ì œê³µ
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**ğŸ”§ Complete Original**")
            df_scimat = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
            
            # ì›ë³¸ í‚¤ì›Œë“œ ì™„ì „ ë³µì› (SciMAT í˜¸í™˜ì„± ìµœìš°ì„ )
            if 'DE_Original' in df_scimat.columns:
                df_scimat['DE'] = df_scimat['DE_Original']
            if 'ID_Original' in df_scimat.columns:
                df_scimat['ID'] = df_scimat['ID_Original']
            
            # ì„ì‹œ ì»¬ëŸ¼ë“¤ë§Œ ì œê±°
            cols_to_drop = ['Classification', 'DE_cleaned', 'ID_cleaned', 'DE_Original', 'ID_Original']
            df_scimat_output = df_scimat.drop(columns=[col for col in cols_to_drop if col in df_scimat.columns])
            
            st.metric("Papers", len(df_scimat_output), delta="Original format")
            
            text_data_scimat = convert_df_to_scimat_format(df_scimat_output)
            st.download_button(
                label="ğŸ“¥ Download Original", 
                data=text_data_scimat, 
                file_name="wos_prep_original.txt", 
                mime="text/plain",
                key="scimat_download",
                use_container_width=True
            )
            st.caption("ğŸ¯ For SciMAT manual preprocessing")
        
        with col2:
            st.markdown("**âš¡ Minimal Processing**")
            df_minimal = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
            
            # ìµœì†Œ ì •ì œ: SciMAT ê·¸ë£¹í•‘ ìµœì í™”
            if 'DE' in df_minimal.columns:
                df_minimal['DE'] = df_minimal['DE'].apply(
                    lambda x: '; '.join([kw.strip().lower() for kw in str(x).split(';') if kw.strip()]) if pd.notna(x) else x
                )
            if 'ID' in df_minimal.columns:
                df_minimal['ID'] = df_minimal['ID'].apply(
                    lambda x: '; '.join([kw.strip().lower() for kw in str(x).split(';') if kw.strip()]) if pd.notna(x) else x
                )
            
            # ì„ì‹œ ì»¬ëŸ¼ë“¤ ì œê±°
            cols_to_drop = ['Classification', 'DE_cleaned', 'ID_cleaned', 'DE_Original', 'ID_Original']
            df_minimal_output = df_minimal.drop(columns=[col for col in cols_to_drop if col in df_minimal.columns])
            
            st.metric("Papers", len(df_minimal_output), delta="Case normalized")
            
            text_data_minimal = convert_df_to_scimat_format(df_minimal_output)
            st.download_button(
                label="ğŸ“¥ Download Minimal", 
                data=text_data_minimal, 
                file_name="wos_prep_minimal.txt", 
                mime="text/plain",
                key="minimal_download",
                use_container_width=True
            )
            st.caption("âœ¨ Optimized for Levenshtein distance")
        
        with col3:
            st.markdown("**ğŸ“Š Full Normalization**")
            df_analysis = df[df['Classification'].isin(['Include (ê´€ë ¨ì—°êµ¬)', 'Review (ê²€í† í•„ìš”)'])].copy()
            
            # ì •ê·œí™”ëœ í‚¤ì›Œë“œë¡œ êµì²´ (Include ë…¼ë¬¸ë§Œ)
            if 'DE_cleaned' in df_analysis.columns: 
                df_analysis.loc[df_analysis['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)', 'DE'] = df_analysis.loc[df_analysis['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)', 'DE_cleaned']
            if 'ID_cleaned' in df_analysis.columns: 
                df_analysis.loc[df_analysis['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)', 'ID'] = df_analysis.loc[df_analysis['Classification'] == 'Include (ê´€ë ¨ì—°êµ¬)', 'ID_cleaned']
            
            # ì„ì‹œ ì»¬ëŸ¼ë“¤ ì œê±°
            cols_to_drop = ['Classification', 'DE_cleaned', 'ID_cleaned', 'DE_Original', 'ID_Original']
            df_analysis_output = df_analysis.drop(columns=[col for col in cols_to_drop if col in df_analysis.columns])
            
            st.metric("Papers", len(df_analysis_output), delta="Fully normalized")
            
            text_data_analysis = convert_df_to_scimat_format(df_analysis_output)
            st.download_button(
                label="ğŸ“¥ Download Normalized", 
                data=text_data_analysis, 
                file_name="wos_prep_normalized.txt", 
                mime="text/plain",
                key="analysis_download",
                use_container_width=True
            )
            st.caption("ğŸ“ˆ For final analysis & papers")
        
        # ì‚¬ìš© ì•ˆë‚´ ë° SciMAT ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ
        st.info("""
        **ğŸ“‹ SciMAT ìµœì í™” ì›Œí¬í”Œë¡œìš°:**
        
        **1ë‹¨ê³„**: **ì›ë³¸ í‚¤ì›Œë“œ** ë˜ëŠ” **ìµœì†Œ ì •ì œ** íŒŒì¼ì„ SciMATì— ì—…ë¡œë“œ
        - SciMATì˜ ê°•ë ¥í•œ ë‚´ì¥ ì „ì²˜ë¦¬ ëª¨ë“ˆ í™œìš©
        - Levenshtein distanceë¡œ ìë™ ê·¸ë£¹í•‘ ìˆ˜í–‰
        - ìˆ˜ë™ ê·¸ë£¹ ì¡°ì • ë° Stop group ì„¤ì •
        
        **2ë‹¨ê³„**: SciMATì—ì„œ Science Mapping ë¶„ì„ ì‹¤í–‰
        - ê¸°ê°„ë³„ ë¶„ì„ ì„¤ì •
        - í´ëŸ¬ìŠ¤í„°ë§ ë° ì‹œê°í™”
        
        **3ë‹¨ê³„**: **ì™„ì „ ì •ê·œí™”** íŒŒì¼ë¡œ ì¶”ê°€ í‚¤ì›Œë“œ ë¶„ì„
        - í‘œì¤€í™”ëœ í‚¤ì›Œë“œë¡œ ì •ë°€ ë¶„ì„
        - ìµœì¢… ë³´ê³ ì„œ ë° ë…¼ë¬¸ ì‘ì„±
        
        **ğŸ’¡ í•µì‹¬**: SciMAT ìì²´ì˜ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ìµœì ì˜ ê²°ê³¼ ë‹¬ì„±
        """)
        
        # í‚¤ì›Œë“œ ì •ê·œí™” í†µê³„
        if include_mask.any():
            include_count = include_mask.sum()
            st.success(f"âœ… í‚¤ì›Œë“œ ì •ê·œí™” ì ìš©: {include_count}ê°œ 'Include' ë…¼ë¬¸")
        
        # SciMAT ì „ë¬¸ê°€ íŒ
        with st.expander("ğŸ’¡ SciMAT ì „ë¬¸ê°€ íŒ - íš¨ê³¼ì ì¸ ê·¸ë£¹í•‘ ì „ëµ"):
            st.write("""
            **ğŸ“Œ SciMATì—ì„œ íš¨ê³¼ì ì¸ í‚¤ì›Œë“œ ê·¸ë£¹í•‘:**
            
            **1. Levenshtein Distance í™œìš©**
            - `Group set` â†’ `Words groups manager` â†’ `Add` ë²„íŠ¼ ì˜† ë„êµ¬ í™œìš©
            - Maximum distance 2-3ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìœ ì‚¬ í‚¤ì›Œë“œ ìë™ íƒì§€
            - "live streaming", "live-streaming", "livestreaming" ë“± ìë™ ê·¸ë£¹í™”
            
            **2. ìˆ˜ë™ ê·¸ë£¹í•‘ ìš°ì„ ìˆœìœ„**
            - ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•œ í‚¤ì›Œë“œë“¤ ìš°ì„  ê·¸ë£¹í™”
            - ë³µìˆ˜í˜•/ë‹¨ìˆ˜í˜•: "consumer" â†” "consumers"
            - ì•½ì–´/í’€ë„¤ì„: "AI" â†” "artificial intelligence"
            
            **3. Stop Group ì„¤ì •**
            - ë„ˆë¬´ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œëŠ” Stop groupìœ¼ë¡œ ì„¤ì •
            - "research", "analysis", "study" ë“± ì œì™¸
            
            **4. ê·¸ë£¹ëª… ì„¤ì • ê·œì¹™**
            - ê°€ì¥ í‘œì¤€ì ì´ê³  ëª…í™•í•œ ìš©ì–´ë¥¼ ê·¸ë£¹ëª…ìœ¼ë¡œ ì„ íƒ
            - ì†Œë¬¸ì í†µì¼ ë° í•˜ì´í”ˆ ëŒ€ì‹  ê³µë°± ì‚¬ìš© ê¶Œì¥
            """)
        
        st.write("**ë¯¸ë¦¬ë³´ê¸° (ìµœì†Œ ì •ì œ - SciMAT ìµœì í™” ë²„ì „)**")
        st.dataframe(df_minimal_output.head(10))
